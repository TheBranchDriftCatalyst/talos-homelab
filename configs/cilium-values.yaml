# Cilium CNI values for Talos Linux cluster
# Generated for catalyst-cluster multi-node setup

# =============================================================================
# Cluster Identity (required for ClusterMesh)
# =============================================================================
cluster:
  name: talos-home
  id: 1

# Replace kube-proxy with Cilium's eBPF implementation
kubeProxyReplacement: true

# =============================================================================
# NodePort Configuration for Multi-Network Access
# =============================================================================
# Include both LAN and Nebula network CIDRs so NodePort services are accessible
# via both interfaces. This is required for ClusterMesh via Nebula overlay.
nodePort:
  addresses:
    - 192.168.1.0/24   # LAN network
    - 10.100.0.0/16    # Nebula overlay network

# Kubernetes API server endpoint
k8sServiceHost: 192.168.1.54
k8sServicePort: 6443

# =============================================================================
# Prometheus Metrics Configuration
# =============================================================================
# Enable Prometheus metrics for observability
# Reference: https://docs.cilium.io/en/stable/observability/metrics/

# Cilium Agent metrics (port 9962)
prometheus:
  enabled: true
  port: 9962
  serviceMonitor:
    enabled: false  # We use PodMonitor instead (no Service exists)

# Single replica for operator (can scale later)
operator:
  replicas: 1
  # Cilium Operator metrics (port 9963)
  prometheus:
    enabled: true
    port: 9963
    serviceMonitor:
      enabled: false  # We use PodMonitor instead

# Use Kubernetes IPAM (compatible with existing pod CIDR)
ipam:
  mode: kubernetes

# Hubble observability
hubble:
  enabled: true
  # Increase ring buffer to prevent lost events (default 4095)
  # At ~150 flows/sec, 4095 fills up causing drops
  # Must be (2^n - 1): 4095, 8191, 16383, 32767, 65535
  eventBufferCapacity: 16383
  relay:
    enabled: true
  ui:
    enabled: true
  # Hubble metrics (port 9965)
  metrics:
    enabled:
      - dns
      - drop
      - tcp
      - flow
      - port-distribution
      - icmp
      - policy:sourceContext=app|workload-name|pod|reserved-identity;destinationContext=app|workload-name|pod|dns|reserved-identity;labelsContext=source_namespace,destination_namespace
      - httpV2:exemplars=true;labelsContext=source_ip,source_namespace,source_workload,destination_ip,destination_namespace,destination_workload,traffic_direction
    port: 9965
    enableOpenMetrics: true
    serviceMonitor:
      enabled: false  # We use PodMonitor instead

# Talos-specific security context
securityContext:
  capabilities:
    ciliumAgent:
      - CHOWN
      - KILL
      - NET_ADMIN
      - NET_RAW
      - IPC_LOCK
      - SYS_ADMIN
      - SYS_RESOURCE
      - DAC_OVERRIDE
      - FOWNER
      - SETGID
      - SETUID
    cleanCiliumState:
      - NET_ADMIN
      - SYS_ADMIN
      - SYS_RESOURCE

# Talos uses cgroupv2
cgroup:
  autoMount:
    enabled: false
  hostRoot: /sys/fs/cgroup

# =============================================================================
# DNS Proxy Configuration
# =============================================================================
# Disable transparent DNS proxy mode - Talos's hostDNS handles DNS forwarding
# The transparent proxy can't reach Talos's 127.0.0.53 from pod namespace
dnsProxy:
  enableTransparentMode: false

# Resource limits to prevent flannel-style throttling issues
resources:
  limits:
    cpu: 1000m
    memory: 1Gi
  requests:
    cpu: 100m
    memory: 256Mi

# =============================================================================
# SPIRE-based Mutual TLS Authentication
# =============================================================================
# Reference: https://docs.cilium.io/en/stable/network/servicemesh/mutual-authentication/mutual-authentication/
#
# Uses SPIRE for workload identity - WireGuard encryption is optional
# SPIRE provides mutual authentication between services via mTLS

authentication:
  mutual:
    spire:
      enabled: true
      install:
        enabled: true
        namespace: cilium-spire
        server:
          dataStorage:
            enabled: true
            storageClass: local-path
            accessMode: ReadWriteOnce
            size: 1Gi

# =============================================================================
# Egress Gateway for VPN Traffic Steering (DISABLED)
# =============================================================================
# Reference: https://docs.cilium.io/en/stable/network/egress-gateway/
#
# DISABLED: Abandoned node-level WireGuard approach due to router NAT issues.
# Now using pod-based gluetun VPN gateway instead.
# Re-enable if you need to route traffic through specific nodes/interfaces.

egressGateway:
  enabled: false

# BPF masquerade required for egress gateway
bpf:
  masquerade: true

# =============================================================================
# WireGuard Transparent Encryption (TESTING - DISABLED)
# =============================================================================
# Reference: https://docs.cilium.io/en/stable/security/network/encryption-wireguard/
#
# ISSUE: WireGuard + VXLAN causes 100% cross-node packet loss
# Tested: MTU=1370, devices="en+" - still broken
# Next: Try native routing mode instead of VXLAN tunnel
#
# encryption:
#   enabled: true
#   type: wireguard
#   nodeEncryption: false

# =============================================================================
# ClusterMesh for Multi-Cluster Connectivity
# =============================================================================
# Reference: https://docs.cilium.io/en/stable/network/clustermesh/clustermesh/
#
# Enables cross-cluster service discovery and pod-to-pod connectivity
# across talos-home (ID:1) and aws-lighthouse (ID:2) clusters via Nebula mesh

clustermesh:
  useAPIServer: true
  apiserver:
    replicas: 1
    service:
      type: NodePort
      nodePort: 32379
    tls:
      auto:
        enabled: true
        method: helm
    # Pin ClusterMesh API server to control plane node (has Nebula)
    # This ensures the NodePort 32379 is accessible via Nebula IP (10.100.0.1)
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
    # Tolerate control-plane taint to actually schedule on the control plane
    tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
