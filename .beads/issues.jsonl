{"id":"TALOS-084","title":"Audit and convert PrometheusRules to Mimir ruler format","description":"Export 32 PrometheusRules and convert to Mimir ruler format.\n\n**Steps:**\n1. Export all PrometheusRules to YAML\n2. Identify critical vs nice-to-have rules\n3. Convert to Mimir ruler YAML format\n4. Deploy as ConfigMaps for Mimir ruler\n5. Verify rules are loaded in Mimir\n\n**Mimir ruler format:**\nRules stored in object storage (MinIO) or ConfigMaps, same PromQL syntax.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-13T22:26:51.384263-08:00","updated_at":"2025-12-13T22:39:26.956629-08:00","closed_at":"2025-12-13T22:39:26.956629-08:00","labels":["alerting","mimir","monitoring"],"dependencies":[{"issue_id":"TALOS-084","depends_on_id":"TALOS-zqh","type":"parent-child","created_at":"2025-12-13T22:27:04.646641-08:00","created_by":"daemon"}]}
{"id":"TALOS-26f","title":"Investigate notification event sources and filtering strategy","description":"Research and document what cluster events we want to be notified about. Consider:\n\n**Flux Events:**\n- GitRepository sync failures\n- Kustomization reconciliation errors\n- HelmRelease failures/rollbacks\n\n**Kubernetes Events:**\n- Pod CrashLoopBackOff\n- Node NotReady\n- PVC binding failures\n- OOMKilled containers\n\n**Prometheus Alerts:**\n- Critical severity alerts from Alertmanager\n\n**Notification Classes (to investigate):**\n- Could we tier notifications? e.g. critical (immediate), warning (batched), info (daily digest)\n- Different delivery channels per class? (Mac push vs Discord vs email)\n- ntfy supports priorities (1-5) and tags - could map to classes\n- Alertmanager has severity labels - could route differently\n\n**Questions to answer:**\n- Use Flux notifications directly or route through Alertmanager?\n- What severity threshold? (error only vs info)\n- Should we deduplicate repeated failures?\n- Rate limiting to prevent notification spam?\n- What notification class taxonomy makes sense for this homelab?","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-12T22:11:20.014472-08:00","updated_at":"2025-12-12T22:12:15.310425-08:00","labels":["notifications","research"],"dependencies":[{"issue_id":"TALOS-26f","depends_on_id":"TALOS-8uv","type":"parent-child","created_at":"2025-12-12T22:11:30.722454-08:00","created_by":"daemon"}]}
{"id":"TALOS-2b2","title":"Create GPU worker node config (talos02-gpu)","description":"Create worker-talos02-gpu.yaml with GPU-specific settings: kernel modules (i915, xe), udev rules for /dev/dri, node labels for GPU scheduling","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T16:53:29.877213-08:00","updated_at":"2025-12-12T17:02:27.885562-08:00","closed_at":"2025-12-12T17:02:27.885562-08:00","dependencies":[{"issue_id":"TALOS-2b2","depends_on_id":"TALOS-fpp","type":"parent-child","created_at":"2025-12-12T16:59:21.940521-08:00","created_by":"daemon"}]}
{"id":"TALOS-2ez","title":"MemeX Discourse Pipeline - Kubernetes Deployment","description":"Deploy MemeX Dagster ML pipeline to talos-homelab Kubernetes cluster.\n\n**Project Overview:**\nMemeX is a Discourse Extraction ML Pipeline that processes documents through:\n- Entity extraction (spaCy NER)\n- Semantic embeddings (SentenceTransformers)\n- Proposition extraction\n- Knowledge graph construction (Neo4j)\n\n**Source:** `../workspace/@memeX` (Docker Compose currently, needs K8s manifests)\n\n**Infrastructure Requirements:**\n\n| Component | Technology | Purpose |\n|-----------|-----------|---------|\n| PostgreSQL 14 | Primary DB | Documents, entities, embeddings |\n| Neo4j 5.15 | Graph DB | Knowledge graph (APOC + GDS) |\n| Redis 7 | Cache | Pipeline caching |\n| Dagster | Orchestration | Job scheduling, asset management |\n| Django API | REST API | Query interface |\n\n**Key Dagster Jobs:**\n- `memex_extraction_job` - Full document → knowledge graph pipeline\n- `congressional_full_pipeline_job` - Congress.gov bill analysis\n- `reddit_user_activity_job` - Reddit data processing\n- Scheduled: Daily congressional ingestion, weekly full pipeline\n\n**Shared Infrastructure with TALOS-aev:**\n- Could share GPU node for embedding generation\n- Similar pattern to Ollama deployment (ML workloads)\n- Prometheus/Grafana integration for monitoring\n\n**Phases:**\n1. Create K8s manifests (Deployments, StatefulSets, Services)\n2. Deploy databases (PostgreSQL, Neo4j, Redis)\n3. Deploy Dagster webserver + daemon\n4. Deploy Django API\n5. Configure Flux GitOps\n6. Integrate with monitoring stack","design":"**Architecture Decisions:**\n\n1. **PostgreSQL**: CloudNativePG operator\n   - Deploy CNPG operator to cluster first\n   - HA PostgreSQL cluster with automated backups\n   - Separate `Cluster` CR for MemeX database\n\n2. **Storage**: NFS (NAS)\n   - PostgreSQL and Neo4j PVCs on NFS storage class\n   - Durable, survives node failures\n   - Configure NFS StorageClass if not exists\n\n3. **GPU Acceleration**: Optional with node affinity\n   - Embedding jobs prefer GPU node (`talos02-gpu`) when available\n   - Fallback to CPU on any node\n   - Use `preferredDuringSchedulingIgnoredDuringExecution` affinity\n\n4. **Ingress Hostnames**:\n   - `memex.talos00` - Django REST API\n   - `dagster.talos00` - Dagster webserver UI\n   - `neo4j.talos00` - Neo4j browser (optional)\n\n**Namespace:** `memex` (dedicated)\n\n**Dependencies:**\n- TALOS-fpp (GPU node) - soft dependency for embedding acceleration\n- NFS StorageClass configured\n- CloudNativePG operator deployed","status":"open","priority":3,"issue_type":"epic","created_at":"2025-12-12T22:50:02.371086-08:00","updated_at":"2025-12-12T22:52:31.581867-08:00","labels":["dagster","memex","ml-pipeline","neo4j","nlp"],"dependencies":[{"issue_id":"TALOS-2ez","depends_on_id":"TALOS-aev","type":"related","created_at":"2025-12-12T22:50:10.453655-08:00","created_by":"daemon"}]}
{"id":"TALOS-34t","title":"Add Intel GPU infrastructure to Flux","description":"Create infrastructure/base/intel-gpu with NFD HelmRelease, Intel Device Plugins Operator, and GPU plugin CR. Add Flux Kustomization.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T16:53:29.968683-08:00","updated_at":"2025-12-12T17:02:27.980284-08:00","closed_at":"2025-12-12T17:02:27.980284-08:00","dependencies":[{"issue_id":"TALOS-34t","depends_on_id":"TALOS-fpp","type":"parent-child","created_at":"2025-12-12T16:59:22.065322-08:00","created_by":"daemon"}]}
{"id":"TALOS-3of","title":"Deploy OTEL Collector to cluster","description":"Deploy OpenTelemetry Collector using the official Helm chart.\n\n**Components:**\n- Deployment mode (gateway) for receiving OTLP from external (Claude Code on Mac)\n- DaemonSet mode for collecting node/pod logs\n\n**Config:**\n- Receivers: OTLP (gRPC 4317, HTTP 4318), prometheus scrape\n- Processors: batch, memory_limiter\n- Exporters: prometheusremotewrite, loki, otlp (tempo)\n\n**Ingress:** `otel.talos00` for OTLP HTTP from Mac","status":"in_progress","priority":1,"issue_type":"task","created_at":"2025-12-13T09:20:15.994792-08:00","updated_at":"2025-12-13T09:20:34.920939-08:00","labels":["infrastructure","otel"],"dependencies":[{"issue_id":"TALOS-3of","depends_on_id":"TALOS-nh8","type":"parent-child","created_at":"2025-12-13T09:20:28.202948-08:00","created_by":"daemon"}]}
{"id":"TALOS-56z","title":"Audit: ArgoCD applications dashboard coverage","description":"Ensure ArgoCD apps (catalyst-ui, kasa-exporter, arr-stack-private) are shown in dashboards with sync status","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.890889-08:00","updated_at":"2025-12-12T18:57:04.454179-08:00","closed_at":"2025-12-12T18:57:04.454179-08:00"}
{"id":"TALOS-58e","title":"Configure Grafana datasources for Loki and Tempo","description":"Add datasources to existing Grafana instance:\n\n- Loki datasource for logs\n- Tempo datasource for traces\n- Configure derived fields for trace correlation\n- Import community dashboards for OTEL","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T09:20:16.157694-08:00","updated_at":"2025-12-13T22:23:07.590385-08:00","closed_at":"2025-12-13T22:23:07.590385-08:00","labels":["grafana","otel"],"dependencies":[{"issue_id":"TALOS-58e","depends_on_id":"TALOS-nh8","type":"parent-child","created_at":"2025-12-13T09:20:28.358582-08:00","created_by":"daemon"}]}
{"id":"TALOS-6of","title":"Create root orchestrator dashboard","description":"dashboard.sh at project root - inline summary mode (default) + interactive menu mode (--interactive)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.772959-08:00","updated_at":"2025-12-12T18:55:36.559887-08:00","closed_at":"2025-12-12T18:55:36.559887-08:00"}
{"id":"TALOS-6qm","title":"Restore dashboard-common.sh library","description":"Restore scripts/lib/dashboard-common.sh from git (commit 7bf34c3^). All dashboards depend on this.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-12T18:40:35.511188-08:00","updated_at":"2025-12-12T18:48:04.918013-08:00","closed_at":"2025-12-12T18:48:04.918013-08:00"}
{"id":"TALOS-7fu","title":"MCP Server Integration for AI-powered cluster management","description":"Deploy MCP servers for AI-assisted infrastructure management:\n\n**High Priority:**\n- Kubernetes MCP Server (cluster queries, Helm management)\n- Grafana MCP Server (dashboard/metric queries)\n- Prometheus MCP Server (PromQL via AI)\n\n**Medium Priority:**\n- FluxCD/ArgoCD MCP Server (may need custom dev)\n\n**Deployment:** Containerized in observability namespace\n\nFrom: docs/06-project-management/enhancement-roadmap.md (Stream 1)","status":"open","priority":3,"issue_type":"epic","created_at":"2025-12-12T22:20:28.686317-08:00","updated_at":"2025-12-12T22:20:28.686317-08:00","labels":["ai","mcp","observability"]}
{"id":"TALOS-7vk","title":"Add --summary/--full flags to existing dashboards","description":"Update infrastructure/dashboard.sh, arr-stack/dashboard.sh, catalyst-llm/dashboard.sh with summary mode for nesting","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.828008-08:00","updated_at":"2025-12-12T18:55:36.604336-08:00","closed_at":"2025-12-12T18:55:36.604336-08:00"}
{"id":"TALOS-8ey","title":"Implement backup strategy for etcd and PVCs","description":"Critical data protection:\n\n- etcd backup automation (Talos provides etcd snapshots)\n- PVC snapshot capabilities\n- Backup storage location (NAS? S3?)\n- Create disaster recovery runbook\n- Test restore procedures\n\nFrom: docs/_archive/TODO.md","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-12T22:20:28.578721-08:00","updated_at":"2025-12-12T22:20:28.578721-08:00","labels":["backup","disaster-recovery","infrastructure"]}
{"id":"TALOS-8uv","title":"macOS Native Notifications for Cluster Events","description":"Implement a notification system that sends critical cluster events to macOS native notifications via ntfy.sh. Goal is to be alerted about important issues without getting overwhelmed by noise.","status":"open","priority":2,"issue_type":"epic","created_at":"2025-12-12T22:11:01.035773-08:00","updated_at":"2025-12-12T22:11:01.035773-08:00","labels":["flux","notifications","observability"]}
{"id":"TALOS-9ku","title":"Configure Plex/Tdarr for GPU transcoding","description":"Update Plex/Tdarr deployments with GPU resource requests, nodeSelector for GPU node, /dev/dri volume mount","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-12T17:02:37.005599-08:00","updated_at":"2025-12-12T17:02:37.005599-08:00","dependencies":[{"issue_id":"TALOS-9ku","depends_on_id":"TALOS-fpp","type":"parent-child","created_at":"2025-12-12T17:02:44.485533-08:00","created_by":"daemon"},{"issue_id":"TALOS-9ku","depends_on_id":"TALOS-dfv","type":"blocks","created_at":"2025-12-12T17:02:44.569654-08:00","created_by":"daemon"}]}
{"id":"TALOS-a23","title":"Configure Media Stack Applications","description":"Complete application-level configuration for the deployed media stack:\n\n- Configure Prowlarr indexers\n- Connect Sonarr → Prowlarr\n- Connect Radarr → Prowlarr\n- Configure Plex media libraries\n- Configure Jellyfin media libraries\n- Set up Homepage dashboard widgets\n- Test end-to-end media workflow\n\nFrom: docs/_archive/TODO.md","status":"open","priority":3,"issue_type":"epic","created_at":"2025-12-12T22:20:28.47382-08:00","updated_at":"2025-12-12T22:20:28.47382-08:00","labels":["configuration","media-stack"]}
{"id":"TALOS-a4s","title":"Audit ServiceMonitors and document coverage","description":"Document what the 56 ServiceMonitors are scraping.\n\n**Purpose:** Ensure Alloy's native discovery covers all targets.\n\n**Steps:**\n1. List all ServiceMonitors with their selectors\n2. Map to Alloy discovery (pods, services, endpoints)\n3. Identify any gaps in Alloy coverage\n4. Archive ServiceMonitor configs for reference\n\n**Note:** Alloy doesn't use ServiceMonitor CRDs - it uses Kubernetes native discovery.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T22:26:51.456741-08:00","updated_at":"2025-12-13T22:39:27.01573-08:00","closed_at":"2025-12-13T22:39:27.01573-08:00","labels":["documentation","monitoring"],"dependencies":[{"issue_id":"TALOS-a4s","depends_on_id":"TALOS-zqh","type":"parent-child","created_at":"2025-12-13T22:27:04.712834-08:00","created_by":"daemon"}]}
{"id":"TALOS-aev","title":"Hybrid LLM Cluster Project","description":"Nebula + Liqo + AWS GPU cluster for LLM inference. Multi-phase project:\n\n**Phase 1: Nebula Infrastructure**\n- CA setup, lighthouse deployment, Talos node configuration\n\n**Phase 2: AWS Infrastructure**\n- VPC, EC2 Spot instances, GPU bootstrap\n\n**Phase 3: Liqo Federation**\n- Homelab + AWS peering, namespace offloading\n\n**Phase 4: Ollama Deployment**\n- GPU-aware deployment, model management\n\n**Phase 5: Automation**\n- Scale-to-zero, IaC, monitoring\n\n**Status:** Suspended (catalyst-llm Flux kustomization suspended)\n\nFrom: docs/05-projects/hybrid-llm-cluster/TODO.md","status":"open","priority":4,"issue_type":"epic","created_at":"2025-12-12T22:20:28.632042-08:00","updated_at":"2025-12-12T22:20:28.632042-08:00","labels":["aws","liqo","llm","nebula","suspended"]}
{"id":"TALOS-afk","title":"Audit: Flux kustomizations dashboard coverage","description":"Ensure all Flux kustomizations (16 total) are displayed in infrastructure dashboard with reconcile status","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.949951-08:00","updated_at":"2025-12-12T18:57:04.493613-08:00","closed_at":"2025-12-12T18:57:04.493613-08:00"}
{"id":"TALOS-ag3","title":"Consider Alertmanager → ntfy integration for Prometheus alerts","description":"Evaluate routing Prometheus/Alertmanager critical alerts to ntfy:\n\n- Could catch issues Flux doesn't see (resource exhaustion, app-level errors)\n- Alertmanager has better deduplication and grouping\n- May need alertmanager-ntfy webhook receiver\n\nThis is stretch goal - start with Flux notifications first.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-12T22:11:20.205606-08:00","updated_at":"2025-12-12T22:11:20.205606-08:00","labels":["monitoring","notifications","stretch"],"dependencies":[{"issue_id":"TALOS-ag3","depends_on_id":"TALOS-8uv","type":"parent-child","created_at":"2025-12-12T22:11:30.880208-08:00","created_by":"daemon"}]}
{"id":"TALOS-agw","title":"MinIO Operator - Multi-tenant S3 Storage Management","description":"Deploy MinIO Operator for declarative S3-compatible storage management.\n\n**Goal:** Replace standalone MinIO with operator-managed tenants for better multi-tenant support.\n\n**Operator:** MinIO Operator (Official)\n- Repo: https://github.com/minio/operator\n- Helm: https://operator.min.io\n\n**CRDs Provided:**\n- `Tenant` - MinIO tenant (isolated S3 namespace)\n- `PolicyBinding` - IAM policy bindings\n\n**Features:**\n- Multi-tenant S3 storage\n- Per-tenant resource isolation\n- Automatic TLS via cert-manager\n- Console per tenant\n- Prometheus metrics\n- Bucket notifications\n\n**Migration Path:**\n1. Deploy MinIO Operator alongside existing standalone MinIO\n2. Create Tenant CR for OTEL stack (mimir, tempo, loki buckets)\n3. Migrate Mimir/Tempo/Loki to new tenant endpoints\n4. Decommission standalone MinIO\n\n**Integration:**\n- Storage: fatboy-nfs-appdata (NFS)\n- TLS: cert-manager (homelab-ca-issuer)\n- Monitoring: ServiceMonitor for Prometheus","status":"open","priority":2,"issue_type":"epic","created_at":"2025-12-13T12:29:44.394253-08:00","updated_at":"2025-12-13T12:29:44.394253-08:00","labels":["infrastructure","minio","operator","s3","storage"]}
{"id":"TALOS-akv","title":"Create Talos Image Factory schematic for i915 extension","description":"Create talos02-gpu-schematic.yaml with siderolabs/i915 and siderolabs/intel-ucode extensions for custom Talos image","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T16:53:29.92203-08:00","updated_at":"2025-12-12T17:02:27.932317-08:00","closed_at":"2025-12-12T17:02:27.932317-08:00","dependencies":[{"issue_id":"TALOS-akv","depends_on_id":"TALOS-fpp","type":"parent-child","created_at":"2025-12-12T16:59:22.015116-08:00","created_by":"daemon"}]}
{"id":"TALOS-dcd","title":"Debug Docker Registry HTTP push failures via Traefik","description":"Cannot push images via Traefik ingress (registry.talos00).\n\n**Symptoms:**\n- HTTP blob upload returns 404 via Traefik proxy\n- Push works via kubectl port-forward to localhost:5000\n\n**Current workaround:** Use port-forward\n\nFrom: infrastructure/base/traefik/STATUS.md","status":"open","priority":3,"issue_type":"bug","created_at":"2025-12-12T22:36:53.632278-08:00","updated_at":"2025-12-12T22:36:53.632278-08:00","labels":["bug","registry","traefik"]}
{"id":"TALOS-dcw","title":"CloudNativePG Operator - PostgreSQL Database Management","description":"Deploy CloudNativePG operator for declarative PostgreSQL cluster management.\n\n**Goal:** Enable provisioning PostgreSQL databases via CRDs instead of manual deployments.\n\n**Operator:** CloudNativePG (CNCF project)\n- Repo: https://github.com/cloudnative-pg/cloudnative-pg\n- Helm: https://cloudnative-pg.github.io/charts\n\n**CRDs Provided:**\n- `Cluster` - PostgreSQL cluster (primary + replicas)\n- `Backup` - On-demand backups\n- `ScheduledBackup` - Cron-based backups\n- `Pooler` - PgBouncer connection pooling\n\n**Features:**\n- Declarative cluster provisioning\n- Automated failover\n- Backup to S3 (MinIO)\n- WAL archiving\n- Connection pooling via PgBouncer\n- Prometheus metrics\n\n**Integration:**\n- Storage: local-path or fatboy-nfs-appdata\n- Backups: MinIO S3 bucket\n- Monitoring: ServiceMonitor for Prometheus","status":"open","priority":2,"issue_type":"epic","created_at":"2025-12-13T12:29:44.221867-08:00","updated_at":"2025-12-13T12:29:44.221867-08:00","labels":["database","infrastructure","operator","postgresql"]}
{"id":"TALOS-dfv","title":"Verify GPU device and NFD labels on talos02-gpu","description":"Confirm /dev/dri/renderD128 exists, verify NFD labels applied (intel.feature.node.kubernetes.io/gpu=true), check gpu.intel.com/i915 in node allocatable","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-12T17:02:36.953222-08:00","updated_at":"2025-12-12T17:02:36.953222-08:00","dependencies":[{"issue_id":"TALOS-dfv","depends_on_id":"TALOS-fpp","type":"parent-child","created_at":"2025-12-12T17:02:44.445149-08:00","created_by":"daemon"},{"issue_id":"TALOS-dfv","depends_on_id":"TALOS-y1c","type":"blocks","created_at":"2025-12-12T17:02:44.528192-08:00","created_by":"daemon"}]}
{"id":"TALOS-dv3","title":"Deploy ntfy Provider for Flux notifications","description":"Add ntfy.sh as a generic webhook provider in flux-notifications:\n\n1. Create Provider manifest pointing to ntfy.sh (or self-hosted)\n2. Choose a secure/unique topic name\n3. Apply and verify connectivity\n\nConsider: self-hosted ntfy vs public ntfy.sh (security of topic names)","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-12T22:11:20.061496-08:00","updated_at":"2025-12-12T22:11:20.061496-08:00","labels":["flux","notifications"],"dependencies":[{"issue_id":"TALOS-dv3","depends_on_id":"TALOS-8uv","type":"parent-child","created_at":"2025-12-12T22:11:30.761622-08:00","created_by":"daemon"},{"issue_id":"TALOS-dv3","depends_on_id":"TALOS-26f","type":"blocks","created_at":"2025-12-12T22:11:30.917892-08:00","created_by":"daemon"}]}
{"id":"TALOS-eq8","title":"Create liqo dashboard","description":"infrastructure/base/liqo/dashboard.sh - control plane, foreign clusters, virtual nodes, peering health","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.603828-08:00","updated_at":"2025-12-12T18:55:36.430662-08:00","closed_at":"2025-12-12T18:55:36.430662-08:00"}
{"id":"TALOS-fds","title":"Audit: All namespaces have dashboard coverage","description":"Verify every namespace (25 total) is covered by at least one dashboard section","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:36.014898-08:00","updated_at":"2025-12-12T18:57:04.534848-08:00","closed_at":"2025-12-12T18:57:04.534848-08:00"}
{"id":"TALOS-fpp","title":"Set up talos02-gpu worker node with Intel Arc","description":"Configure new ASUS NUC 15 Pro (RNUC15U5) as GPU-enabled worker node for media transcoding workloads (Plex, Tdarr). Intel Core Ultra 5 225H with integrated Intel Arc GPU (Xe2 architecture).","status":"open","priority":1,"issue_type":"epic","created_at":"2025-12-12T16:53:20.891543-08:00","updated_at":"2025-12-12T16:53:20.891543-08:00"}
{"id":"TALOS-gxu","title":"Create intel-gpu dashboard","description":"infrastructure/base/intel-gpu/dashboard.sh - NFD status, GPU labels, device plugin, allocatable resources","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.722026-08:00","updated_at":"2025-12-12T18:55:36.518316-08:00","closed_at":"2025-12-12T18:55:36.518316-08:00"}
{"id":"TALOS-i11","title":"Add HTTPS/TLS to Traefik with cert-manager","description":"Currently all services are HTTP only. Implement HTTPS:\n\n- Deploy cert-manager\n- Configure Let's Encrypt certificates (or self-signed for internal)\n- Update Traefik IngressRoutes for TLS\n- Add rate limiting middleware\n\nFrom: docs/_archive/TODO.md","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-12T22:20:28.525447-08:00","updated_at":"2025-12-12T22:20:28.525447-08:00","labels":["infrastructure","security","traefik"]}
{"id":"TALOS-jtp","title":"Suspend kube-prometheus-stack HelmRelease","description":"Suspend the HelmRelease before removal to validate metrics flow.\n\n**Steps:**\n1. Suspend HelmRelease (keeps resources but stops reconciliation)\n2. Verify Alloy still scraping kube-state-metrics\n3. Verify Mimir receiving metrics\n4. Monitor for 10-15 minutes\n\n**Rollback:** Unsuspend if issues detected.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-13T22:26:51.539689-08:00","updated_at":"2025-12-13T22:34:55.494869-08:00","closed_at":"2025-12-13T22:34:55.494869-08:00","labels":["migration","monitoring"],"dependencies":[{"issue_id":"TALOS-jtp","depends_on_id":"TALOS-zqh","type":"parent-child","created_at":"2025-12-13T22:27:04.77225-08:00","created_by":"daemon"},{"issue_id":"TALOS-jtp","depends_on_id":"TALOS-ofm","type":"blocks","created_at":"2025-12-13T22:27:04.896508-08:00","created_by":"daemon"}]}
{"id":"TALOS-jzy","title":"Deploy Grafana Loki for log storage","description":"Deploy Loki as lightweight log aggregation system.\n\n**Mode:** Single binary or simple-scalable (start simple)\n\n**Storage:** \n- NFS for chunks (or local-path for eval)\n- 30-day retention to match current\n\n**Integration:**\n- OTEL Collector exports logs via Loki exporter\n- Add Loki datasource to existing Grafana","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T09:20:16.045711-08:00","updated_at":"2025-12-13T22:23:07.468614-08:00","closed_at":"2025-12-13T22:23:07.468614-08:00","labels":["logs","loki","otel"],"dependencies":[{"issue_id":"TALOS-jzy","depends_on_id":"TALOS-nh8","type":"parent-child","created_at":"2025-12-13T09:20:28.256726-08:00","created_by":"daemon"},{"issue_id":"TALOS-jzy","depends_on_id":"TALOS-3of","type":"blocks","created_at":"2025-12-13T09:20:28.471317-08:00","created_by":"daemon"}]}
{"id":"TALOS-m0x","title":"Create Mac provisioning for ntfy-desktop (Homebrew/Ansible)","description":"Create automated installation for ntfy-desktop on developer Macs:\n\n**Options to evaluate:**\n1. **Homebrew Cask** - Check if ntfy-desktop has one, or create a custom tap\n2. **Ansible role** - Add to @machines bootstrap playbook\n3. **Shell script** - Simple curl/install script in dotfiles\n\n**Should include:**\n- Download correct binary (darwin-arm64 vs darwin-amd64)\n- Install to /Applications or ~/Applications\n- Pre-configure with cluster topic subscription\n- Set up launch at login (optional)\n- Configure notification preferences\n\n**Integration points:**\n- `catalyst/@machines` - Ansible bootstrap\n- `catalyst/@dotfiles-2024` - Brewfile or install script\n- Document in workspace onboarding docs","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-12T22:16:36.775827-08:00","updated_at":"2025-12-12T22:16:36.775827-08:00","labels":["devex","macos","notifications","provisioning"],"dependencies":[{"issue_id":"TALOS-m0x","depends_on_id":"TALOS-8uv","type":"parent-child","created_at":"2025-12-12T22:16:43.086097-08:00","created_by":"daemon"},{"issue_id":"TALOS-m0x","depends_on_id":"TALOS-v3a","type":"blocks","created_at":"2025-12-12T22:16:43.110579-08:00","created_by":"daemon"}]}
{"id":"TALOS-mhk","title":"Deploy Grafana Tempo for distributed tracing","description":"Deploy Tempo for trace storage - NEW capability!\n\n**Mode:** Single binary (monolithic for homelab scale)\n\n**Storage:** Local or NFS\n\n**Integration:**\n- OTEL Collector exports traces via OTLP\n- Add Tempo datasource to existing Grafana\n- Enable trace-to-logs correlation","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T09:20:16.101749-08:00","updated_at":"2025-12-13T22:23:07.522863-08:00","closed_at":"2025-12-13T22:23:07.522863-08:00","labels":["otel","tempo","traces"],"dependencies":[{"issue_id":"TALOS-mhk","depends_on_id":"TALOS-nh8","type":"parent-child","created_at":"2025-12-13T09:20:28.309172-08:00","created_by":"daemon"},{"issue_id":"TALOS-mhk","depends_on_id":"TALOS-3of","type":"blocks","created_at":"2025-12-13T09:20:28.52501-08:00","created_by":"daemon"}]}
{"id":"TALOS-nh8","title":"OpenTelemetry Stack Evaluation - LGTM Migration","description":"Deploy OpenTelemetry-based observability stack alongside existing monitoring for evaluation.\n\n**Goal:** Evaluate OTEL Collector + Loki + Tempo as replacement for Fluent Bit + OpenSearch + Graylog\n\n**Architecture:**\n```\nApps/Claude Code ──OTLP──► OTEL Collector ──► Loki (logs)\n                                          ──► Tempo (traces)\n                                          ──► Prometheus (metrics, existing)\n                                                    │\n                                                    ▼\n                                              Grafana (existing)\n```\n\n**What's NEW:**\n- OTEL Collector (unified collection)\n- Loki (lightweight log storage, replaces OpenSearch+Graylog)\n- Tempo (distributed tracing - new capability!)\n\n**What's KEPT:**\n- Prometheus (metrics backend)\n- Grafana (add Loki+Tempo datasources)\n- Alertmanager\n\n**What's REPLACED (after evaluation):**\n- Fluent Bit → OTEL Collector\n- OpenSearch → Loki\n- Graylog → Grafana (LogQL)\n\n**Namespace:** `monitoring` (extend existing) or `otel` (isolated test)\n\n**Success Criteria:**\n- Claude Code metrics flowing via OTLP\n- Logs queryable in Grafana via Loki\n- Traces visible in Grafana via Tempo\n- Compare resource usage vs current stack","design":"**V2 Stack Architecture (Operator-Based):**\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                     V2-OTEL STACK                                │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                  │\n│  OPERATORS (CRD Providers):                                     │\n│  ├── OTEL Operator      → OpenTelemetryCollector, Instrumentation│\n│  ├── Tempo Operator     → TempoStack, TempoMonolithic           │\n│  ├── Grafana Operator   → GrafanaDashboard, GrafanaDatasource   │\n│  └── Prometheus Operator → ServiceMonitor, PrometheusRule       │\n│                                                                  │\n│  DATA PLANE:                                                    │\n│  ├── Alloy             → Unified collector (metrics/logs/traces)│\n│  ├── Mimir             → Metrics backend (replaces Prometheus)  │\n│  ├── Loki              → Logs backend                           │\n│  └── Tempo             → Traces backend (via Tempo Operator)    │\n│                                                                  │\n│  VISUALIZATION:                                                 │\n│  └── Grafana           → Via Grafana Operator CRs               │\n│                                                                  │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n**Directory Structure:**\n```\ninfrastructure/base/monitoring/\n├── shared/\n│   ├── namespace.yaml\n│   └── grafana-operator/\n├── v1/\n│   └── kube-prometheus-stack/\n└── v2-otel/\n    ├── operators/\n    │   ├── otel-operator/\n    │   └── tempo-operator/\n    ├── mimir/\n    ├── loki/\n    ├── tempo/\n    ├── alloy/\n    ├── grafana-datasources/\n    └── grafana-dashboards/\n```","notes":"V2 OTEL Stack - FULLY DEPLOYED\n\n**Base Infrastructure (commits 097c203 - 186980a):**\n\ncert-manager:\n- Version: v1.16.2\n- ClusterIssuers: selfsigned-issuer, homelab-ca-issuer\n- Status: Running\n\nMinIO:\n- Version: v5.2.0\n- Storage: fatboy-nfs-appdata (NFS)\n- Buckets: mimir (with alertmanager/, ruler/ prefixes)\n- Endpoints: minio.talos00 (console), s3.talos00 (API)\n- Status: Running\n\n**Operators:**\n- OTEL Operator: Running\n- Tempo Operator: Running (in monitoring namespace)\n- Grafana Operator: Running\n\n**Data Plane:**\n- Mimir: All 14 components running\n  - Storage: MinIO S3 backend\n  - Separate prefixes for blocks, alertmanager, ruler\n- Loki: Running (3 pods)\n- Tempo: Running (Helm chart)\n- Alloy: Running\n  - Metrics -\u003e Mimir (OTLP HTTP)\n  - Logs -\u003e Loki\n  - Traces -\u003e Tempo\n\n**Grafana Datasources:**\n- Mimir (primary, default)\n- Prometheus (ServiceMonitor metrics)\n- Loki\n- Tempo (with correlations to Mimir/Loki)\n\n**V1 Stack:**\n- monitoring.yaml: suspended\n- observability.yaml: suspended\n- kube-prometheus-stack: resumed for ServiceMonitor scrapes\n\nStack is fully operational and ready for testing.","status":"in_progress","priority":2,"issue_type":"epic","created_at":"2025-12-13T09:19:56.282179-08:00","updated_at":"2025-12-13T12:10:21.728511-08:00","labels":["loki","monitoring","observability","otel","tempo"]}
{"id":"TALOS-nsj","title":"Create linkerd dashboard","description":"infrastructure/base/linkerd/dashboard.sh - control plane, viz, meshed namespaces, linkerd check","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.658563-08:00","updated_at":"2025-12-12T18:55:36.473441-08:00","closed_at":"2025-12-12T18:55:36.473441-08:00"}
{"id":"TALOS-ofm","title":"Deploy standalone kube-state-metrics","description":"Deploy kube-state-metrics as standalone HelmRelease in v2-otel stack.\n\n**Helm chart:** kube-state-metrics (from prometheus-community)\n**Namespace:** monitoring\n**Key config:**\n- Enable all collectors\n- Add prometheus.io/scrape annotation for Alloy discovery\n- Resource limits appropriate for homelab","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-13T22:26:51.323392-08:00","updated_at":"2025-12-13T22:33:51.622814-08:00","closed_at":"2025-12-13T22:33:51.622814-08:00","labels":["kube-state-metrics","monitoring"],"dependencies":[{"issue_id":"TALOS-ofm","depends_on_id":"TALOS-zqh","type":"parent-child","created_at":"2025-12-13T22:27:04.582015-08:00","created_by":"daemon"}]}
{"id":"TALOS-pkz","title":"Fix ArgoCD catalyst-ui sync failure","description":"ArgoCD application for catalyst-ui is defined but not syncing.\n\n**Symptoms:**\n- Application shows as \"not syncing\"\n- Likely repo access or image pull issue\n\n**Investigation needed:**\n- Check repository credentials\n- Test manual sync via CLI\n- Check ArgoCD application-controller logs\n\nFrom: infrastructure/base/argocd/STATUS.md","status":"open","priority":3,"issue_type":"bug","created_at":"2025-12-12T22:36:53.584946-08:00","updated_at":"2025-12-12T22:36:53.584946-08:00","labels":["argocd","bug","catalyst-ui"]}
{"id":"TALOS-pnj","title":"Catalyst DNS Sync - Kubernetes DNS Controller","description":"Go-based Kubernetes controller that syncs Traefik IngressRoutes to Technitium DNS server.\n\n**Current Status:** Phase 1 MVP in progress (~70% complete)\n\n**Phase 1 MVP (Current):**\n- Core CRUD DNS sync (watch Ingress/IngressRoute, create/update/delete A records)\n- Prometheus metrics endpoint\n- Health probes (/healthz, /readyz)\n- Dev mode (updates /etc/hosts instead of DNS)\n- Kubernetes deployment manifests\n\n**Phase 2 (Future):**\n- Web UI Dashboard at dns.talos00/ui\n- DNS test endpoint\n- Certificate expiration tracking\n\n**Location:** catalyst-dns-sync/ (in this repo)\n\nDetailed spec: docs/05-projects/catalyst-dns-sync/proposal.md","status":"open","priority":3,"issue_type":"epic","created_at":"2025-12-12T22:36:53.539634-08:00","updated_at":"2025-12-12T22:36:53.539634-08:00","labels":["controller","dns","go","infrastructure"]}
{"id":"TALOS-q4s","title":"Configure Flux Alert CRDs for critical events only","description":"Create Alert manifests that filter for critical/error events only:\n\n- eventSeverity: error (not info)\n- Specific event sources (not wildcards initially)\n- Test with intentional failures to verify signal vs noise\n\nBased on findings from investigation task.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-12T22:11:20.109228-08:00","updated_at":"2025-12-12T22:11:20.109228-08:00","labels":["flux","notifications"],"dependencies":[{"issue_id":"TALOS-q4s","depends_on_id":"TALOS-8uv","type":"parent-child","created_at":"2025-12-12T22:11:30.807663-08:00","created_by":"daemon"},{"issue_id":"TALOS-q4s","depends_on_id":"TALOS-26f","type":"blocks","created_at":"2025-12-12T22:11:30.959557-08:00","created_by":"daemon"},{"issue_id":"TALOS-q4s","depends_on_id":"TALOS-dv3","type":"blocks","created_at":"2025-12-12T22:11:31.0012-08:00","created_by":"daemon"}]}
{"id":"TALOS-sy4","title":"Remove kube-prometheus-stack and cleanup CRDs","description":"Final removal of kube-prometheus-stack.\n\n**Steps:**\n1. Delete HelmRelease\n2. Delete ServiceMonitors (orphaned CRs)\n3. Delete PrometheusRules (orphaned CRs)\n4. Optionally remove prometheus-operator CRDs\n5. Clean up any leftover PVCs\n6. Update Flux kustomization\n\n**Cleanup:**\n- Remove monitoring.yaml Flux Kustomization or update path\n- Archive old configs in docs/","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-13T22:26:51.613162-08:00","updated_at":"2025-12-13T22:39:00.430438-08:00","closed_at":"2025-12-13T22:39:00.430438-08:00","labels":["cleanup","monitoring"],"dependencies":[{"issue_id":"TALOS-sy4","depends_on_id":"TALOS-zqh","type":"parent-child","created_at":"2025-12-13T22:27:04.831848-08:00","created_by":"daemon"},{"issue_id":"TALOS-sy4","depends_on_id":"TALOS-jtp","type":"blocks","created_at":"2025-12-13T22:27:04.964314-08:00","created_by":"daemon"}]}
{"id":"TALOS-t7u","title":"Configure Claude Code OTEL export to cluster","description":"Set up local Mac environment to export Claude Code telemetry to cluster.\n\n**Environment variables:**\n```bash\nexport CLAUDE_CODE_ENABLE_TELEMETRY=1\nexport OTEL_METRICS_EXPORTER=otlp\nexport OTEL_LOGS_EXPORTER=otlp\nexport OTEL_EXPORTER_OTLP_ENDPOINT=http://otel.talos00:4318\n```\n\n**Add to:** ~/.zshrc or shell profile\n\n**Verify:** Metrics appear in Grafana","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-13T09:20:16.222178-08:00","updated_at":"2025-12-13T09:20:16.222178-08:00","labels":["claude-code","macos","otel"],"dependencies":[{"issue_id":"TALOS-t7u","depends_on_id":"TALOS-nh8","type":"parent-child","created_at":"2025-12-13T09:20:28.422174-08:00","created_by":"daemon"},{"issue_id":"TALOS-t7u","depends_on_id":"TALOS-3of","type":"blocks","created_at":"2025-12-13T09:20:28.577573-08:00","created_by":"daemon"}]}
{"id":"TALOS-v3a","title":"Set up ntfy-desktop client on Mac","description":"Install and configure ntfy-desktop on macOS:\n\n1. Download from https://github.com/Aetherinox/ntfy-desktop/releases (darwin-arm64)\n2. Subscribe to the cluster topic\n3. Configure notification preferences (sound, badge, etc.)\n4. Test end-to-end with a manual ntfy publish","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-12T22:11:20.154666-08:00","updated_at":"2025-12-12T22:11:20.154666-08:00","labels":["macos","notifications"],"dependencies":[{"issue_id":"TALOS-v3a","depends_on_id":"TALOS-8uv","type":"parent-child","created_at":"2025-12-12T22:11:30.842375-08:00","created_by":"daemon"},{"issue_id":"TALOS-v3a","depends_on_id":"TALOS-dv3","type":"blocks","created_at":"2025-12-12T22:11:31.042716-08:00","created_by":"daemon"}]}
{"id":"TALOS-w1k","title":"Complete Tilt development workflow integration","description":"Finish Tilt extensions integration (currently ~60%):\n\n**Remaining:**\n- [ ] Test `tilt up` end-to-end\n- [ ] Consider namespace extension for dev isolation\n- [ ] Optimize build/deploy cycles\n- [ ] Team training documentation\n\n**Completed:**\n- helm_resource, k8s_attach, uibutton, dotenv\n\nFrom: docs/06-project-management/enhancement-roadmap.md (Stream 2)","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-12T22:20:28.74797-08:00","updated_at":"2025-12-12T22:20:28.74797-08:00","labels":["devex","tilt"]}
{"id":"TALOS-w6g","title":"MongoDB Community Operator - MongoDB Database Management","description":"Deploy MongoDB Community Operator for declarative MongoDB replica set management.\n\n**Goal:** Enable provisioning MongoDB databases via CRDs instead of manual deployments.\n\n**Operator:** MongoDB Community Operator (Official)\n- Repo: https://github.com/mongodb/mongodb-kubernetes-operator\n- Helm: https://mongodb.github.io/helm-charts\n\n**CRDs Provided:**\n- `MongoDBCommunity` - MongoDB replica set\n\n**Features:**\n- Declarative replica set provisioning\n- Automated member management\n- TLS encryption\n- SCRAM authentication\n- Prometheus metrics via mongodb-exporter\n\n**Integration:**\n- Storage: local-path or fatboy-nfs-appdata\n- Monitoring: ServiceMonitor for Prometheus\n- Potential users: Graylog (currently using manual MongoDB)","status":"open","priority":2,"issue_type":"epic","created_at":"2025-12-13T12:29:44.304084-08:00","updated_at":"2025-12-13T12:29:44.304084-08:00","labels":["database","infrastructure","mongodb","operator"]}
{"id":"TALOS-wbb","title":"Reorganize Talos config directory structure","description":"Move machine configs to configs/nodes/, create patches/ directory, add README.md documentation","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T16:53:29.837756-08:00","updated_at":"2025-12-12T17:02:27.81464-08:00","closed_at":"2025-12-12T17:02:27.81464-08:00","dependencies":[{"issue_id":"TALOS-wbb","depends_on_id":"TALOS-fpp","type":"parent-child","created_at":"2025-12-12T16:59:14.468368-08:00","created_by":"daemon"}]}
{"id":"TALOS-xnh","title":"Dashboard System Audit \u0026 Refactor","description":"Restore broken dashboard library, create domain dashboards (nebula, liqo, linkerd, intel-gpu), build root orchestrator with inline/interactive modes, audit all deployed components for dashboard coverage","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-12T18:40:14.361907-08:00","updated_at":"2025-12-12T18:57:04.576917-08:00","closed_at":"2025-12-12T18:57:04.576917-08:00"}
{"id":"TALOS-y1c","title":"Boot talos02-gpu with custom Talos image","description":"Generate custom Talos ISO from factory.talos.dev with i915 extension, boot NUC, apply machine config","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-12T17:02:36.902399-08:00","updated_at":"2025-12-12T17:02:36.902399-08:00","dependencies":[{"issue_id":"TALOS-y1c","depends_on_id":"TALOS-fpp","type":"parent-child","created_at":"2025-12-12T17:02:44.406656-08:00","created_by":"daemon"}]}
{"id":"TALOS-zqh","title":"Remove kube-prometheus-stack and migrate to V2 OTEL","description":"Migrate from kube-prometheus-stack to pure V2 OTEL stack (Alloy + Mimir + Loki + Tempo).\n\n**Components to remove:**\n- Prometheus (replaced by Mimir)\n- Alertmanager (Mimir has built-in)\n- prometheus-operator (Alloy uses native discovery)\n- Bundled Grafana (using grafana-operator)\n\n**Components to migrate:**\n- kube-state-metrics → standalone deployment\n- 32 PrometheusRules → Mimir ruler format\n- 56 ServiceMonitors → Document/archive (Alloy uses native discovery)\n\n**Success criteria:**\n- Standalone kube-state-metrics running\n- Alerting rules migrated to Mimir\n- kube-prometheus-stack HelmRelease removed\n- Metrics still flowing to Mimir","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-13T22:26:29.825365-08:00","updated_at":"2025-12-13T22:39:32.683209-08:00","closed_at":"2025-12-13T22:39:32.683209-08:00","labels":["migration","monitoring","otel"]}
{"id":"TALOS-zut","title":"Create nebula dashboard","description":"infrastructure/base/nebula/dashboard.sh - mesh nodes, lighthouse, AWS worker connectivity, overlay IPs","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.557884-08:00","updated_at":"2025-12-12T18:55:36.384594-08:00","closed_at":"2025-12-12T18:55:36.384594-08:00"}
