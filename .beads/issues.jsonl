{"id":"TALOS-01z","title":"Add node-exporter scraping to Alloy","description":"Node-exporter metrics (node_*) are not present in Mimir. Need to add scrape config to Alloy for node-exporter. This blocks several dashboards including node-exporter-full.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-14T11:32:25.667044-08:00","updated_at":"2025-12-14T12:10:41.751938-08:00","closed_at":"2025-12-14T12:10:41.751938-08:00","labels":["alloy","infrastructure","scraping"]}
{"id":"TALOS-07y","title":"Deploy Readarr service for homepage widget","description":"Homepage Readarr widget shows API error because readarr.media.svc.cluster.local doesn't exist (ENOTFOUND).\n\nEither:\n1. Deploy Readarr to the media namespace\n2. Or remove Readarr from homepage services config if not needed","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-15T17:48:25.764419-08:00","updated_at":"2025-12-15T17:48:25.764419-08:00","labels":["arr-stack","homepage","readarr"]}
{"id":"TALOS-084","title":"Audit and convert PrometheusRules to Mimir ruler format","description":"Export 32 PrometheusRules and convert to Mimir ruler format.\n\n**Steps:**\n1. Export all PrometheusRules to YAML\n2. Identify critical vs nice-to-have rules\n3. Convert to Mimir ruler YAML format\n4. Deploy as ConfigMaps for Mimir ruler\n5. Verify rules are loaded in Mimir\n\n**Mimir ruler format:**\nRules stored in object storage (MinIO) or ConfigMaps, same PromQL syntax.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-13T22:26:51.384263-08:00","updated_at":"2025-12-13T22:39:26.956629-08:00","closed_at":"2025-12-13T22:39:26.956629-08:00","labels":["alerting","mimir","monitoring"],"dependencies":[{"issue_id":"TALOS-084","depends_on_id":"TALOS-zqh","type":"parent-child","created_at":"2025-12-13T22:27:04.646641-08:00","created_by":"daemon"}]}
{"id":"TALOS-0hlm","title":"One Dashboard to Rule Them All - Unified Grafana Metrics View","description":"Enhance or create a master Grafana dashboard that provides a unified view of top-level metrics across the entire cluster with flexible filtering by service, pod, namespace, or all resources.\n\n## Goals\n- Single pane of glass for cluster health overview\n- Drill-down capability from cluster → namespace → service → pod\n- Consistent metrics presentation across all workloads\n- Quick identification of resource hogs and anomalies\n\n## Approach\n- **Option A**: Enhance existing high-level dashboard with new filtering/drill-down capabilities\n- **Option B**: Create new unified dashboard if existing one is too specialized\n\n## Key Features\n- **Top-level overview**: CPU, memory, network, disk I/O across cluster\n- **Filtering**: By namespace, service, pod, node\n- **Service health**: Request rates, error rates, latency (RED metrics)\n- **Resource usage**: Requests vs limits vs actual usage\n- **Anomaly highlighting**: Visual indicators for pods exceeding thresholds\n\n## Potential Sub-tasks\n- Audit existing dashboards to identify enhancement opportunities\n- Design dashboard layout and panel structure\n- Create Prometheus queries for unified metrics\n- Implement variable-based filtering (namespace, service, pod dropdowns)\n- Add threshold-based alerting indicators\n- Create drill-down links to detailed dashboards","status":"open","priority":2,"issue_type":"epic","created_at":"2025-12-19T21:56:45.535491-08:00","updated_at":"2025-12-19T21:56:59.325449-08:00","labels":["dashboards","grafana","observability"]}
{"id":"TALOS-0nb","title":"Upgrade dependency graph layout engine from Dagre to ELK","description":"The current Dagre layout doesn't handle compound/grouped graphs well. ELK (Eclipse Layout Kernel) has native support for:\n- Compound graphs (parent nodes containing children)\n- Hierarchical layouts optimized for dependency graphs\n- Better edge routing\n- Port-based connections\n\nThis will improve the Epic grouping feature and overall graph readability.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T11:32:19.326226-08:00","updated_at":"2025-12-16T11:36:38.292299-08:00","closed_at":"2025-12-16T11:36:38.292299-08:00","labels":["beads-manager","react-flow","ui"]}
{"id":"TALOS-11j","title":"Phase 4: Apply Pod Security Standards to media namespace","description":"Enforce Pod Security Standards (PSS) at namespace level.\n\n## Tasks\n- [ ] Start with `baseline` profile (warn mode)\n- [ ] Fix violations identified\n- [ ] Upgrade to `baseline` enforce mode\n- [ ] Evaluate `restricted` profile feasibility\n- [ ] Document exceptions\n\n## Namespace Labels\n```yaml\nmetadata:\n  labels:\n    pod-security.kubernetes.io/enforce: baseline\n    pod-security.kubernetes.io/warn: restricted\n    pod-security.kubernetes.io/audit: restricted\n```","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-17T17:53:45.672989-08:00","updated_at":"2025-12-17T17:53:45.672989-08:00","labels":["pss","security"],"dependencies":[{"issue_id":"TALOS-11j","depends_on_id":"TALOS-wlu","type":"parent-child","created_at":"2025-12-17T17:54:02.699317-08:00","created_by":"daemon"},{"issue_id":"TALOS-11j","depends_on_id":"TALOS-za1","type":"blocks","created_at":"2025-12-17T17:54:12.091774-08:00","created_by":"daemon"}]}
{"id":"TALOS-1ed","title":"Fix dashboard: liqo-overview","description":"Assess and fix/replace liqo-overview dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:54.362186-08:00","updated_at":"2025-12-14T11:31:54.362186-08:00","labels":["dashboard","hybrid-cluster"],"dependencies":[{"issue_id":"TALOS-1ed","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:54.363274-08:00","created_by":"daemon"}]}
{"id":"TALOS-1ji","title":"Fix dashboard: cilium-hubble","description":"Assess and fix/replace cilium-hubble dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:30:49.024359-08:00","updated_at":"2025-12-14T17:59:15.215558-08:00","closed_at":"2025-12-14T17:59:15.215558-08:00","labels":["dashboard","networking"],"dependencies":[{"issue_id":"TALOS-1ji","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:30:49.025483-08:00","created_by":"daemon"}]}
{"id":"TALOS-1js","title":"Detail panel shows filtered-out issue when filters applied","description":"**Bug**: When a status filter is applied, the detail panel continues showing a previously selected issue even if that issue is now filtered out of the graph view.\n\n**Steps to reproduce**:\n1. Click on a \"Closed\" issue to open detail panel\n2. Click \"Open\" status filter\n3. Graph now shows only open issues\n4. Detail panel still shows the closed issue\n\n**Expected**: Either clear the detail panel, or show a message like \"Selected issue is filtered out\"\n**Actual**: Shows stale/confusing data\n\n**Severity**: Low - UX confusion but not blocking","status":"closed","priority":3,"issue_type":"bug","created_at":"2025-12-16T10:09:39.409371-08:00","updated_at":"2025-12-16T10:27:23.273586-08:00","closed_at":"2025-12-16T10:27:23.273586-08:00","labels":["beads-manager","ui","ux"]}
{"id":"TALOS-1kv","title":"WebSocket connection instability - repeated connect/disconnect cycle","description":"**Bug**: WebSocket connection shows repeated connect/disconnect/reconnect cycle in console.\n\n**Console messages observed**:\n```\n[WS] Connected\n[WS] Disconnected\n[WS] Reconnecting...\n[WS] Connected\n[WS] Disconnected\n...\n```\n\n**Location**: `src/client/hooks/useWebSocket.ts` lines 16, 20, 23, 28\n\n**Impact**: May cause unnecessary re-renders, potential data sync issues, or performance degradation.\n\n**Investigation needed**: Check if this is a server-side timeout issue, client reconnect logic, or Vite HMR interaction.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-16T10:09:39.288891-08:00","updated_at":"2025-12-16T10:24:47.020339-08:00","closed_at":"2025-12-16T10:24:47.020339-08:00","labels":["beads-manager","ui","websocket"]}
{"id":"TALOS-21a","title":"Fix dashboard: linkerd-deployment","description":"Assess and fix/replace linkerd-deployment dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:04.122035-08:00","updated_at":"2025-12-14T11:31:04.122035-08:00","labels":["dashboard","networking"],"dependencies":[{"issue_id":"TALOS-21a","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:04.123083-08:00","created_by":"daemon"}]}
{"id":"TALOS-21j","title":"Fix Plex widget API key","description":"Verify Plex widget is working. May need API key (HOMEPAGE_VAR_PLEX_KEY) configured.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-15T15:48:20.910477-08:00","updated_at":"2025-12-17T08:21:24.132683-08:00","closed_at":"2025-12-17T08:21:24.132683-08:00","labels":["homepage","media"]}
{"id":"TALOS-22p","title":"Phase 1: Network Segmentation with CiliumNetworkPolicies","description":"Create CiliumNetworkPolicies for media namespace microsegmentation.\n\n## Tasks\n- [ ] Create default-deny ingress policy for media namespace\n- [ ] Create PostgreSQL isolation policy (only arr-stack pods)\n- [ ] Create Traefik → services allow policy\n- [ ] Create egress policy for external API access (TMDB, TVDB, etc)\n- [ ] Test policies don't break existing functionality\n\n## Files to Create\n- applications/arr-stack/base/networkpolicies/default-deny.yaml\n- applications/arr-stack/base/networkpolicies/postgresql-isolation.yaml\n- applications/arr-stack/base/networkpolicies/traefik-ingress.yaml\n- applications/arr-stack/base/networkpolicies/external-egress.yaml","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-17T17:53:39.839974-08:00","updated_at":"2025-12-17T17:53:39.839974-08:00","labels":["network","security"],"dependencies":[{"issue_id":"TALOS-22p","depends_on_id":"TALOS-wlu","type":"parent-child","created_at":"2025-12-17T17:53:58.976798-08:00","created_by":"daemon"}]}
{"id":"TALOS-26f","title":"Investigate notification event sources and filtering strategy","description":"Research and document what cluster events we want to be notified about. Consider:\n\n**Flux Events:**\n- GitRepository sync failures\n- Kustomization reconciliation errors\n- HelmRelease failures/rollbacks\n\n**Kubernetes Events:**\n- Pod CrashLoopBackOff\n- Node NotReady\n- PVC binding failures\n- OOMKilled containers\n\n**Prometheus Alerts:**\n- Critical severity alerts from Alertmanager\n\n**Notification Classes (to investigate):**\n- Could we tier notifications? e.g. critical (immediate), warning (batched), info (daily digest)\n- Different delivery channels per class? (Mac push vs Discord vs email)\n- ntfy supports priorities (1-5) and tags - could map to classes\n- Alertmanager has severity labels - could route differently\n\n**Questions to answer:**\n- Use Flux notifications directly or route through Alertmanager?\n- What severity threshold? (error only vs info)\n- Should we deduplicate repeated failures?\n- Rate limiting to prevent notification spam?\n- What notification class taxonomy makes sense for this homelab?","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-12T22:11:20.014472-08:00","updated_at":"2025-12-12T22:12:15.310425-08:00","labels":["notifications","research"],"dependencies":[{"issue_id":"TALOS-26f","depends_on_id":"TALOS-8uv","type":"parent-child","created_at":"2025-12-12T22:11:30.722454-08:00","created_by":"daemon"}]}
{"id":"TALOS-2b2","title":"Create GPU worker node config (talos02-gpu)","description":"Create worker-talos02-gpu.yaml with GPU-specific settings: kernel modules (i915, xe), udev rules for /dev/dri, node labels for GPU scheduling","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T16:53:29.877213-08:00","updated_at":"2025-12-12T17:02:27.885562-08:00","closed_at":"2025-12-12T17:02:27.885562-08:00","dependencies":[{"issue_id":"TALOS-2b2","depends_on_id":"TALOS-fpp","type":"parent-child","created_at":"2025-12-12T16:59:21.940521-08:00","created_by":"daemon"}]}
{"id":"TALOS-2ez","title":"MemeX Discourse Pipeline - Kubernetes Deployment","description":"Deploy MemeX Dagster ML pipeline to talos-homelab Kubernetes cluster.\n\n**Project Overview:**\nMemeX is a Discourse Extraction ML Pipeline that processes documents through:\n- Entity extraction (spaCy NER)\n- Semantic embeddings (SentenceTransformers)\n- Proposition extraction\n- Knowledge graph construction (Neo4j)\n\n**Source:** `../workspace/@memeX` (Docker Compose currently, needs K8s manifests)\n\n**Infrastructure Requirements:**\n\n| Component | Technology | Purpose |\n|-----------|-----------|---------|\n| PostgreSQL 14 | Primary DB | Documents, entities, embeddings |\n| Neo4j 5.15 | Graph DB | Knowledge graph (APOC + GDS) |\n| Redis 7 | Cache | Pipeline caching |\n| Dagster | Orchestration | Job scheduling, asset management |\n| Django API | REST API | Query interface |\n\n**Key Dagster Jobs:**\n- `memex_extraction_job` - Full document → knowledge graph pipeline\n- `congressional_full_pipeline_job` - Congress.gov bill analysis\n- `reddit_user_activity_job` - Reddit data processing\n- Scheduled: Daily congressional ingestion, weekly full pipeline\n\n**Shared Infrastructure with TALOS-aev:**\n- Could share GPU node for embedding generation\n- Similar pattern to Ollama deployment (ML workloads)\n- Prometheus/Grafana integration for monitoring\n\n**Phases:**\n1. Create K8s manifests (Deployments, StatefulSets, Services)\n2. Deploy databases (PostgreSQL, Neo4j, Redis)\n3. Deploy Dagster webserver + daemon\n4. Deploy Django API\n5. Configure Flux GitOps\n6. Integrate with monitoring stack","design":"**Architecture Decisions:**\n\n1. **PostgreSQL**: CloudNativePG operator\n   - Deploy CNPG operator to cluster first\n   - HA PostgreSQL cluster with automated backups\n   - Separate `Cluster` CR for MemeX database\n\n2. **Storage**: NFS (NAS)\n   - PostgreSQL and Neo4j PVCs on NFS storage class\n   - Durable, survives node failures\n   - Configure NFS StorageClass if not exists\n\n3. **GPU Acceleration**: Optional with node affinity\n   - Embedding jobs prefer GPU node (`talos02-gpu`) when available\n   - Fallback to CPU on any node\n   - Use `preferredDuringSchedulingIgnoredDuringExecution` affinity\n\n4. **Ingress Hostnames**:\n   - `memex.talos00` - Django REST API\n   - `dagster.talos00` - Dagster webserver UI\n   - `neo4j.talos00` - Neo4j browser (optional)\n\n**Namespace:** `memex` (dedicated)\n\n**Dependencies:**\n- TALOS-fpp (GPU node) - soft dependency for embedding acceleration\n- NFS StorageClass configured\n- CloudNativePG operator deployed","status":"open","priority":3,"issue_type":"epic","created_at":"2025-12-12T22:50:02.371086-08:00","updated_at":"2025-12-12T22:52:31.581867-08:00","labels":["dagster","memex","ml-pipeline","neo4j","nlp"],"dependencies":[{"issue_id":"TALOS-2ez","depends_on_id":"TALOS-aev","type":"related","created_at":"2025-12-12T22:50:10.453655-08:00","created_by":"daemon"}]}
{"id":"TALOS-2fm","title":"Fix dashboard: aws-ec2-instances","description":"Assess and fix/replace aws-ec2-instances dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:54.197674-08:00","updated_at":"2025-12-14T11:31:54.197674-08:00","labels":["dashboard","hybrid-cluster"],"dependencies":[{"issue_id":"TALOS-2fm","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:54.198745-08:00","created_by":"daemon"}]}
{"id":"TALOS-2kn","title":"Implement group-by-Epic layout for dependency graph","description":"**Feature Request**: Implement visual grouping of issues by their parent Epic in the dependency graph.\n\n**Implementation ideas**:\n- Group nodes visually by Epic (colored regions, bounding boxes, or clusters)\n- Epic nodes could be collapsible containers\n- Use React Flow's grouping/subflow features\n- Maintain dependency edges across Epic boundaries\n\n**Benefits**:\n- Better visual organization for large issue sets\n- Easier to understand project structure at a glance\n- Natural hierarchy visualization\n\n**Related to**: Filter by Epic feature (implement filtering first)","design":"## Implementation Design\n\n**Approach**: Use React Flow's parent/group node feature to visually cluster issues by Epic.\n\n### Key Changes:\n\n1. **Identify Epic Groups**\n   - Find all issues with `issue_type === 'epic'`\n   - For each Epic, find children via `parent-child` dependency\n   - Issues without an Epic parent go in an \"Ungrouped\" section\n\n2. **Create Group Nodes**\n   - Epic becomes a group node (`type: 'group'`)\n   - Children have `parentId` set to their Epic's ID\n   - Group nodes have colored backgrounds (purple for Epic theme)\n\n3. **Layout Strategy**\n   - Use separate Dagre layouts per group\n   - Position groups in a grid/row layout\n   - Child positions are relative to group parent\n\n4. **Visual Design**\n   - Epic group: semi-transparent purple background, rounded corners\n   - Epic title displayed at top of group\n   - Child count badge\n   - Ungrouped issues in a separate gray region\n\n5. **Toggle Feature**\n   - Add \"Group by Epic\" toggle in legend\n   - When disabled, show flat layout (current behavior)\n   - When enabled, show grouped layout","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-16T10:09:39.747638-08:00","updated_at":"2025-12-16T12:08:59.139107-08:00","closed_at":"2025-12-16T12:08:59.139107-08:00","labels":["beads-manager","feature","react-flow","ui"],"dependencies":[{"issue_id":"TALOS-2kn","depends_on_id":"TALOS-t3p","type":"blocks","created_at":"2025-12-16T10:09:56.376549-08:00","created_by":"daemon"}]}
{"id":"TALOS-2v5","title":"Fix Grafana widget - update service URL","description":"Update Grafana widget URL from kube-prometheus-stack-grafana to grafana-service.monitoring.svc.cluster.local:3000","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-15T15:48:19.695085-08:00","updated_at":"2025-12-15T16:02:07.399813-08:00","closed_at":"2025-12-15T16:02:07.399813-08:00","labels":["homepage","monitoring"]}
{"id":"TALOS-2xd","title":"Phase 4: Enable Cilium WireGuard encryption for pod-to-pod","description":"Enable transparent pod-to-pod encryption using Cilium's WireGuard integration.\n\n## Tasks\n- [ ] Verify Cilium version supports encryption\n- [ ] Enable encryption in Cilium HelmRelease\n- [ ] Test pod-to-pod traffic is encrypted\n- [ ] Monitor performance impact\n- [ ] Document decision on mTLS scope\n\n## Config\n```yaml\nencryption:\n  enabled: true\n  type: wireguard\n```\n\n## Note\nThis provides encryption without full mTLS complexity. Good balance for homelab.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-17T17:53:45.086486-08:00","updated_at":"2025-12-17T17:53:45.086486-08:00","labels":["encryption","network","security"],"dependencies":[{"issue_id":"TALOS-2xd","depends_on_id":"TALOS-wlu","type":"parent-child","created_at":"2025-12-17T17:54:02.232006-08:00","created_by":"daemon"}]}
{"id":"TALOS-2z4","title":"Fix prometheus.talos00 and alertmanager.talos00 hrefs","description":"The IngressRoutes for prometheus.talos00 and alertmanager.talos00 were deleted. Either create new IngressRoutes for Mimir endpoints or update hrefs to point to Grafana.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-15T15:48:20.404596-08:00","updated_at":"2025-12-15T16:02:07.594936-08:00","closed_at":"2025-12-15T16:02:07.594936-08:00","labels":["homepage","monitoring"]}
{"id":"TALOS-2zkp","title":"Implement centralized user management with Synology Directory Server (LDAP)","description":"Currently, user/group IDs are inconsistent across systems (Synology, TrueNAS, Kubernetes pods), causing NFS permission issues like the /synology-archive mount problem where Tdarr's abc user (uid 1000) couldn't access files owned by Synology's media user (uid 1026).\n\n**Goal**: Implement centralized LDAP-based user management using Synology Directory Server to provide consistent user/group resolution across all systems.\n\n**Systems to integrate**:\n- Synology NAS (LDAP Server - authoritative source)\n- TrueNAS (LDAP Client)\n- Kubernetes workloads (via SSSD sidecar or application-level LDAP auth)\n- Future: Authelia for SSO across web apps\n\n**Current pain points**:\n- NFS exports require manual squash configuration per share\n- UID/GID mismatches cause permission denied errors\n- No central place to manage users/groups\n- Apps like Tdarr, *arr stack run as different UIDs than NAS users","design":"## Implementation Phases\n\n### Phase 1: Synology Directory Server Setup\n1. Enable Directory Server package on Synology DSM\n2. Create LDAP domain (e.g., dc=homelab,dc=local)\n3. Define standard groups:\n   - `media` (gid 1000) - media applications\n   - `users` (gid 100) - general users\n   - `admins` - administrative access\n4. Create service accounts for NFS/app access\n5. Migrate existing Synology users to directory\n\n### Phase 2: TrueNAS LDAP Integration\n1. Configure TrueNAS as LDAP client\n2. Point to Synology Directory Server\n3. Test NFS exports with LDAP users\n4. Verify UID/GID consistency\n\n### Phase 3: Kubernetes Integration Options\n- **Option A**: SSSD sidecar container for pods needing user resolution\n- **Option B**: Application-level LDAP (Authelia, app configs)\n- **Option C**: Static UID mapping with fsGroup matching LDAP GIDs\n\n### Phase 4: SSO with Authelia\n1. Configure Authelia with LDAP backend\n2. Protect web apps with forward auth\n3. Single sign-on across all services\n\n## NFS Permission Strategy\nAfter LDAP:\n- All NFS exports use consistent UID/GID from directory\n- Squash settings: \"No mapping\" (trust LDAP UIDs)\n- K8s pods run with fsGroup matching LDAP media group","acceptance_criteria":"- [ ] Synology Directory Server running and accessible\n- [ ] At least 3 standard groups defined (media, users, admins)\n- [ ] TrueNAS successfully authenticating against Synology LDAP\n- [ ] NFS mounts work without manual squash overrides\n- [ ] Tdarr can access /synology-archive without permission errors\n- [ ] Documentation updated with LDAP architecture","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-19T19:37:22.029803-08:00","updated_at":"2025-12-19T19:37:22.029803-08:00","labels":["infrastructure","ldap","nfs","security"]}
{"id":"TALOS-2zs","title":"Fix dashboard: k8s-monitoring-overview","description":"Assess and fix/replace k8s-monitoring-overview dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:30:34.180502-08:00","updated_at":"2025-12-14T12:52:14.434873-08:00","closed_at":"2025-12-14T12:52:14.434873-08:00","labels":["dashboard","kubernetes"],"dependencies":[{"issue_id":"TALOS-2zs","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:30:34.181414-08:00","created_by":"daemon"}]}
{"id":"TALOS-34t","title":"Add Intel GPU infrastructure to Flux","description":"Create infrastructure/base/intel-gpu with NFD HelmRelease, Intel Device Plugins Operator, and GPU plugin CR. Add Flux Kustomization.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T16:53:29.968683-08:00","updated_at":"2025-12-12T17:02:27.980284-08:00","closed_at":"2025-12-12T17:02:27.980284-08:00","dependencies":[{"issue_id":"TALOS-34t","depends_on_id":"TALOS-fpp","type":"parent-child","created_at":"2025-12-12T16:59:22.065322-08:00","created_by":"daemon"}]}
{"id":"TALOS-369","title":"Fix dashboard: linkerd-service","description":"Assess and fix/replace linkerd-service dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:04.260823-08:00","updated_at":"2025-12-14T11:31:04.260823-08:00","labels":["dashboard","networking"],"dependencies":[{"issue_id":"TALOS-369","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:04.261749-08:00","created_by":"daemon"}]}
{"id":"TALOS-38eb","title":"Enable TLS encryption for LDAP connections","description":"After initial LDAP setup is working and tested, enable encrypted connections to secure LDAP traffic.\n\n**Current state**: LDAP server configured without forced encryption for simpler initial setup\n\n**Why deferred**:\n- TLS adds certificate management complexity\n- Kubernetes pods need CA certs mounted\n- Apps (Authentik, Grafana, etc.) need LDAPS config\n- Want to verify basic LDAP works first before adding TLS layer\n\n**When to implement**: After all services are successfully using plain LDAP (port 389)","design":"## Implementation Steps\n\n1. **Certificate Setup**\n   - Option A: Use Synology's Let's Encrypt cert (if external domain)\n   - Option B: Generate self-signed CA for homelab.local\n   - Export CA cert for distribution to clients\n\n2. **Synology Configuration**\n   - Enable \"Force clients to use encrypted connections\"\n   - Verify LDAPS listening on port 636\n\n3. **Client Updates**\n   - TrueNAS: Update LDAP config to use LDAPS/STARTTLS\n   - Authentik: Configure LDAPS with CA cert\n   - Grafana: Update LDAP config for TLS\n   - Other apps: Update connection strings\n\n4. **Kubernetes Integration**\n   - Create ConfigMap/Secret with CA certificate\n   - Mount CA cert into pods that connect to LDAP\n   - Update LDAP connection URLs to ldaps://\n\n## Connection Options\n- **LDAPS** (port 636): SSL from start\n- **STARTTLS** (port 389): Upgrade to TLS after connect\n\nRecommend LDAPS for simplicity.","acceptance_criteria":"- [ ] TLS certificate configured on Synology LDAP server\n- [ ] \"Force encrypted connections\" enabled\n- [ ] TrueNAS connecting via LDAPS successfully\n- [ ] Authentik using LDAPS for authentication\n- [ ] CA cert distributed to Kubernetes as ConfigMap\n- [ ] All LDAP-enabled apps tested and working over TLS","notes":"**Current Synology LDAP Settings (2024-12-19)**:\n- \"Disallow anonymous binds\" = DISABLED (temporarily, to avoid breaking current system)\n- \"Force encrypted connections\" = DISABLED (deferred to this task)\n- \"Kick idle connections\" = 1 minute\n\n**TODO when implementing this task**:\n1. Re-enable \"Disallow anonymous binds\" \n2. Enable \"Force encrypted connections\"\n3. Test all clients still work with both settings enabled","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-19T22:59:26.299913-08:00","updated_at":"2025-12-19T23:01:50.881727-08:00","labels":["ldap","security","tls"],"dependencies":[{"issue_id":"TALOS-38eb","depends_on_id":"TALOS-2zkp","type":"blocks","created_at":"2025-12-19T22:59:26.30269-08:00","created_by":"daemon"}]}
{"id":"TALOS-3d6p","title":"VPN rotation CronJob with deterministic balancing","description":"Implement automated VPN server rotation across all gluetun sidecars.\n\nRequirements:\n- CronJob runs ~every 40 min with stochastic jitter\n- Deterministic rotation: track usage ratios, balance across servers\n- Auto-discover all gluetun pods (currently 4: qbittorrent, gluetun, secure-chrome, securexng)\n- Read available servers from protonvpn-credentials secret\n- Push metrics to Mimir pushgateway\n- Update Grafana VPN dashboard with rotation metrics\n\nCurrent servers: se-be-1 (Belgium), se-de-1 (Germany), se-in-1 (India), se-nl-1 (Netherlands)","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-01T16:42:19.23711-08:00","updated_at":"2026-01-01T16:52:46.879994-08:00","closed_at":"2026-01-01T16:52:46.879994-08:00","labels":["cronjob","monitoring","vpn"]}
{"id":"TALOS-3fed","title":"TOR/Darkweb Spider/Scraper/Archiver/Explorer - Private Internal Tool","description":"Build a private-facing TOR/darkweb exploration and archival system for the homelab.\n\n**Core Capabilities:**\n- Spider/crawler for .onion sites\n- Content scraper and archiver\n- Search/exploration interface\n- Full-text indexing of archived content\n\n**Architecture Considerations:**\n- TOR proxy sidecar (similar to gluetun VPN pattern)\n- Headless browser for JS-heavy sites (Playwright/Puppeteer)\n- Storage backend for archived pages (MinIO/S3 or NFS)\n- Search index (OpenSearch/Meilisearch)\n- Web UI for browsing archives\n\n**Security:**\n- Private-facing only (no external access)\n- mTLS or Authentik SSO protection\n- Isolated network namespace\n- No logging of access patterns\n\n**Potential Components:**\n- tor-proxy container\n- scrapy/crawlee spider\n- archive storage (WARC format?)\n- search/index service\n- explorer web UI\n\n**References:**\n- ArchiveBox, Brozzler, Heritrix for archival patterns\n- OnionScan for .onion discovery","design":"**Spec Document:** `docs/05-projects/darkweb-archiver/SPEC.md`\n\n## Architecture Summary\n\nDeployed in isolated `security-zone` namespace with:\n- Default-deny Cilium NetworkPolicies\n- Egress only to Tor network\n- Jump host + Authentik SSO + mTLS for admin access\n- Co-located with Cowrie honeypot (TALOS-5tj)\n\n## Core Components\n\n1. **Tor Proxy** - dperson/torproxy sidecar\n2. **Browsertrix Crawler** - Browser-based crawling via Tor SOCKS5\n3. **ArchiveBox** - Multi-format archival (WARC, PDF, screenshots)\n4. **Meilisearch** - Full-text search (Phase 2)\n5. **PostgreSQL** - Metadata + job queue\n\n## Security Zone Infrastructure\n\n```\ninfrastructure/base/security-zone/\n├── namespace.yaml\n├── network-policies/          # 7 Cilium policies\n├── jump-host/                 # Bastion entry point\n├── darkweb-archiver/          # This project\n└── cowrie-honeypot/           # TALOS-5tj\n```\n\n## Implementation Phases\n\n1. **Foundation** - Namespace, NetworkPolicies, Tor proxy, ArchiveBox, manual crawl\n2. **Automation** - CronJobs, Browsertrix, PostgreSQL job queue\n3. **Search \u0026 Scale** - Meilisearch, Grafana dashboards, multiple workers\n4. **Advanced** - Content filtering, link analysis, API","notes":"Spec document created: 2025-12-23\n\nKey decisions:\n- Use `security-zone` namespace shared with Cowrie honeypot\n- Browsertrix Crawler over Scrapy (better JS handling)\n- ArchiveBox over raw WARC storage (better UI)\n- Start with SQLite FTS5, add Meilisearch later\n- Cilium NetworkPolicies for isolation (7 policies defined)","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T17:29:59.830082-08:00","updated_at":"2025-12-23T14:15:47.076258-08:00","labels":["archival","infrastructure","privacy","tor"],"dependencies":[{"issue_id":"TALOS-3fed","depends_on_id":"TALOS-5tj","type":"related","created_at":"2025-12-23T14:16:13.502718-08:00","created_by":"daemon"}]}
{"id":"TALOS-3k2","title":"Fix RBAC: Remove cluster-admin from Headlamp","description":"CRITICAL: Headlamp has cluster-admin binding which is excessive.\n\n## Tasks\n- [ ] Create custom ClusterRole with read-only permissions\n- [ ] Update ClusterRoleBinding to use new role\n- [ ] Test Headlamp functionality\n- [ ] Document required permissions\n\n## Current (Dangerous)\n```yaml\nroleRef:\n  kind: ClusterRole\n  name: cluster-admin  # REMOVE THIS\n```\n\n## Target\n```yaml\nroleRef:\n  kind: ClusterRole\n  name: headlamp-readonly  # Custom limited role\n```","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-17T17:53:46.284537-08:00","updated_at":"2025-12-17T17:53:46.284537-08:00","labels":["critical","rbac","security"],"dependencies":[{"issue_id":"TALOS-3k2","depends_on_id":"TALOS-wlu","type":"parent-child","created_at":"2025-12-17T17:54:03.182868-08:00","created_by":"daemon"}]}
{"id":"TALOS-3nqs","title":"Local-path storage migration strategy + external access","description":"Need a better approach for local-path storage that supports:\n1. Easy migration between nodes (currently PVCs are node-locked)\n2. External access (NFS export of local data?)\n3. Use as cache layer for performance-critical apps (SQLite DBs)\n\nOptions to evaluate:\n- **Longhorn**: Replicated storage, easy migration, but overhead\n- **TopoLVM**: Better local provisioning with migration support\n- **Local-path + NFS sidecar**: Export local storage via NFS for external access\n- **Rook-Ceph**: Full distributed storage but heavy\n\nCurrent pain points:\n- When node affinity changes, pods can't schedule because PV is node-locked\n- No way to access local data from outside cluster\n- Manual data migration needed when moving workloads","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-20T16:45:32.040408-08:00","updated_at":"2025-12-20T16:45:32.040408-08:00","labels":["infrastructure","storage"]}
{"id":"TALOS-3of","title":"Deploy OTEL Collector to cluster","description":"Deploy OpenTelemetry Collector using the official Helm chart.\n\n**Components:**\n- Deployment mode (gateway) for receiving OTLP from external (Claude Code on Mac)\n- DaemonSet mode for collecting node/pod logs\n\n**Config:**\n- Receivers: OTLP (gRPC 4317, HTTP 4318), prometheus scrape\n- Processors: batch, memory_limiter\n- Exporters: prometheusremotewrite, loki, otlp (tempo)\n\n**Ingress:** `otel.talos00` for OTLP HTTP from Mac","status":"in_progress","priority":1,"issue_type":"task","created_at":"2025-12-13T09:20:15.994792-08:00","updated_at":"2025-12-13T09:20:34.920939-08:00","labels":["infrastructure","otel"],"dependencies":[{"issue_id":"TALOS-3of","depends_on_id":"TALOS-nh8","type":"parent-child","created_at":"2025-12-13T09:20:28.202948-08:00","created_by":"daemon"}]}
{"id":"TALOS-3tw","title":"Fix dashboard: mongodb-instance-summary","description":"Assess and fix/replace mongodb-instance-summary dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:39.127961-08:00","updated_at":"2025-12-14T13:04:56.742045-08:00","closed_at":"2025-12-14T13:04:56.742045-08:00","labels":["dashboard","observability"],"dependencies":[{"issue_id":"TALOS-3tw","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:39.128921-08:00","created_by":"daemon"}]}
{"id":"TALOS-3uf","title":"Fix dashboard: opensearch-exporter","description":"Assess and fix/replace opensearch-exporter dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:53.990754-08:00","updated_at":"2025-12-14T13:04:56.841492-08:00","closed_at":"2025-12-14T13:04:56.841492-08:00","labels":["dashboard","observability"],"dependencies":[{"issue_id":"TALOS-3uf","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:53.991634-08:00","created_by":"daemon"}]}
{"id":"TALOS-4dz3","title":"Integrate Kubernetes Descheduler for workload rebalancing","description":"Install Descheduler to automatically rebalance workloads across nodes based on utilization. Must be configured carefully to avoid conflicts with KEDA scale-to-zero behavior.\n\nConsiderations:\n- Exclude KEDA-managed workloads from descheduler policies\n- Use node affinity/anti-affinity to guide placement\n- Configure LowNodeUtilization strategy for talos03 (currently 4% CPU)\n- Consider RemovePodsViolatingNodeAffinity for GPU workload placement\n- Add PodDisruptionBudgets to critical services before enabling","design":"Strategy: Install descheduler with conservative policies first. Add namespace exclusions for KEDA-scaled workloads. Use profiles: LowNodeUtilization, RemovePodsViolatingTopologySpreadConstraint. Consider time-based scheduling (only rebalance during low-traffic hours).","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-19T18:22:07.190003-08:00","updated_at":"2025-12-19T18:22:07.190003-08:00","dependencies":[{"issue_id":"TALOS-4dz3","depends_on_id":"TALOS-b1gd","type":"blocks","created_at":"2025-12-19T18:22:16.251509-08:00","created_by":"daemon"},{"issue_id":"TALOS-4dz3","depends_on_id":"TALOS-hcv1","type":"blocks","created_at":"2025-12-19T18:22:21.945599-08:00","created_by":"daemon"}]}
{"id":"TALOS-4f4","title":"Deploy Longhorn distributed storage on Talos cluster","description":"Deploy Longhorn for replicated block storage across all 3 nodes. Replaces single-node local-path with distributed storage that survives node failures.\n\n**Requirements:**\n- Talos system extensions: iscsi-tools, util-linux-tools\n- Kubelet extraMounts for /var/lib/longhorn\n- Privileged namespace (longhorn-system)\n- Dedicated disk space on each node\n\n**Benefits:**\n- 3-replica storage (data on all nodes)\n- Automatic failover\n- Snapshots \u0026 S3 backups to MinIO\n- Volume expansion\n- Web UI for management","design":"## Implementation Plan\n\n### Phase 1: Talos Machine Config Updates (requires reboots)\n1. Add iscsi-tools and util-linux-tools extensions\n2. Add kubelet extraMounts for /var/lib/longhorn\n3. Apply config to all 3 nodes (rolling reboots)\n\n### Phase 2: Kubernetes Resources\n1. Create longhorn-system namespace with privileged PSS\n2. Add Longhorn HelmRepository\n3. Deploy Longhorn via HelmRelease\n4. Configure default StorageClass\n\n### Phase 3: Migration (optional)\n1. Create new PVCs with longhorn StorageClass\n2. Migrate data from local-path PVCs\n3. Update deployments to use new PVCs\n4. Delete old local-path PVCs\n\n### Phase 4: Backup Configuration\n1. Configure S3 backup target (MinIO)\n2. Set up recurring backup schedules\n3. Test restore procedures","notes":"**Decision:** Keep local-path as default StorageClass. Use longhorn explicitly for critical workloads only.\n\n**Candidates for longhorn StorageClass:**\n- authentik/postgresql (auth DB)\n- monitoring/grafana-pvc (dashboards)\n- monitoring/loki, mimir-*, tempo (metrics/logs retention)\n- registry/nexus-data (container images)\n- minio/data0-* (object storage)\n\n**Keep on local-path:**\n- scratch/* (dev/test)\n- media/*-config (can recreate from *arr backups)\n- Anything with external backup strategy","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-16T22:57:47.730962-08:00","updated_at":"2025-12-16T23:01:04.162819-08:00","labels":["infrastructure","storage"]}
{"id":"TALOS-4icp","title":"Create migrate-arr-stack.sh orchestrator script","description":"Create shell script to orchestrate the migration:\n\n```bash\nscripts/migrate-arr-stack.sh\n```\n\nFeatures:\n- Pre-flight checks (source exists, target pods stopped)\n- Scale down Sonarr/Radarr deployments\n- Apply migration Job\n- Wait for completion, stream logs\n- Scale back up\n- Post-migration validation\n\nOptions:\n- `--dry-run` - Show what would be done\n- `--app={sonarr|radarr|all}` - Migrate specific app\n- `--skip-db` - Config only, no database migration","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T09:24:11.054963-08:00","updated_at":"2025-12-20T10:07:58.106196-08:00","closed_at":"2025-12-20T10:07:58.106196-08:00","dependencies":[{"issue_id":"TALOS-4icp","depends_on_id":"TALOS-n8an","type":"parent-child","created_at":"2025-12-20T09:24:23.113507-08:00","created_by":"daemon"},{"issue_id":"TALOS-4icp","depends_on_id":"TALOS-is9u","type":"blocks","created_at":"2025-12-20T09:24:39.982594-08:00","created_by":"daemon"}]}
{"id":"TALOS-4jm8","title":"Fix Plex collections appearing in library view","description":"Kometa-created collections are showing up in the main library view in Plex, cluttering the movie/TV show listings.\n\n## Expected Behavior\nCollections should only appear in the \"Collections\" tab, not mixed in with regular media items.\n\n## Current Behavior\nCollections appear alongside movies/shows in the library grid view.\n\n## Root Cause\nKometa's default `collection_mode` setting allows collections to appear in library view.\n\n## Fix\nUpdate Kometa config to hide collections from library:\n\n```yaml\nlibraries:\n  Movies:\n    collection_files:\n      # ... existing collection files ...\n    settings:\n      collection_mode: hide  # or 'hideItems'\n```\n\n### Collection Mode Options\n- `default` - Collections visible in library (current, problematic)\n- `hide` - Collections only in Collections tab\n- `hideItems` - Collections visible, but items hidden from library when in collection\n\n## Files to Update\n- `applications/arr-stack/base/kometa/configmap.yaml`\n\n## Verification\n1. Run Kometa after config change\n2. Check Plex library view - collections should not appear\n3. Check Collections tab - all collections should be there","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-31T20:42:12.036074-08:00","updated_at":"2025-12-31T22:13:45.727184-08:00","closed_at":"2025-12-31T22:13:45.727184-08:00","labels":["bug","kometa","plex"],"dependencies":[{"issue_id":"TALOS-4jm8","depends_on_id":"TALOS-4ubj","type":"parent-child","created_at":"2025-12-31T20:42:17.610565-08:00","created_by":"daemon"}]}
{"id":"TALOS-4qwy","title":"Investigate root cause of gluetun IPv6 routing rule conflicts","description":"The gluetun VPN container experiences IPv6 routing rule conflicts on pod restarts in Kubernetes. Error: \"adding IPv6 rule: adding ip rule 101: from all to all table 51820: file exists\"\n\nCurrent workaround: initContainer that cleans up stale ip rules before gluetun starts.\n\nRoot cause investigation needed:\n- Why do ip rules persist after pod termination?\n- Is this a Kubernetes network namespace cleanup issue?\n- Should gluetun handle this more gracefully?\n- Consider disabling IPv6 entirely at the CNI level for vpn-gateway namespace\n\nReference: https://github.com/qdm12/gluetun-wiki/blob/main/setup/advanced/kubernetes.md#adding-ipv6-rule--file-exists","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-23T10:32:47.252107-08:00","updated_at":"2025-12-23T10:32:47.252107-08:00"}
{"id":"TALOS-4ubj","title":"Epic: Synthwave Overlay System","description":"Unified synthwave overlay system for Plex media library using Kometa + Posterizarr.\n\n## Vision\nCohesive neon synthwave aesthetic across all Plex posters:\n- Neon glow borders (cyan #00FFFF, magenta #FF00FF)\n- Dark base (#1a1a2e)\n- Info badges (resolution, audio, ratings) in synthwave colors\n\n## Architecture\n```\n┌─────────────────────────────────────────┐\n│           POSTERIZARR                   │\n│  • Fetches textless posters             │\n│  • Applies glow overlay PNG             │\n│  • Adds neon border frame               │\n│  • Saves to shared assets folder        │\n└──────────────────┬──────────────────────┘\n                   │\n                   ▼\n┌─────────────────────────────────────────┐\n│              KOMETA                     │\n│  • Reads styled posters from assets     │\n│  • Adds info overlays on top:           │\n│    - Resolution (4K, 1080p)             │\n│    - HDR/DV badges                      │\n│    - Audio codec (Atmos, DTS:X)         │\n│    - Ratings (IMDb)                     │\n│    - Award ribbons                      │\n│    - Show status (Airing/Ended)         │\n│  • Manages collections                  │\n└─────────────────────────────────────────┘\n```\n\n## Synthwave Color Palette\n- Primary: #00FFFF (Cyan)\n- Secondary: #FF00FF (Magenta)\n- Accent: #FFD700 (Gold)\n- Warning: #FF1493 (Hot Pink)\n- Background: #1a1a2e (Dark Purple)\n\n## Deliverables\n- [ ] Custom synthwave glow overlay PNG\n- [ ] Posterizarr config aligned with Kometa\n- [ ] Deconflicted overlay pipelines\n- [ ] Upgraded Kometa configs from Google Drive","status":"open","priority":2,"issue_type":"epic","created_at":"2025-12-31T20:39:23.262823-08:00","updated_at":"2025-12-31T20:39:23.262823-08:00","labels":["epic","kometa","media","posterizarr","synthwave"]}
{"id":"TALOS-4ut","title":"Fix dashboard: pod-cleanup","description":"Assess and fix/replace pod-cleanup dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:38.75003-08:00","updated_at":"2025-12-14T22:40:57.68808-08:00","closed_at":"2025-12-14T22:40:57.68808-08:00","labels":["dashboard","infrastructure"],"dependencies":[{"issue_id":"TALOS-4ut","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:38.751023-08:00","created_by":"daemon"}]}
{"id":"TALOS-4v2","title":"Phase 1: Enable TLS on all arr-stack IngressRoutes","description":"Add TLS termination to all arr-stack IngressRoutes using existing TLSStore wildcard cert.\n\n## Tasks\n- [ ] Add `tls: {}` to all IngressRoutes in applications/arr-stack/base/*/ingressroute.yaml\n- [ ] Verify TLSStore default certificate is working\n- [ ] Test HTTPS access to all services\n- [ ] Update /etc/hosts or DNS if needed\n\n## Services (12 total)\n- prowlarr, sonarr, radarr, overseerr\n- plex, jellyfin, tdarr\n- tautulli, posterr, posterizarr\n- kometa (no ingress), homepage","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-17T17:53:40.415865-08:00","updated_at":"2025-12-17T17:53:40.415865-08:00","labels":["security","tls"],"dependencies":[{"issue_id":"TALOS-4v2","depends_on_id":"TALOS-wlu","type":"parent-child","created_at":"2025-12-17T17:53:59.035561-08:00","created_by":"daemon"}]}
{"id":"TALOS-56z","title":"Audit: ArgoCD applications dashboard coverage","description":"Ensure ArgoCD apps (catalyst-ui, kasa-exporter, arr-stack-private) are shown in dashboards with sync status","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.890889-08:00","updated_at":"2025-12-12T18:57:04.454179-08:00","closed_at":"2025-12-12T18:57:04.454179-08:00"}
{"id":"TALOS-57l","title":"Database Operator Layer - Centralized Database Provisioning","description":"Create a `databases` namespace with database operators (CloudNativePG, MongoDB Community, MinIO) that apps can use to provision databases via CRDs.\n\n**Goal:** Centralized database infrastructure layer where applications request databases declaratively.\n\n**Components:**\n1. CloudNativePG Operator - PostgreSQL clusters via `Cluster` CRD\n2. MongoDB Community Operator - MongoDB replica sets via `MongoDBCommunity` CRD  \n3. MinIO Operator - S3 storage via `Tenant` CRD\n\n**Scratch Namespace Demo:**\n- Example PostgreSQL cluster\n- Example MongoDB replica set\n- Example MinIO tenant\n- CloudBeaver UI for database visualization\n\n**Structure:**\n```\ninfrastructure/base/databases/\n├── namespace.yaml\n├── cloudnative-pg/\n├── mongodb-operator/\n├── minio-operator/\n└── kustomization.yaml\n\napplications/scratch/\n├── example-postgres/\n├── example-mongodb/\n├── example-minio/\n├── cloudbeaver/\n└── kustomization.yaml\n```","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-16T11:30:26.861619-08:00","updated_at":"2025-12-16T11:44:50.786999-08:00","closed_at":"2025-12-16T11:44:50.786999-08:00","labels":["database","infrastructure","operator","scratch"]}
{"id":"TALOS-58e","title":"Configure Grafana datasources for Loki and Tempo","description":"Add datasources to existing Grafana instance:\n\n- Loki datasource for logs\n- Tempo datasource for traces\n- Configure derived fields for trace correlation\n- Import community dashboards for OTEL","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T09:20:16.157694-08:00","updated_at":"2025-12-13T22:23:07.590385-08:00","closed_at":"2025-12-13T22:23:07.590385-08:00","labels":["grafana","otel"],"dependencies":[{"issue_id":"TALOS-58e","depends_on_id":"TALOS-nh8","type":"parent-child","created_at":"2025-12-13T09:20:28.358582-08:00","created_by":"daemon"}]}
{"id":"TALOS-5gz","title":"Configure ArgoCD API key for homepage widget","description":"ArgoCD widget shows 401 Unauthorized. Need to create an API key:\n1. Create an ArgoCD API token via CLI or UI\n2. Add ARGOCD_API_KEY to arr-api-keys secret:\n   kubectl patch secret arr-api-keys -n media --type=merge -p '{\"stringData\":{\"ARGOCD_API_KEY\":\"your-token\"}}'\n3. Restart homepage deployment\n\nAlternative: Update sync-api-keys.sh script to extract ArgoCD token automatically","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-15T17:48:25.280226-08:00","updated_at":"2025-12-15T18:00:21.06205-08:00","closed_at":"2025-12-15T18:00:21.06205-08:00","labels":["api-keys","argocd","homepage"]}
{"id":"TALOS-5hm","title":"Fix dashboard: cilium-operator","description":"Assess and fix/replace cilium-operator dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:30:49.173027-08:00","updated_at":"2025-12-14T17:59:15.161074-08:00","closed_at":"2025-12-14T17:59:15.161074-08:00","labels":["dashboard","networking"],"dependencies":[{"issue_id":"TALOS-5hm","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:30:49.173971-08:00","created_by":"daemon"}]}
{"id":"TALOS-5tj","title":"Kubernetes Honeypot Infrastructure","description":"Research and deploy a Kubernetes-based honeypot system with session capture and replay capabilities.\n\n## Candidates Identified\n\n### Cowrie (Most Likely)\n- Medium-to-high interaction SSH/Telnet honeypot\n- Session recording in UML-compatible format (`var/lib/cowrie/tty/`)\n- Built-in `playlog` utility for replay\n- Asciinema export for sharing sessions\n- File capture (wget/curl downloads, SFTP/scp uploads)\n- K8s deployment: https://github.com/cowrie/k8s-cowrie\n- Helm chart: https://artifacthub.io/packages/helm/emmas-charts/cowrie\n- Docs: https://docs.cowrie.org/\n\n### T-Pot (Comprehensive Platform)\n- All-in-one with 20+ honeypots including Cowrie\n- Docker-based, ELK stack visualization\n- Live attack map\n- Requires 8-16GB RAM, 128GB disk\n- GitHub: https://github.com/telekom-security/tpotce\n\n### RAMAPOT (K8s Native)\n- Multi-honeypot on k3d (Cowrie SSH, Elasticpot, Redis)\n- Centralized logging with Elastic Stack\n- Helm-based deployment\n- GitHub: https://github.com/alikallel/RAMAPOT\n\n### ShadowKube (2025 Research)\n- Converts compromised pods into honeypots automatically\n- Behavioral monitoring + shadow cluster isolation\n- Academic paper: https://link.springer.com/article/10.1186/s42400-025-00372-7\n\n## Session Replay Example (Cowrie)\n```bash\nbin/playlog var/lib/cowrie/tty/\u003csession-file\u003e\nbin/asciinema var/lib/cowrie/tty/\u003csession-file\u003e \u003e session.cast\nasciinema play session.cast\n```","design":"**Shared Spec:** `docs/05-projects/darkweb-archiver/SPEC.md` (Section 12)\n\n## Architecture Summary\n\nDeployed in isolated `security-zone` namespace alongside DarkWeb Archiver (TALOS-3fed):\n- Default-deny Cilium NetworkPolicies\n- External ingress allowed only for honeypot ports (22, 23)\n- Jump host + Authentik SSO for session replay/analysis access\n- Dedicated LoadBalancer IP for external attack surface\n\n## Components\n\n1. **Cowrie** - Medium/high interaction SSH/Telnet honeypot\n2. **Session Storage** - PVC for TTY recordings + downloads\n3. **Replay UI** - asciinema integration for session playback\n4. **Metrics** - Prometheus exporter for attack statistics\n\n## Session Artifacts\n\n| Type | Location | Purpose |\n|------|----------|---------|\n| TTY sessions | `/var/lib/cowrie/tty/` | Full keystroke recordings |\n| Downloads | `/var/lib/cowrie/downloads/` | Malware samples |\n| JSON logs | Loki | Credentials, commands, IPs |\n\n## Network Access\n\n- External: `192.168.1.200:22/23` (LoadBalancer)\n- Internal: `cowrie.security-zone.svc:2222/2223`\n- Admin UI: `https://security.talos00` (via jump-host)","notes":"Architecture aligned with TALOS-3fed (DarkWeb Archiver) - both in security-zone namespace.\n\nSpec document: docs/05-projects/darkweb-archiver/SPEC.md","status":"open","priority":3,"issue_type":"epic","created_at":"2025-12-18T13:57:50.658775-08:00","updated_at":"2025-12-23T14:16:02.549166-08:00","labels":["honeypot","research","security"]}
{"id":"TALOS-5vs","title":"Fix dashboard: loki-promtail","description":"Assess and fix/replace loki-promtail dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:23.913726-08:00","updated_at":"2025-12-14T13:04:56.442758-08:00","closed_at":"2025-12-14T13:04:56.442758-08:00","labels":["dashboard","logging"],"dependencies":[{"issue_id":"TALOS-5vs","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:23.914744-08:00","created_by":"daemon"}]}
{"id":"TALOS-668","title":"Integrate Kubernetes Descheduler for workload rebalancing","description":"talos00 is at 76% memory while talos01 is at 18%. Need automatic workload rebalancing.\n\nThe Kubernetes Descheduler evicts pods based on policies so they can be rescheduled to better-suited nodes. This helps with:\n- Node resource imbalance (current issue)\n- Pod affinity/anti-affinity violations\n- Taint/toleration changes\n- Node utilization optimization\n\nReference: https://github.com/kubernetes-sigs/descheduler","design":"## Implementation Plan\n\n1. **Add Descheduler HelmRelease** to `infrastructure/base/`\n   - Use official helm chart: `descheduler/descheduler`\n   - Configure as CronJob (run every 5-10 minutes)\n\n2. **Configure Descheduler Policies**:\n   - `LowNodeUtilization` - move pods from over-utilized to under-utilized nodes\n   - `RemoveDuplicates` - ensure replicas spread across nodes\n   - `RemovePodsViolatingNodeAffinity` - fix affinity violations\n\n3. **Set Resource Thresholds**:\n   - Target: 50% CPU, 60% memory utilization\n   - Evict from nodes above 70% memory\n\n4. **Add PodDisruptionBudgets** for critical workloads to prevent over-eviction\n\n5. **Integrate with Grafana dashboard** for visibility","acceptance_criteria":"- [ ] Descheduler deployed via HelmRelease\n- [ ] Policies configured for memory-based rebalancing\n- [ ] Tested: pods migrate from talos00 to talos01 when imbalanced\n- [ ] PDBs protect critical services\n- [ ] Metrics visible in Grafana","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-18T17:53:35.132117-08:00","updated_at":"2025-12-19T18:24:47.706381-08:00","closed_at":"2025-12-19T18:24:47.706381-08:00","labels":["infrastructure","monitoring","optimization"]}
{"id":"TALOS-68hf","title":"Create PVCs for qBittorrent config and downloads","description":"Create PersistentVolumeClaims for qBittorrent configuration (local-path) and downloads (NFS to Synology). Mount shared media paths for completed downloads.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T13:31:37.666834-08:00","updated_at":"2025-12-20T13:34:08.431443-08:00","closed_at":"2025-12-20T13:34:08.431443-08:00","dependencies":[{"issue_id":"TALOS-68hf","depends_on_id":"TALOS-m6xf","type":"blocks","created_at":"2025-12-20T13:31:37.668635-08:00","created_by":"daemon"}]}
{"id":"TALOS-69e","title":"Fix dashboard: goldilocks-vpa","description":"Assess and fix/replace goldilocks-vpa dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:54.124464-08:00","updated_at":"2025-12-14T11:31:54.124464-08:00","labels":["dashboard","gitops"],"dependencies":[{"issue_id":"TALOS-69e","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:54.125407-08:00","created_by":"daemon"}]}
{"id":"TALOS-6au","title":"Fix pod-cleanup counter bug (shell subshell issue)","description":"The enhanced pod-cleanup job has a shell scripting bug where counter increments inside while loops are lost because pipes create subshells.\n\nCurrent behavior: All counters show 0 even though resources are being deleted.\nExpected: Counters should reflect actual number of resources cleaned.\n\nRoot cause: In bash/sh, `command | while read ...; do COUNTER=$((COUNTER+1)); done` runs the while loop in a subshell, so COUNTER increments are lost.\n\nSolutions:\n1. Use process substitution: `while read ...; do ...; done \u003c \u003c(command)`\n2. Use a temp file to count\n3. Use heredoc approach\n4. Refactor to use arrays or files for counting\n\nThe job is functional (deletions work), just the metrics counters are inaccurate.","status":"closed","priority":3,"issue_type":"bug","created_at":"2025-12-14T22:29:48.684506-08:00","updated_at":"2025-12-14T22:40:57.645381-08:00","closed_at":"2025-12-14T22:40:57.645381-08:00","labels":["cronjob","metrics","shell"]}
{"id":"TALOS-6km","title":"Phase 2: Add rate-limiting middleware to Traefik","description":"Create and apply rate-limiting middleware to prevent abuse.\n\n## Tasks\n- [ ] Create rate-limit middleware in infrastructure/base/traefik/middlewares.yaml\n- [ ] Configure sensible defaults (100 req/s average, 200 burst)\n- [ ] Apply to public-facing services\n- [ ] Test rate limiting works\n\n## Middleware Config\n```yaml\napiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: rate-limit\nspec:\n  rateLimit:\n    average: 100\n    burst: 200\n```","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T17:53:42.166306-08:00","updated_at":"2025-12-17T17:53:42.166306-08:00","labels":["security","traefik"],"dependencies":[{"issue_id":"TALOS-6km","depends_on_id":"TALOS-wlu","type":"parent-child","created_at":"2025-12-17T17:53:59.867367-08:00","created_by":"daemon"}]}
{"id":"TALOS-6of","title":"Create root orchestrator dashboard","description":"dashboard.sh at project root - inline summary mode (default) + interactive menu mode (--interactive)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.772959-08:00","updated_at":"2025-12-12T18:55:36.559887-08:00","closed_at":"2025-12-12T18:55:36.559887-08:00"}
{"id":"TALOS-6qm","title":"Restore dashboard-common.sh library","description":"Restore scripts/lib/dashboard-common.sh from git (commit 7bf34c3^). All dashboards depend on this.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-12T18:40:35.511188-08:00","updated_at":"2025-12-12T18:48:04.918013-08:00","closed_at":"2025-12-12T18:48:04.918013-08:00"}
{"id":"TALOS-702b","title":"Add Tiltfile migration button with confirmation","description":"Add migration button to arr-stack Tiltfile:\n\n```python\ncmd_button(\n    name='btn-migrate-arr-stack',\n    argv=['bash', 'scripts/migrate-arr-stack.sh'],\n    text='Migrate from Drogon',\n    icon_name='database',\n    requires_confirmation=True,\n    inputs=[\n        choice_input('app', choices=['all', 'sonarr', 'radarr']),\n        bool_input('dry_run', true_string='--dry-run'),\n    ]\n)\n```\n\nLocation: `applications/arr-stack/Tiltfile`","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T09:24:11.364234-08:00","updated_at":"2025-12-20T10:08:36.604095-08:00","closed_at":"2025-12-20T10:08:36.604095-08:00","dependencies":[{"issue_id":"TALOS-702b","depends_on_id":"TALOS-n8an","type":"parent-child","created_at":"2025-12-20T09:24:23.162325-08:00","created_by":"daemon"},{"issue_id":"TALOS-702b","depends_on_id":"TALOS-4icp","type":"blocks","created_at":"2025-12-20T09:24:40.023601-08:00","created_by":"daemon"}]}
{"id":"TALOS-707","title":"RETROSPECTIVE: Intel Arc GPU failure - root cause unknown","description":"## Problem\nIntel GPU stopped working after node reboot on talos02-gpu.\n\n## Hardware\n- ASUS NUC 15 Pro (RNUC15U5) - Intel Core Ultra 5 225H (Arrow Lake-H)\n- GPU: Intel Arc 130T (actually Meteor Lake iGPU, device ID 7d51)\n\n## Root Cause Analysis\n\n### 1. Device Identification\n- Device 7d51 is NOT a discrete Arc GPU\n- It's a Meteor Lake integrated GPU (marketed as \"Arc 130T\")\n- Requires the new `xe` driver, not `i915`\n\n### 2. Driver Issues\n- **i915 driver**: Causes \"GPU wedged\" error on boot - cannot initialize Meteor Lake\n- **xe driver**: Requires `force_probe=7d51` since device is unsupported\n\n### 3. xe Driver Failure Sequence\n```\n1. xe.force_probe=7d51 - driver attempts to probe\n2. Firmware loads successfully:\n   - DMC: i915/mtl_dmc.bin v2.23 ✓\n   - GuC: i915/mtl_guc_70.bin v70.53.0 ✓\n   - HuC: i915/mtl_huc_gsc.bin v8.5.4 ✓\n   - GSC: i915/mtl_gsc_1.bin v102.1.15.1926 ✓\n3. GSC workaround job times out:\n   ERROR: Tile0: GT1: hwe gsccs0: emit_wa_job failed (-ETIME)\n4. Driver probe fails with error -62 (ETIME)\n5. No /dev/dri devices created\n```\n\n### 4. Attempted Fixes\n- ✓ Upgraded to Talos 1.12.0-rc.1 (required for xe driver)\n- ✓ Added siderolabs/xe extension\n- ✓ Added xe.force_probe=7d51 kernel arg\n- ✓ Added i915 firmware extension (for mtl_* firmware files)\n- ✓ Blacklisted i915 module (module_blacklist=i915)\n- ✗ Power management params (intel_idle.max_cstate=0, processor.max_cstate=0)\n\n### 5. Current State\n- Node running Talos 1.12.0-rc.1\n- xe module loaded, but fails to probe device\n- No DRM devices (/dev/dri doesn't exist)\n- GPU transcoding NOT functional\n\n## Upstream Issue\nThis is tracked at freedesktop.org: https://gitlab.freedesktop.org/drm/xe/kernel/-/issues/2025\n\n## Open Questions\n1. WHY did GPU work before reboot on Talos 1.11.1?\n   - Hypothesis: i915 may have worked in some state that was lost on reboot\n   - Could be BIOS/firmware state dependent\n2. Is there a kernel version where xe works for device 7d51?\n3. Can GSC timeout be extended or workaround disabled?\n\n## Recommendations\n1. **Short term**: Accept no GPU on this node, use talos01 for transcoding\n2. **Medium term**: Wait for Talos 1.12 stable with updated xe driver\n3. **Long term**: Monitor upstream xe driver fixes for Meteor Lake\n\n## Schema for future reference\nTalos Image Factory schematic (saved in configs/nodes/talos02-gpu-schematic.yaml):\n- Extensions: siderolabs/xe, siderolabs/i915, siderolabs/intel-ucode\n- Kernel args: xe.force_probe=7d51, i915.force_probe=!7d51, module_blacklist=i915","notes":"## Rollback Completed - GPU Still Wedged\n\nRolled back to original working configuration:\n- Talos v1.11.1\n- Schematic: 4b3cd373a192c8469e859b7a0cfbed3ecc3577c4a2d346a37b0aeff9cd17cdb0\n- Extensions: siderolabs/i915, siderolabs/intel-ucode\n\n**Result**: GPU STILL WEDGED with exact same error:\n```\ni915 0000:00:02.0: [drm] *ERROR* Failed to initialize GPU, declaring it wedged!\n```\n\nThis proves the issue is NOT the driver - it's a hardware/firmware state problem that persists across reboots.\n\n**Next Step**: Cold boot (full power cycle - unplug power for 30s) to reset GPU firmware state.\n\nUpdated files:\n- RETRO-GPU-2025-12-19.md - Full timeline and root cause analysis\n- worker-talos02-gpu.yaml - Rolled back to i915 schematic\n- talos02-gpu-schematic.yaml - Updated with current state","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T14:38:31.005237-08:00","updated_at":"2025-12-19T15:19:37.137719-08:00","closed_at":"2025-12-19T15:19:37.137719-08:00"}
{"id":"TALOS-75rw","title":"Enable Intel NPU (AI Boost) support on talos06","description":"The GMKtec Nucbox EVO T1 has an Intel AI Boost NPU (13 TOPS INT8) that is currently NOT supported in Talos Linux.\n\nNo official siderolabs extension exists yet. Options:\n1. Wait for official extension from Siderolabs\n2. Build custom Talos extension using intel/linux-npu-driver\n3. Run NPU workloads in a privileged container with host device access\n\nReferences:\n- https://github.com/intel/linux-npu-driver\n- https://github.com/siderolabs/extensions\n\nThis is low priority - GPU transcoding is the main use case for now.","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-01T08:43:01.499555-08:00","updated_at":"2026-01-01T08:43:01.499555-08:00","labels":["future","hardware"],"dependencies":[{"issue_id":"TALOS-75rw","depends_on_id":"TALOS-ho2k","type":"blocks","created_at":"2026-01-01T08:43:11.327893-08:00","created_by":"daemon"}]}
{"id":"TALOS-7a4","title":"Fix dashboard: linkerd-top-line","description":"Assess and fix/replace linkerd-top-line dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:04.339199-08:00","updated_at":"2025-12-14T11:31:04.339199-08:00","labels":["dashboard","networking"],"dependencies":[{"issue_id":"TALOS-7a4","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:04.340099-08:00","created_by":"daemon"}]}
{"id":"TALOS-7dm","title":"Investigate Cilium transparent DNS proxy + Talos hostDNS conflict","description":"After disabling Cilium egress gateway, the transparent DNS proxy (169.254.116.108) stopped forwarding to upstream. Currently using workaround (CoreDNS forwards to 8.8.8.8 directly).\n\nRoot cause investigation needed:\n- Interaction between Cilium `dnsproxy-enable-transparent-mode` and Talos `hostDNS.forwardKubeDNSToHost`\n- Why 169.254.116.108 is still injected even with transparent mode disabled\n- Consider disabling Talos forwardKubeDNSToHost or making 8.8.8.8 the permanent solution\n\nCurrent workaround in CoreDNS ConfigMap: `forward . 8.8.8.8 8.8.4.4`","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-18T12:10:15.409188-08:00","updated_at":"2025-12-18T12:10:15.409188-08:00","labels":["cilium","dns","talos","tech-debt"]}
{"id":"TALOS-7fu","title":"MCP Server Integration for AI-powered cluster management","description":"Deploy MCP servers for AI-assisted infrastructure management:\n\n**High Priority:**\n- Kubernetes MCP Server (cluster queries, Helm management)\n- Grafana MCP Server (dashboard/metric queries)\n- Prometheus MCP Server (PromQL via AI)\n\n**Medium Priority:**\n- FluxCD/ArgoCD MCP Server (may need custom dev)\n\n**Deployment:** Containerized in observability namespace\n\nFrom: docs/06-project-management/enhancement-roadmap.md (Stream 1)","status":"open","priority":3,"issue_type":"epic","created_at":"2025-12-12T22:20:28.686317-08:00","updated_at":"2025-12-12T22:20:28.686317-08:00","labels":["ai","mcp","observability"]}
{"id":"TALOS-7jw","title":"Fix dashboard: k8s-cluster-monitoring","description":"Assess and fix/replace k8s-cluster-monitoring dashboard for V2 OTEL stack compatibility","notes":"**Audit 2025-12-14**: Partial functionality\n- ✅ Cluster-level metrics work: memory 27.7%, CPU 6.88%, filesystem 18.54%\n- ❌ Pod-level panels show \"No data\": Pods CPU usage, Pods memory usage, Pods network I/O\n- Root cause: Pod queries may use labels not present in Alloy-scraped metrics\n- Action needed: Investigate pod-level query label requirements","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:30:34.041967-08:00","updated_at":"2025-12-14T15:57:44.174632-08:00","labels":["dashboard","kubernetes"],"dependencies":[{"issue_id":"TALOS-7jw","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:30:34.042964-08:00","created_by":"daemon"}]}
{"id":"TALOS-7ls","title":"Configure Plex API token for homepage widget","description":"Plex server needs to be claimed by the user before the token can be extracted. After claiming:\n1. Run `./applications/arr-stack/scripts/sync-api-keys.sh` or use Tilt \"Sync API Keys\" button\n2. The script extracts PlexOnlineToken from Preferences.xml\n3. Homepage pod needs restart to pick up new secret","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-15T17:48:24.832639-08:00","updated_at":"2025-12-17T08:21:24.178482-08:00","closed_at":"2025-12-17T08:21:24.178482-08:00","labels":["api-keys","homepage","plex"]}
{"id":"TALOS-7pg","title":"Fix dashboard: resource-efficiency","description":"Assess and fix/replace resource-efficiency dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:38.891415-08:00","updated_at":"2025-12-14T11:31:38.891415-08:00","labels":["dashboard","infrastructure"],"dependencies":[{"issue_id":"TALOS-7pg","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:38.892417-08:00","created_by":"daemon"}]}
{"id":"TALOS-7vj5","title":"Post-migration: Fix library paths and validate","description":"After migration completes:\n\n1. Update root folder paths in Sonarr/Radarr to match K8s mounts\n2. Verify media library shows correct items\n3. Test indexer connectivity\n4. Test download client connectivity\n5. Verify posters/artwork display (if MediaCover migrated)\n\nMay need API calls or manual UI steps - document the process.","notes":"Post-migration steps documented in migrate-arr-stack.sh output. User must manually: 1) Trigger restore via UI 2) Update library paths 3) Test connectivity","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-20T09:24:11.688929-08:00","updated_at":"2025-12-20T10:08:36.665561-08:00","dependencies":[{"issue_id":"TALOS-7vj5","depends_on_id":"TALOS-n8an","type":"parent-child","created_at":"2025-12-20T09:24:23.204265-08:00","created_by":"daemon"},{"issue_id":"TALOS-7vj5","depends_on_id":"TALOS-702b","type":"blocks","created_at":"2025-12-20T09:24:40.06629-08:00","created_by":"daemon"}]}
{"id":"TALOS-7vk","title":"Add --summary/--full flags to existing dashboards","description":"Update infrastructure/dashboard.sh, arr-stack/dashboard.sh, catalyst-llm/dashboard.sh with summary mode for nesting","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.828008-08:00","updated_at":"2025-12-12T18:55:36.604336-08:00","closed_at":"2025-12-12T18:55:36.604336-08:00"}
{"id":"TALOS-7y5","title":"Fix Nebula overlay - all nodes have same IP 10.42.1.1","description":"All 3 Talos nodes are configured with the same Nebula IP `10.42.1.1/16` on the nebula1 interface. Each node should have a unique IP in the overlay network.\n\n**Current state:**\n- talos00: 10.42.1.1/16\n- talos01: 10.42.1.1/16  \n- talos02-gpu: 10.42.1.1/16\n\n**Impact:** The Talos health check reports `discovered nodes: [\"10.42.1.1\" \"10.42.1.1\" \"10.42.1.1\"]` but the cluster still works because Kubernetes uses LAN IPs (192.168.1.x), not Nebula overlay.\n\n**Fix:** Update Nebula certs/configs to assign unique IPs to each node.","status":"open","priority":3,"issue_type":"bug","created_at":"2025-12-17T15:31:25.732623-08:00","updated_at":"2025-12-17T15:31:25.732623-08:00","labels":["config","nebula","networking"]}
{"id":"TALOS-7zy","title":"Fix Jellyfin widget API key","description":"Verify Jellyfin widget is working. May need API key (HOMEPAGE_VAR_JELLYFIN_KEY) configured.","status":"open","priority":2,"issue_type":"bug","created_at":"2025-12-15T15:48:21.141602-08:00","updated_at":"2025-12-15T15:48:21.141602-08:00","labels":["homepage","media"]}
{"id":"TALOS-84bs","title":"Taint control plane to prevent non-core scheduling","description":"Add NoSchedule taint to talos00 control plane node to prevent non-core workloads from scheduling there.\n\nCurrent state: 59 pods on talos00, no taints\nTarget state: Only core infrastructure on talos00\n\nCore infrastructure (add tolerations):\n- kube-system, cert-manager, external-secrets, flux-system, argocd\n- traefik, liqo, cilium-spire, local-path-storage, node-feature-discovery\n\nWorkloads to migrate:\n- catalyst, catalyst-llm, media, media-private, monitoring\n- minio, registry, scratch, vpn-gateway, authentik, infra-control, default","design":"1. Update Talos machine config to add taint: node-role.kubernetes.io/control-plane:NoSchedule\n2. Add tolerations to core infrastructure deployments\n3. Let scheduler naturally migrate non-tolerated pods to workers\n4. Monitor for any pods stuck in Pending","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-19T18:33:05.206453-08:00","updated_at":"2025-12-19T19:51:31.888997-08:00","closed_at":"2025-12-19T19:51:31.888997-08:00"}
{"id":"TALOS-86f4","title":"Configure Sonarr/Radarr to use qBittorrent download client","description":"Add qBittorrent as download client in Sonarr and Radarr. Configure API credentials and category mappings for automatic import.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-20T13:31:38.36237-08:00","updated_at":"2025-12-20T13:31:38.36237-08:00","dependencies":[{"issue_id":"TALOS-86f4","depends_on_id":"TALOS-m6xf","type":"blocks","created_at":"2025-12-20T13:31:38.365166-08:00","created_by":"daemon"}]}
{"id":"TALOS-86i","title":"Fix dashboard: llm-scaler","description":"Assess and fix/replace llm-scaler dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:32:08.80723-08:00","updated_at":"2025-12-14T11:32:08.80723-08:00","labels":["dashboard","hybrid-cluster"],"dependencies":[{"issue_id":"TALOS-86i","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:32:08.808215-08:00","created_by":"daemon"}]}
{"id":"TALOS-8ey","title":"Implement backup strategy for etcd and PVCs","description":"Critical data protection:\n\n- etcd backup automation (Talos provides etcd snapshots)\n- PVC snapshot capabilities\n- Backup storage location (NAS? S3?)\n- Create disaster recovery runbook\n- Test restore procedures\n\nFrom: docs/_archive/TODO.md","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-12T22:20:28.578721-08:00","updated_at":"2025-12-20T16:24:29.662917-08:00","closed_at":"2025-12-20T16:24:29.662917-08:00","labels":["backup","disaster-recovery","infrastructure"]}
{"id":"TALOS-8fh","title":"Fix dashboard: kasa-forecasting-analytics","description":"Assess and fix/replace kasa-forecasting-analytics dashboard for V2 OTEL stack compatibility","notes":"**Audit 2025-12-14**: Mostly broken\n- ✅ Working: Month Progress (46.0%)\n- ❌ \"No data\": Projected monthly cost, budget status, cost trends, power trends, week/month comparisons, anomaly detection\n- ❌ Error: \"Cost by Hour of Day\" panel throws TypeError\n- Root cause: Recording rules may be missing for forecast calculations\n- Priority: Investigate required recording rules for forecast panels","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:32:09.099283-08:00","updated_at":"2025-12-14T15:57:44.280412-08:00","labels":["dashboard","kasa"],"dependencies":[{"issue_id":"TALOS-8fh","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:32:09.101091-08:00","created_by":"daemon"}]}
{"id":"TALOS-8fv","title":"Phase 3: Pin all image tags (remove :latest)","description":"Replace all :latest tags with pinned versions for reproducibility and security.\n\n## Images to Pin\n- posterr:latest → petersem/posterr:v2.x.x\n- busybox:latest → busybox:1.36\n- kometateam/kometa:latest → kometateam/kometa:v2.x.x\n- ghcr.io/fscorrupt/posterizarr:latest → pin to release\n- ghcr.io/gethomepage/homepage:latest → pin to release\n- lscr.io/linuxserver/* (6 images) → pin to date tag or version\n\n## Tasks\n- [ ] Research current stable versions for each image\n- [ ] Update all deployment.yaml files\n- [ ] Consider ArgoCD Image Updater for automated updates\n- [ ] Test deployments still work","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-17T17:53:43.243459-08:00","updated_at":"2025-12-17T17:53:43.243459-08:00","labels":["containers","security"],"dependencies":[{"issue_id":"TALOS-8fv","depends_on_id":"TALOS-wlu","type":"parent-child","created_at":"2025-12-17T17:54:00.806387-08:00","created_by":"daemon"}]}
{"id":"TALOS-8uv","title":"macOS Native Notifications for Cluster Events","description":"Implement a notification system that sends critical cluster events to macOS native notifications via ntfy.sh. Goal is to be alerted about important issues without getting overwhelmed by noise.","status":"open","priority":2,"issue_type":"epic","created_at":"2025-12-12T22:11:01.035773-08:00","updated_at":"2025-12-12T22:11:01.035773-08:00","labels":["flux","notifications","observability"]}
{"id":"TALOS-8vh","title":"Fix Overseerr homepage widget 403 error","description":"Overseerr API key is synced but the /api/v1/request/count endpoint returns 403 Forbidden.\n\nThe API key works for /api/v1/status but not request/count. This is likely a permissions issue in Overseerr - the API user may need elevated permissions to access request counts.\n\nInvestigation needed:\n1. Check Overseerr user/permissions settings\n2. Verify the API key has the right scope\n3. May need to use a different endpoint or widget type","status":"open","priority":2,"issue_type":"bug","created_at":"2025-12-15T17:48:25.510941-08:00","updated_at":"2025-12-15T17:48:25.510941-08:00","labels":["api-keys","homepage","overseerr"]}
{"id":"TALOS-8w5","title":"Phase 1: Disable Traefik insecure API","description":"Remove --api.insecure=true from Traefik HelmRelease.\n\n## Tasks\n- [ ] Remove `--api.insecure=true` from additionalArguments\n- [ ] Configure secure dashboard access via IngressRoute with TLS\n- [ ] Add Authentik middleware to Traefik dashboard\n- [ ] Test dashboard still accessible securely\n\n## File\ninfrastructure/base/traefik/helmrelease.yaml","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-17T17:53:41.001235-08:00","updated_at":"2025-12-17T17:53:41.001235-08:00","labels":["security","traefik"],"dependencies":[{"issue_id":"TALOS-8w5","depends_on_id":"TALOS-wlu","type":"parent-child","created_at":"2025-12-17T17:53:59.087876-08:00","created_by":"daemon"}]}
{"id":"TALOS-8zc","title":"Fix dashboard: cilium-agent","description":"Assess and fix/replace cilium-agent dashboard for V2 OTEL stack compatibility","notes":"**Fixed 2025-12-14**: Added k8s_app label extraction to Alloy config.\n\nRoot cause: Dashboard queries use `k8s_app=\"cilium\"` but Alloy wasn't extracting the `k8s-app` pod label.\n\nFix: Added relabel rule in Alloy helmrelease.yaml to extract `__meta_kubernetes_pod_label_k8s_app` as `k8s_app`.\n\nWorking panels after fix:\n- BPF map pressure\n- Virtual Memory Bytes\n- Resident memory status\n- Open file descriptors\n- System-wide BPF memory usage\n\nSome panels show \"No data\" which is expected (e.g., Errors \u0026 Warnings when no errors exist).","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:30:48.968794-08:00","updated_at":"2025-12-14T17:59:07.581067-08:00","closed_at":"2025-12-14T17:59:07.581067-08:00","labels":["dashboard","networking"],"dependencies":[{"issue_id":"TALOS-8zc","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:30:48.96991-08:00","created_by":"daemon"}]}
{"id":"TALOS-98j","title":"Deploy MongoDB Community Operator","description":"Deploy MongoDB Community Operator to databases namespace via HelmRelease. This enables MongoDB replica set provisioning via `MongoDBCommunity` CRD.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T11:30:40.364819-08:00","updated_at":"2025-12-16T11:42:51.480491-08:00","closed_at":"2025-12-16T11:42:51.480491-08:00","labels":["database","mongodb","operator"],"dependencies":[{"issue_id":"TALOS-98j","depends_on_id":"TALOS-eoq","type":"blocks","created_at":"2025-12-16T11:30:58.357929-08:00","created_by":"daemon"}]}
{"id":"TALOS-9ku","title":"Configure Plex/Tdarr for GPU transcoding","description":"Update Plex/Tdarr deployments with GPU resource requests, nodeSelector for GPU node, /dev/dri volume mount","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T17:02:37.005599-08:00","updated_at":"2025-12-15T13:33:19.619846-08:00","closed_at":"2025-12-15T13:33:19.619846-08:00","dependencies":[{"issue_id":"TALOS-9ku","depends_on_id":"TALOS-fpp","type":"parent-child","created_at":"2025-12-12T17:02:44.485533-08:00","created_by":"daemon"},{"issue_id":"TALOS-9ku","depends_on_id":"TALOS-dfv","type":"blocks","created_at":"2025-12-12T17:02:44.569654-08:00","created_by":"daemon"}]}
{"id":"TALOS-9ly","title":"Fix dashboard: loki-stack-monitoring","description":"Assess and fix/replace loki-stack-monitoring dashboard for V2 OTEL stack compatibility","notes":"**Audit 2024-12-15**: Loki stack monitoring dashboard (14055) needs testing but datasource mapping should work with Mimir+Loki setup.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:23.993789-08:00","updated_at":"2025-12-14T17:14:37.266374-08:00","closed_at":"2025-12-14T17:14:37.266374-08:00","labels":["dashboard","logging"],"dependencies":[{"issue_id":"TALOS-9ly","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:23.994809-08:00","created_by":"daemon"}]}
{"id":"TALOS-9pl","title":"Fix Alertmanager widget - use Mimir Alertmanager","description":"Update Alertmanager widget to use mimir-alertmanager.monitoring.svc.cluster.local:8080. Remove the broken server/container reference.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-15T15:48:20.129048-08:00","updated_at":"2025-12-15T16:02:07.532458-08:00","closed_at":"2025-12-15T16:02:07.532458-08:00","labels":["homepage","monitoring"]}
{"id":"TALOS-a23","title":"Configure Media Stack Applications","description":"Complete application-level configuration for the deployed media stack:\n\n- Configure Prowlarr indexers\n- Connect Sonarr → Prowlarr\n- Connect Radarr → Prowlarr\n- Configure Plex media libraries\n- Configure Jellyfin media libraries\n- Set up Homepage dashboard widgets\n- Test end-to-end media workflow\n\nFrom: docs/_archive/TODO.md","status":"open","priority":3,"issue_type":"epic","created_at":"2025-12-12T22:20:28.47382-08:00","updated_at":"2025-12-12T22:20:28.47382-08:00","labels":["configuration","media-stack"]}
{"id":"TALOS-a2ck","title":"Bluetooth Integration via ESPHome Proxies","description":"Add Bluetooth device support to the cluster using ESPHome Bluetooth Proxies. Since Talos Linux doesn't include Bluetooth kernel modules by default and cluster nodes don't have Bluetooth hardware, ESPHome ESP32 devices provide the best path forward.\n\n## Current State\n- No Bluetooth hardware on cluster nodes (talos00-talos05)\n- Talos doesn't include btusb, bluetooth kernel modules\n- Home Assistant integration planned (TALOS-fwrz)\n\n## Proposed Solution: ESPHome Bluetooth Proxies\nESPHome devices with ESP32 chips can act as Bluetooth proxies, forwarding BLE advertisements to Home Assistant over WiFi.\n\n### Benefits\n- No kernel module changes needed\n- Distributed coverage (place ESP32s around house)\n- Low cost (~$5-10 per ESP32 device)\n- Native Home Assistant integration\n- Supports BLE sensors, trackers, presence detection\n\n### Hardware Options\n- ESP32 DevKit boards\n- M5Stack Atom Lite\n- ESP32-C3 boards (lower power)\n\n### Implementation\n1. Flash ESP32 with ESPHome firmware\n2. Configure bluetooth_proxy component\n3. Home Assistant auto-discovers proxies\n4. BLE devices appear in HA\n\n## Alternatives Considered\n- USB Bluetooth dongles: Would require Talos extension for btusb module\n- Talos system extensions: Complex, requires custom image builds","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-19T21:55:31.736114-08:00","updated_at":"2025-12-19T21:55:31.736114-08:00","labels":["esphome","home-assistant","iot"],"dependencies":[{"issue_id":"TALOS-a2ck","depends_on_id":"TALOS-fwrz","type":"blocks","created_at":"2025-12-19T21:55:37.616782-08:00","created_by":"daemon"},{"issue_id":"TALOS-a2ck","depends_on_id":"TALOS-oik1","type":"related","created_at":"2025-12-19T21:59:37.733248-08:00","created_by":"daemon"}]}
{"id":"TALOS-a4s","title":"Audit ServiceMonitors and document coverage","description":"Document what the 56 ServiceMonitors are scraping.\n\n**Purpose:** Ensure Alloy's native discovery covers all targets.\n\n**Steps:**\n1. List all ServiceMonitors with their selectors\n2. Map to Alloy discovery (pods, services, endpoints)\n3. Identify any gaps in Alloy coverage\n4. Archive ServiceMonitor configs for reference\n\n**Note:** Alloy doesn't use ServiceMonitor CRDs - it uses Kubernetes native discovery.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T22:26:51.456741-08:00","updated_at":"2025-12-13T22:39:27.01573-08:00","closed_at":"2025-12-13T22:39:27.01573-08:00","labels":["documentation","monitoring"],"dependencies":[{"issue_id":"TALOS-a4s","depends_on_id":"TALOS-zqh","type":"parent-child","created_at":"2025-12-13T22:27:04.712834-08:00","created_by":"daemon"}]}
{"id":"TALOS-a68a","title":"Configure Scrypted Arlo integration with Frigate","description":"Scrypted has been deployed to bridge Arlo cameras to Frigate via RTSP rebroadcast.\n\n**Completed:**\n- Scrypted deployment in scratch namespace (talos02-gpu)\n- PVC for config persistence (5Gi)\n- Service exposing HTTP/RTSP ports\n- IngressRoute at http://scrypted.talos00\n\n**Remaining:**\n1. Access Scrypted UI and create admin account\n2. Install @scrypted/arlo plugin\n3. Add Arlo credentials and complete 2FA\n4. Install @scrypted/rebroadcast plugin\n5. Get RTSP URLs for each camera\n6. Update Frigate configmap with camera configs\n7. Test detection on Arlo camera feeds","acceptance_criteria":"- [ ] Arlo cameras visible in Scrypted\n- [ ] RTSP streams accessible from Frigate\n- [ ] Frigate detecting objects on Arlo feeds\n- [ ] No excessive latency (\u003c 5s delay)","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-30T15:18:12.805215-08:00","updated_at":"2025-12-30T15:18:12.805215-08:00","labels":["arlo","camera","frigate"]}
{"id":"TALOS-a6w","title":"Fix dashboard: graylog-metrics","description":"Assess and fix/replace graylog-metrics dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:38.966519-08:00","updated_at":"2025-12-14T13:04:56.541947-08:00","closed_at":"2025-12-14T13:04:56.541947-08:00","labels":["dashboard","observability"],"dependencies":[{"issue_id":"TALOS-a6w","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:38.967505-08:00","created_by":"daemon"}]}
{"id":"TALOS-a8g7","title":"Configure gluetun with ProtonVPN WireGuard for qBittorrent","description":"Set up gluetun environment variables for ProtonVPN WireGuard connection. Use ExternalSecret to pull WireGuard private key from 1Password. Configure port forwarding if needed for seeding.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T13:31:37.354492-08:00","updated_at":"2025-12-20T13:34:08.3708-08:00","closed_at":"2025-12-20T13:34:08.3708-08:00","dependencies":[{"issue_id":"TALOS-a8g7","depends_on_id":"TALOS-m6xf","type":"blocks","created_at":"2025-12-20T13:31:37.357326-08:00","created_by":"daemon"}]}
{"id":"TALOS-adp","title":"Create SecureXNG UI overlay for dynamic proxy selection","description":"Add a UI overlay/widget to SecureXNG that allows users to:\n- View current VPN exit location/IP\n- Switch between available proxy endpoints on the fly\n- Show connection status and latency for each proxy\n- Integrate with gluetun control API for real-time server switching\n\nCould be implemented as a userscript or browser extension that overlays on the SearXNG interface.","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-18T13:48:32.384115-08:00","updated_at":"2025-12-18T13:48:32.384115-08:00","labels":["enhancement","ui","vpn-gateway"]}
{"id":"TALOS-aev","title":"Hybrid LLM Cluster Project","description":"Nebula + Liqo + AWS GPU cluster for LLM inference. Multi-phase project:\n\n**Phase 1: Nebula Infrastructure**\n- CA setup, lighthouse deployment, Talos node configuration\n\n**Phase 2: AWS Infrastructure**\n- VPC, EC2 Spot instances, GPU bootstrap\n\n**Phase 3: Liqo Federation**\n- Homelab + AWS peering, namespace offloading\n\n**Phase 4: Ollama Deployment**\n- GPU-aware deployment, model management\n\n**Phase 5: Automation**\n- Scale-to-zero, IaC, monitoring\n\n**Status:** Suspended (catalyst-llm Flux kustomization suspended)\n\nFrom: docs/05-projects/hybrid-llm-cluster/TODO.md","status":"open","priority":4,"issue_type":"epic","created_at":"2025-12-12T22:20:28.632042-08:00","updated_at":"2025-12-12T22:20:28.632042-08:00","labels":["aws","liqo","llm","nebula","suspended"]}
{"id":"TALOS-afk","title":"Audit: Flux kustomizations dashboard coverage","description":"Ensure all Flux kustomizations (16 total) are displayed in infrastructure dashboard with reconcile status","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.949951-08:00","updated_at":"2025-12-12T18:57:04.493613-08:00","closed_at":"2025-12-12T18:57:04.493613-08:00"}
{"id":"TALOS-ag3","title":"Consider Alertmanager → ntfy integration for Prometheus alerts","description":"Evaluate routing Prometheus/Alertmanager critical alerts to ntfy:\n\n- Could catch issues Flux doesn't see (resource exhaustion, app-level errors)\n- Alertmanager has better deduplication and grouping\n- May need alertmanager-ntfy webhook receiver\n\nThis is stretch goal - start with Flux notifications first.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-12T22:11:20.205606-08:00","updated_at":"2025-12-12T22:11:20.205606-08:00","labels":["monitoring","notifications","stretch"],"dependencies":[{"issue_id":"TALOS-ag3","depends_on_id":"TALOS-8uv","type":"parent-child","created_at":"2025-12-12T22:11:30.880208-08:00","created_by":"daemon"}]}
{"id":"TALOS-agw","title":"MinIO Operator - Multi-tenant S3 Storage Management","description":"Deploy MinIO Operator for declarative S3-compatible storage management.\n\n**Goal:** Replace standalone MinIO with operator-managed tenants for better multi-tenant support.\n\n**Operator:** MinIO Operator (Official)\n- Repo: https://github.com/minio/operator\n- Helm: https://operator.min.io\n\n**CRDs Provided:**\n- `Tenant` - MinIO tenant (isolated S3 namespace)\n- `PolicyBinding` - IAM policy bindings\n\n**Features:**\n- Multi-tenant S3 storage\n- Per-tenant resource isolation\n- Automatic TLS via cert-manager\n- Console per tenant\n- Prometheus metrics\n- Bucket notifications\n\n**Migration Path:**\n1. Deploy MinIO Operator alongside existing standalone MinIO\n2. Create Tenant CR for OTEL stack (mimir, tempo, loki buckets)\n3. Migrate Mimir/Tempo/Loki to new tenant endpoints\n4. Decommission standalone MinIO\n\n**Integration:**\n- Storage: fatboy-nfs-appdata (NFS)\n- TLS: cert-manager (homelab-ca-issuer)\n- Monitoring: ServiceMonitor for Prometheus","status":"open","priority":2,"issue_type":"epic","created_at":"2025-12-13T12:29:44.394253-08:00","updated_at":"2025-12-13T12:29:44.394253-08:00","labels":["infrastructure","minio","operator","s3","storage"]}
{"id":"TALOS-akv","title":"Create Talos Image Factory schematic for i915 extension","description":"Create talos02-gpu-schematic.yaml with siderolabs/i915 and siderolabs/intel-ucode extensions for custom Talos image","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T16:53:29.92203-08:00","updated_at":"2025-12-12T17:02:27.932317-08:00","closed_at":"2025-12-12T17:02:27.932317-08:00","dependencies":[{"issue_id":"TALOS-akv","depends_on_id":"TALOS-fpp","type":"parent-child","created_at":"2025-12-12T16:59:22.015116-08:00","created_by":"daemon"}]}
{"id":"TALOS-asg","title":"Deploy CloudBeaver database UI in scratch namespace","description":"Deploy CloudBeaver (web-based DBeaver alternative) to scratch namespace to visualize and manage the example databases.\n\nCloudBeaver provides:\n- Web-based database management UI\n- Support for PostgreSQL, MongoDB, and S3 browsing\n- Connection to all three example databases\n\nIngress: cloudbeaver.talos00","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T11:30:40.65463-08:00","updated_at":"2025-12-16T11:44:50.735353-08:00","closed_at":"2025-12-16T11:44:50.735353-08:00","labels":["database","scratch","ui"],"dependencies":[{"issue_id":"TALOS-asg","depends_on_id":"TALOS-n3q","type":"blocks","created_at":"2025-12-16T11:30:58.682772-08:00","created_by":"daemon"}]}
{"id":"TALOS-aty0","title":"Add K8s Services for Nexus proxy ports","description":"Create Kubernetes Services to expose each Nexus proxy repository port (5001-5005) so containerd can reach them via cluster DNS.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-20T16:18:15.855563-08:00","updated_at":"2025-12-20T16:18:15.855563-08:00","dependencies":[{"issue_id":"TALOS-aty0","depends_on_id":"TALOS-wjb9","type":"blocks","created_at":"2025-12-20T16:18:15.856884-08:00","created_by":"daemon"}]}
{"id":"TALOS-b1gd","title":"Integrate KEDA for event-driven autoscaling","description":"Install and configure KEDA (Kubernetes Event-driven Autoscaling) for workload autoscaling. This enables scale-to-zero and event-driven scaling based on metrics, HTTP traffic, cron schedules, etc.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-19T18:17:49.467026-08:00","updated_at":"2025-12-19T18:17:49.467026-08:00"}
{"id":"TALOS-blw0","title":"Add qBittorrent exporter for Prometheus metrics","description":"Optional: Add prometheus exporter sidecar for qBittorrent metrics (download/upload speeds, active torrents, etc). Create ServiceMonitor for scraping.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-20T13:31:38.694547-08:00","updated_at":"2025-12-20T13:55:29.452586-08:00","closed_at":"2025-12-20T13:55:29.452586-08:00","dependencies":[{"issue_id":"TALOS-blw0","depends_on_id":"TALOS-m6xf","type":"blocks","created_at":"2025-12-20T13:31:38.696793-08:00","created_by":"daemon"}]}
{"id":"TALOS-bns","title":"Add Let's Encrypt ClusterIssuer with Cloudflare DNS-01","description":"Set up Let's Encrypt certificate issuance using DNS-01 challenge via Cloudflare API.\n\n**Components:**\n- ExternalSecret for Cloudflare API token in cert-manager namespace\n- ClusterIssuer for Let's Encrypt (staging + production)\n- Wildcard Certificate for *.knowledgedump.space\n- Update Traefik TLS configuration\n\n**1Password Item:** cloudflare-dns (same token as DDNS)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T18:35:11.982838-08:00","updated_at":"2025-12-16T18:45:29.293146-08:00","closed_at":"2025-12-16T18:45:29.293146-08:00","labels":["cert-manager","cloudflare","infrastructure","letsencrypt"]}
{"id":"TALOS-bxh","title":"Remove linkerd completely","description":"Remove redundant linkerd service mesh and enable mTLS via Cilium instead.\n\n**Why:**\n- Cilium already provides service mesh features (mTLS, observability, L7 policies)\n- Linkerd is barely used (only 12 pods meshed, mostly test apps)\n- Linkerd has policy controller timeout errors\n- Sidecar-less Cilium mesh is more efficient (eBPF-based)\n- Saves ~500Mi+ memory\n\n**Discovery - Files to Remove/Update:**\n\n**Delete entirely:**\n- `infrastructure/base/linkerd/` (directory)\n- `infrastructure/base/namespaces/linkerd.yaml`\n- `infrastructure/base/monitoring/grafana-dashboards/linkerd-dashboards.yaml`\n- `scripts/deploy-linkerd.sh`\n- `scripts/deploy-linkerd-viz.sh`\n- `dashboard.sh` (root - if linkerd-specific)\n\n**Update (remove linkerd references):**\n- `infrastructure/base/namespaces/kustomization.yaml`\n- `infrastructure/base/monitoring/grafana-dashboards/kustomization.yaml`\n- `infrastructure/base/namespaces/scratch.yaml`\n- `docs/02-architecture/service-mesh.md`\n- `docs/02-architecture/infrastructure-diagrams.md`\n- `docs/03-operations/RESOURCE-OPTIMIZATION.md`\n- `docs/08-monitoring/GRAFANA-DASHBOARDS.md`\n- `catalyst_repo.yaml`\n\n**Cluster Resources to Delete:**\n- Namespace: `linkerd`\n- Namespace: `linkerd-viz`\n- 10 CRDs:\n  - authorizationpolicies.policy.linkerd.io\n  - egressnetworks.policy.linkerd.io\n  - externalworkloads.workload.linkerd.io\n  - httplocalratelimitpolicies.policy.linkerd.io\n  - httproutes.policy.linkerd.io\n  - meshtlsauthentications.policy.linkerd.io\n  - networkauthentications.policy.linkerd.io\n  - serverauthorizations.policy.linkerd.io\n  - servers.policy.linkerd.io\n  - serviceprofiles.linkerd.io\n\n**No injection annotations found** - clean removal possible.","design":"**Phase 1: Remove linkerd-viz**\n1. Delete linkerd-viz namespace:\n   ```bash\n   kubectl delete namespace linkerd-viz\n   ```\n2. Remove linkerd-viz from Flux/manifests if managed\n\n**Phase 2: Remove linkerd control plane**\n1. Remove linkerd injection annotations from any pods/namespaces\n2. Delete linkerd namespace:\n   ```bash\n   kubectl delete namespace linkerd\n   ```\n3. Remove linkerd CRDs:\n   ```bash\n   kubectl get crd | grep linkerd | xargs kubectl delete crd\n   ```\n4. Remove from repo:\n   - `infrastructure/base/linkerd/`\n   - `infrastructure/base/namespaces/linkerd.yaml`\n   - `infrastructure/base/monitoring/grafana-dashboards/linkerd-dashboards.yaml`\n\n**Phase 3: Enable Cilium mTLS**\n1. Update `configs/cilium-values.yaml`:\n   ```yaml\n   authentication:\n     mutual:\n       spiffe:\n         enabled: true\n   ```\n2. Regenerate Cilium manifest:\n   ```bash\n   helm template cilium cilium/cilium \\\n     --version 1.16.5 \\\n     --namespace kube-system \\\n     -f configs/cilium-values.yaml \\\n     \u003e configs/cilium-manifest.yaml\n   ```\n3. Apply updated Cilium config\n4. Create CiliumNetworkPolicy to require mTLS for specific namespaces\n\n**Phase 4: Verify**\n1. Confirm linkerd fully removed\n2. Test Cilium mTLS working: `cilium connectivity test`\n3. Check Hubble showing encrypted traffic","acceptance_criteria":"- [ ] linkerd namespace deleted\n- [ ] linkerd-viz namespace deleted\n- [ ] linkerd CRDs removed\n- [ ] linkerd manifests removed from repo\n- [ ] Cilium mTLS enabled: `cilium config view | grep mesh-auth`\n- [ ] mTLS working: `cilium connectivity test` passes\n- [ ] Hubble shows encrypted traffic between services","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-17T12:22:54.077403-08:00","updated_at":"2025-12-17T13:09:44.652936-08:00","closed_at":"2025-12-17T13:09:44.652936-08:00","labels":["cilium","cleanup","linkerd","mtls"]}
{"id":"TALOS-bypd","title":"Refactor llm-scaler: Add React UI with catalyst-ui + Tiltfile dev workflow","description":"Break up the llm-scaler monolithic Go binary with embedded HTML UI into a proper full-stack app:\n\n**Current State:**\n- Go backend with embedded HTML string in ui.go\n- Single Docker image\n- No hot reload development\n\n**Target State:**\n- Go backend (API + WebSocket)\n- React frontend using catalyst-ui components\n- Multi-stage Dockerfile (Node build + Go build)\n- Tiltfile for live reload development connecting to real cluster\n\n**Implementation:**\n1. Create `ui/` directory with React app (Vite + catalyst-ui)\n2. Update Dockerfile for multi-stage build (Node → Go → Runtime)\n3. Modify Go to serve static files from `/ui/dist/`\n4. Add Tiltfile for dev workflow:\n   - Live reload React dev server\n   - Port-forward to real cluster services (ollama, etc.)\n   - Watch and rebuild Go on changes\n5. Keep embedded UI as fallback for single-binary deployment","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-01T19:11:41.953014-08:00","updated_at":"2026-01-01T19:11:41.953014-08:00"}
{"id":"TALOS-c7t","title":"Implement VPN rotation and failover for gluetun","description":"Add automatic VPN server rotation and failover capabilities to the gluetun deployment.\n\nCurrent state:\n- Single ProtonVPN WireGuard key (se-nl-1) hardcoded in deployment\n- 1Password has multiple keys available: se-nl-1, se-de-1, se-be-1, se-in-1\n\nFeatures to implement:\n1. Health-based failover - switch servers when VPN connection drops\n2. Scheduled rotation - rotate servers on a schedule for privacy\n3. Geographic rotation - cycle through different exit countries\n4. Metrics integration - expose rotation events to Prometheus/Grafana\n\nImplementation options:\n- Gluetun control API supports server switching via PUT /v1/vpn/status\n- Could use CronJob or sidecar controller to manage rotation\n- Consider using all 4 ProtonVPN Secure Core keys for rotation pool","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-18T13:22:25.534962-08:00","updated_at":"2025-12-18T13:22:25.534962-08:00","labels":["enhancement","protonvpn","vpn-gateway"]}
{"id":"TALOS-cb9","title":"Fix dashboard: k8s-volumes","description":"Assess and fix/replace k8s-volumes dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:30:34.420148-08:00","updated_at":"2025-12-14T15:57:25.723663-08:00","closed_at":"2025-12-14T15:57:25.723663-08:00","labels":["dashboard","kubernetes"],"dependencies":[{"issue_id":"TALOS-cb9","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:30:34.421034-08:00","created_by":"daemon"}]}
{"id":"TALOS-cgq","title":"Configure Tdarr transcoding profiles for Intel Arc GPU","description":"Set up Tdarr transcoding profiles optimized for Intel Arc GPU with QSV hardware acceleration.\n\n## Environment\n- Intel Arc GPU with QSV support\n- Available encoders: hevc_qsv, av1_qsv, h264_qsv\n- Available decoders: h264_qsv, hevc_qsv, av1_qsv, vp9_qsv\n- Media library: Mixed 1080p and 4K content in single folder\n- Transcode cache: 10Gi tmpfs at /temp\n\n## Transcoding Strategy\n- Target codec: HEVC (max compatibility)\n- Skip files already in: HEVC, AV1, VP9\n- No bitrate filtering needed (codec filter sufficient)\n\n## FFmpeg Command\n```bash\n-hwaccel qsv -hwaccel_output_format qsv \\\n-c:v hevc_qsv -preset medium -global_quality 22 -look_ahead 1 \\\n-c:a copy -c:s copy -max_muxing_queue_size 9999\n```\n\n## Quality Settings (global_quality / ICQ)\n- 18-20: Archival / Transparent quality\n- 22-24: **Recommended** - High quality, good compression\n- 26-28: Streaming / Space saving\n- 30+: Aggressive compression\n\n## Important Notes\n1. HEVC is ~50% smaller than H.264 at same quality\n2. AV1 is ~40-50% smaller than HEVC (but less device support)\n3. Use `-look_ahead 1` for better quality with hevc_qsv\n4. On Linux/Arc: Avoid QSV for H.264 **decoding** (unstable), use software decode + QSV encode\n\n## Alternative AV1 Command (if device support not an issue)\n```bash\n-init_hw_device qsv=hw:/dev/dri/renderD128 -filter_hw_device hw \\\n-c:v av1_qsv -preset veryslow -global_quality 24 \\\n-look_ahead_depth 100 -adaptive_i 1 -adaptive_b 1 \\\n-b_strategy 1 -extbrc 1 \\\n-c:a copy -c:s copy\n```\n\n## Tdarr Flow to Implement\n1. Filter: Skip if codec is hevc, av1, or vp9\n2. Transcode: hevc_qsv -preset medium -global_quality 22 -look_ahead 1\n3. Container: MKV\n4. Audio: Copy\n5. Subtitles: Copy\n\n## References\n- https://github.com/malk-on/av1-qsv-intel-arc-guide\n- https://nelsonslog.wordpress.com/2022/08/22/ffmpeg-and-hevc_qsv-intel-quick-sync-settings/\n- https://arstech.net/video-encoding-bitrates-for-4k-and-1080p-with-h-264-and-h-265/","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T07:58:22.629091-08:00","updated_at":"2025-12-16T16:21:17.269909-08:00","closed_at":"2025-12-16T16:21:17.269909-08:00","labels":["gpu","intel-arc","tdarr","transcoding"]}
{"id":"TALOS-cpl","title":"Enable Cilium mTLS service mesh","description":"Enable mTLS via Cilium after linkerd removal.\n\nRequires maintenance window due to potential brief network disruption during Cilium rollout.","design":"1. Update `configs/cilium-values.yaml`:\n   ```yaml\n   authentication:\n     mutual:\n       spiffe:\n         enabled: true\n   ```\n2. Regenerate Cilium manifest\n3. Apply during low-traffic window\n4. Verify with `cilium connectivity test`","notes":"## Status: Blocked - WireGuard + VXLAN incompatible in this environment\n\n### Testing Summary (Dec 17, 2025):\n1. **WireGuard encryption enabled** - broke cross-node connectivity (100% packet loss)\n2. **MTU adjustment tested** (1370) - no improvement\n3. **Device selection tested** (`devices: \"en+\"`) - no improvement\n4. **WireGuard disabled** - connectivity restored after pod restarts\n\n### Root Cause Analysis:\n- WireGuard + VXLAN double encapsulation works per Cilium docs\n- **BUT** in our environment it consistently breaks cross-node connectivity\n- Possible causes:\n  - Nebula overlay network interference (nebula1 interface detected by Cilium)\n  - Some network layer issue with VXLAN + WireGuard combined\n  - BPF/datapath issue\n\n### Key Learnings:\n1. **Pod restarts required** - Pods retain stale network state after Cilium config changes\n2. **Force delete Cilium pods** - `kubectl rollout restart` may not fully reload config\n3. **Verify configmap changes** - kubectl apply doesn't always update configmap\n\n### Options for mTLS:\n1. **Native routing mode** - Try disabling VXLAN tunnel, use native routing with WireGuard\n2. **IPsec instead of WireGuard** - Different encryption mechanism\n3. **Application-level mTLS** - Use Envoy sidecar or Cilium L7 policies without WireGuard\n\n### References:\n- [Cilium WireGuard Docs](https://docs.cilium.io/en/stable/security/network/encryption-wireguard/)\n- [GitHub Issue #28413](https://github.com/cilium/cilium/issues/28413) - MTU with WireGuard","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-17T12:49:21.818988-08:00","updated_at":"2025-12-17T16:15:43.893097-08:00","closed_at":"2025-12-17T16:15:43.893097-08:00","labels":["cilium","mtls","service-mesh"],"dependencies":[{"issue_id":"TALOS-cpl","depends_on_id":"TALOS-bxh","type":"blocks","created_at":"2025-12-17T12:49:21.820083-08:00","created_by":"daemon"}]}
{"id":"TALOS-d6r","title":"Fix dashboard: kasa-real-time-monitoring","description":"Assess and fix/replace kasa-real-time-monitoring dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:32:09.18377-08:00","updated_at":"2025-12-14T15:57:25.821373-08:00","closed_at":"2025-12-14T15:57:25.821373-08:00","labels":["dashboard","kasa"],"dependencies":[{"issue_id":"TALOS-d6r","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:32:09.18488-08:00","created_by":"daemon"}]}
{"id":"TALOS-d6v","title":"Fix dashboard: loki-logs-quick","description":"Assess and fix/replace loki-logs-quick dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:23.838444-08:00","updated_at":"2025-12-14T17:10:34.788641-08:00","closed_at":"2025-12-14T17:10:34.788641-08:00","labels":["dashboard","logging"],"dependencies":[{"issue_id":"TALOS-d6v","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:23.839602-08:00","created_by":"daemon"}]}
{"id":"TALOS-da3","title":"Deploy Cloudflare DDNS for *.knowledgedump.space","description":"Set up Cloudflare Dynamic DNS to automatically update DNS records when home IP changes.\n\n**Domain:** *.knowledgedump.space (wildcard)\n**1Password Item:** cloudflare-dns (password field contains API token)\n\n**Components:**\n- ExternalSecret to pull CF API token from 1Password\n- Cloudflare DDNS container (favonia/cloudflare-ddns)\n- Deployment with periodic updates","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T18:30:16.908696-08:00","updated_at":"2025-12-16T18:34:00.464027-08:00","closed_at":"2025-12-16T18:34:00.464027-08:00","labels":["cloudflare","dns","infrastructure"]}
{"id":"TALOS-damr","title":"Investigate scheduled cluster rolling restarts","description":"Research whether periodic scheduled rolling restarts of pods/nodes would benefit cluster health and resource efficiency.\n\n## Questions to Answer\n- Would scheduled restarts help with memory leaks in long-running pods?\n- How would this interact with descheduler (TALOS-4dz3)?\n- What's the right cadence (daily, weekly, on low-usage periods)?\n- Should this be pod-level (CronJob-based) or node-level (Talos reboot)?\n- Impact on stateful workloads and PVCs?\n\n## Options to Evaluate\n1. **Kured** - Kubernetes reboot daemon for node restarts\n2. **CronJob + kubectl rollout restart** - Pod-level restarts on schedule\n3. **Descheduler LowNodeUtilization** - Already planned, may achieve similar goals\n4. **Talos scheduled maintenance** - OS-level node rotation\n\n## Considerations\n- Homelab context: less critical than prod, but still want stability\n- Media workloads (Plex, Tdarr) shouldn't restart mid-transcode\n- Could coordinate with KEDA scale-down windows","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-30T14:28:30.254653-08:00","updated_at":"2025-12-30T14:28:30.254653-08:00","labels":["infrastructure","research","scheduling"],"dependencies":[{"issue_id":"TALOS-damr","depends_on_id":"TALOS-4dz3","type":"related","created_at":"2025-12-30T14:28:37.292017-08:00","created_by":"daemon"},{"issue_id":"TALOS-damr","depends_on_id":"TALOS-hcv1","type":"related","created_at":"2025-12-30T14:28:43.474538-08:00","created_by":"daemon"}]}
{"id":"TALOS-dbk","title":"Fix dashboard: linkerd-daemonset","description":"Assess and fix/replace linkerd-daemonset dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:30:49.339557-08:00","updated_at":"2025-12-14T11:30:49.339557-08:00","labels":["dashboard","networking"],"dependencies":[{"issue_id":"TALOS-dbk","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:30:49.34046-08:00","created_by":"daemon"}]}
{"id":"TALOS-dc9","title":"Add search functionality across task domain object","description":"**Feature Request**: Add search/filter capability that searches across the entire task domain object.\n\n**Search fields should include**:\n- Title\n- Description\n- Design notes\n- Acceptance criteria\n- Labels\n- Issue ID\n\n**UI suggestions**:\n- Search input in sidebar above filters\n- Real-time filtering as user types\n- Highlight matching text in results\n\n**Use case**: Quickly find issues by keyword when dealing with large backlogs (118+ issues)","status":"closed","priority":2,"issue_type":"feature","assignee":"subagent-1","created_at":"2025-12-16T10:09:39.527943-08:00","updated_at":"2025-12-16T10:59:01.041677-08:00","closed_at":"2025-12-16T10:59:01.041677-08:00","labels":["beads-manager","feature","ui"]}
{"id":"TALOS-dcd","title":"Debug Docker Registry HTTP push failures via Traefik","description":"Cannot push images via Traefik ingress (registry.talos00).\n\n**Symptoms:**\n- HTTP blob upload returns 404 via Traefik proxy\n- Push works via kubectl port-forward to localhost:5000\n\n**Current workaround:** Use port-forward\n\nFrom: infrastructure/base/traefik/STATUS.md","status":"open","priority":3,"issue_type":"bug","created_at":"2025-12-12T22:36:53.632278-08:00","updated_at":"2025-12-12T22:36:53.632278-08:00","labels":["bug","registry","traefik"]}
{"id":"TALOS-dcw","title":"CloudNativePG Operator - PostgreSQL Database Management","description":"Deploy CloudNativePG operator for declarative PostgreSQL cluster management.\n\n**Goal:** Enable provisioning PostgreSQL databases via CRDs instead of manual deployments.\n\n**Operator:** CloudNativePG (CNCF project)\n- Repo: https://github.com/cloudnative-pg/cloudnative-pg\n- Helm: https://cloudnative-pg.github.io/charts\n\n**CRDs Provided:**\n- `Cluster` - PostgreSQL cluster (primary + replicas)\n- `Backup` - On-demand backups\n- `ScheduledBackup` - Cron-based backups\n- `Pooler` - PgBouncer connection pooling\n\n**Features:**\n- Declarative cluster provisioning\n- Automated failover\n- Backup to S3 (MinIO)\n- WAL archiving\n- Connection pooling via PgBouncer\n- Prometheus metrics\n\n**Integration:**\n- Storage: local-path or fatboy-nfs-appdata\n- Backups: MinIO S3 bucket\n- Monitoring: ServiceMonitor for Prometheus","status":"open","priority":2,"issue_type":"epic","created_at":"2025-12-13T12:29:44.221867-08:00","updated_at":"2025-12-13T12:29:44.221867-08:00","labels":["database","infrastructure","operator","postgresql"]}
{"id":"TALOS-dfv","title":"Verify GPU device and NFD labels on talos02-gpu","description":"Confirm /dev/dri/renderD128 exists, verify NFD labels applied (intel.feature.node.kubernetes.io/gpu=true), check gpu.intel.com/i915 in node allocatable","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T17:02:36.953222-08:00","updated_at":"2025-12-15T13:33:19.545049-08:00","closed_at":"2025-12-15T13:33:19.545049-08:00","dependencies":[{"issue_id":"TALOS-dfv","depends_on_id":"TALOS-fpp","type":"parent-child","created_at":"2025-12-12T17:02:44.445149-08:00","created_by":"daemon"},{"issue_id":"TALOS-dfv","depends_on_id":"TALOS-y1c","type":"blocks","created_at":"2025-12-12T17:02:44.528192-08:00","created_by":"daemon"}]}
{"id":"TALOS-dl1","title":"Fix dashboard: hybrid-cluster-overview","description":"Assess and fix/replace hybrid-cluster-overview dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:54.280301-08:00","updated_at":"2025-12-14T11:31:54.280301-08:00","labels":["dashboard","hybrid-cluster"],"dependencies":[{"issue_id":"TALOS-dl1","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:54.281178-08:00","created_by":"daemon"}]}
{"id":"TALOS-dv3","title":"Deploy ntfy Provider for Flux notifications","description":"Add ntfy.sh as a generic webhook provider in flux-notifications:\n\n1. Create Provider manifest pointing to ntfy.sh (or self-hosted)\n2. Choose a secure/unique topic name\n3. Apply and verify connectivity\n\nConsider: self-hosted ntfy vs public ntfy.sh (security of topic names)","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-12T22:11:20.061496-08:00","updated_at":"2025-12-12T22:11:20.061496-08:00","labels":["flux","notifications"],"dependencies":[{"issue_id":"TALOS-dv3","depends_on_id":"TALOS-8uv","type":"parent-child","created_at":"2025-12-12T22:11:30.761622-08:00","created_by":"daemon"},{"issue_id":"TALOS-dv3","depends_on_id":"TALOS-26f","type":"blocks","created_at":"2025-12-12T22:11:30.917892-08:00","created_by":"daemon"}]}
{"id":"TALOS-e18","title":"Phase 2: Apply Authentik middleware to management services","description":"Apply Authentik ForwardAuth middleware to arr-stack management services.\n\n## Tasks\n- [ ] Verify Authentik middleware is properly configured\n- [ ] Add middleware reference to Sonarr, Radarr, Prowlarr IngressRoutes\n- [ ] Add middleware to Tdarr, Tautulli IngressRoutes\n- [ ] Keep Plex/Jellyfin with app-level auth (they handle it)\n- [ ] Configure Authentik application entries for each service\n- [ ] Test SSO login flow\n\n## Skip Auth For (app handles it)\n- Plex (Plex account auth)\n- Jellyfin (built-in auth)\n- Overseerr (Plex OAuth)","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T17:53:41.55037-08:00","updated_at":"2025-12-17T17:53:41.55037-08:00","labels":["auth","security"],"dependencies":[{"issue_id":"TALOS-e18","depends_on_id":"TALOS-wlu","type":"parent-child","created_at":"2025-12-17T17:53:59.409569-08:00","created_by":"daemon"},{"issue_id":"TALOS-e18","depends_on_id":"TALOS-4v2","type":"blocks","created_at":"2025-12-17T17:54:12.008559-08:00","created_by":"daemon"}]}
{"id":"TALOS-e9x","title":"Add WireGuard metrics exporter and Grafana dashboard","description":"Export WireGuard tunnel metrics (handshakes, bytes transferred, peer status) to Mimir and create Grafana dashboard for monitoring VPN health.","design":"Options:\n1. prometheus-wireguard-exporter - Prometheus exporter for WireGuard\n2. wireguard_exporter - Rust-based exporter\n3. Custom script with node_exporter textfile collector\n\nNeed to scrape from talos01 where wg0 interface exists.","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-18T07:39:14.728852-08:00","updated_at":"2025-12-18T07:39:14.728852-08:00","labels":["grafana","monitoring","vpn"],"dependencies":[{"issue_id":"TALOS-e9x","depends_on_id":"TALOS-y1b","type":"blocks","created_at":"2025-12-18T07:39:14.730864-08:00","created_by":"daemon"}]}
{"id":"TALOS-ed1","title":"Fix dashboard: cilium-policy-verdicts","description":"Assess and fix/replace cilium-policy-verdicts dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:30:49.249505-08:00","updated_at":"2025-12-14T17:59:15.35231-08:00","closed_at":"2025-12-14T17:59:15.35231-08:00","labels":["dashboard","networking"],"dependencies":[{"issue_id":"TALOS-ed1","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:30:49.250499-08:00","created_by":"daemon"}]}
{"id":"TALOS-eoq","title":"Create databases namespace and base structure","description":"Create the databases namespace and base kustomization structure for hosting the database operators.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T11:30:40.230775-08:00","updated_at":"2025-12-16T11:42:51.324329-08:00","closed_at":"2025-12-16T11:42:51.324329-08:00","labels":["database","infrastructure"],"dependencies":[{"issue_id":"TALOS-eoq","depends_on_id":"TALOS-57l","type":"parent-child","created_at":"2025-12-16T11:30:58.814848-08:00","created_by":"daemon"}]}
{"id":"TALOS-eq8","title":"Create liqo dashboard","description":"infrastructure/base/liqo/dashboard.sh - control plane, foreign clusters, virtual nodes, peering health","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.603828-08:00","updated_at":"2025-12-12T18:55:36.430662-08:00","closed_at":"2025-12-12T18:55:36.430662-08:00"}
{"id":"TALOS-er9s","title":"LDAP Identity Management for NFS Consistency","description":"Set up centralized LDAP identity management to fix NFS permission mismatches across Synology, TrueNAS, and Kubernetes pods.\n\n**Problem:**\n- Synology: uid 1026 = panda, gid 100 = users\n- K8s LinuxServer containers: uid 1000 = abc\n- TrueNAS: uid 3000 = media group\n- Result: Permission denied errors when apps try to write to NFS mounts\n\n**Solution:**\nSynology LDAP Server (ldap.talos00.local) as central identity provider\n\n**Current State:**\n- Synology LDAP Server package installed\n- Default groups exist: users, Directory Operators, Directory Clients, Directory Consumers, administrators","design":"## Architecture\n\n```\n┌─────────────────┐\n│ Synology LDAP   │ ← Central identity (ldap.talos00.local)\n│ (ldap.talos00)  │\n└────────┬────────┘\n         │\n    ┌────┴────┬──────────┐\n    ▼         ▼          ▼\n┌───────┐ ┌───────┐ ┌──────────┐\n│Synology│ │TrueNAS│ │ K8s Pods │\n│  NFS   │ │  NFS  │ │(optional)│\n└────────┘ └───────┘ └──────────┘\n```\n\n## User/Group Strategy\n\n**Groups to create in LDAP:**\n- `media` (gid 1000) - All media apps (sonarr, radarr, plex, etc.)\n- `downloads` (gid 1001) - Download clients (sabnzbd, etc.)\n\n**Users to create:**\n- `mediaservice` (uid 1000, primary group: media) - Service account for all *arr apps\n- `panda` (uid 1001, groups: media, users) - Human admin\n\n## Implementation Steps\n\n1. **LDAP Server Config**\n   - Create media group (gid 1000)\n   - Create mediaservice user (uid 1000)\n   - Create panda user with proper groups\n\n2. **Synology NFS Integration**\n   - Enable NFSv4 idmapd\n   - Configure idmapd to use LDAP\n   - Update NFS exports\n\n3. **TrueNAS Integration** (optional)\n   - Configure LDAP client\n   - Update dataset permissions\n\n4. **File Permission Migration**\n   - chown media shares to new uid/gid\n   - Verify access from K8s pods\n\n5. **K8s Pod Configuration**\n   - Update securityContext to use uid 1000\n   - Or deploy SSSD sidecar for full LDAP integration","acceptance_criteria":"- [ ] LDAP has media group (gid 1000) and mediaservice user (uid 1000)\n- [ ] Synology NFS uses LDAP for identity mapping\n- [ ] K8s pods can write to /data/synology/tv without permission errors\n- [ ] Sonarr can add /data/synology/tv as root folder\n- [ ] File ownership is consistent across all NFS mounts","notes":"## Completed\n\nLDAP and NFS permission setup complete:\n- Synology LDAP Server running at ldap.talos00.local\n- User `k8s` created with UID 1000 (after disabling UID shifting in Domain/LDAP settings)\n- Synology joined to its own LDAP as client\n- NFS export set to \"squash all users to admin\" - fixes all permission issues\n\nVerified working:\n- Sonarr can access /data/synology/tv (accessible: true)\n- Sonarr can access /data/truenas/tv (accessible: true)  \n- Tdarr can write to all Synology paths (movies, tv, archive)\n- Radarr can write to Synology downloads\n- SABnzbd can write to downloads\n\nRemote Path Mapping configured:\n- Sonarr: SABnzbd `/downloads/complete` → `/data/downloads/complete`\n\n**User action needed (UI only):**\n- Sonarr Mass Editor: Reassign series from old root folders to /data/synology/tv\n- Delete old inaccessible root folders (1-7) in Settings","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-20T13:37:23.167672-08:00","updated_at":"2025-12-20T14:28:18.349908-08:00","closed_at":"2025-12-20T14:28:18.349908-08:00","labels":["infrastructure","ldap","nfs","synology"]}
{"id":"TALOS-eujl","title":"Test and verify pull-through cache working","description":"Deploy test pods pulling from each registry, verify traffic routes through Nexus proxy, confirm cache hits on second pull.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-20T16:18:16.129529-08:00","updated_at":"2025-12-20T16:18:16.129529-08:00","dependencies":[{"issue_id":"TALOS-eujl","depends_on_id":"TALOS-wjb9","type":"blocks","created_at":"2025-12-20T16:18:16.130386-08:00","created_by":"daemon"}]}
{"id":"TALOS-fckq","title":"Update Talos machine config with registry mirrors","description":"Add `machine.registries.mirrors` section to Talos controlplane/worker configs to route image pulls through local Nexus proxies. Requires node reboot to apply.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-20T16:18:15.969486-08:00","updated_at":"2025-12-20T16:18:15.969486-08:00","dependencies":[{"issue_id":"TALOS-fckq","depends_on_id":"TALOS-wjb9","type":"blocks","created_at":"2025-12-20T16:18:15.971181-08:00","created_by":"daemon"}]}
{"id":"TALOS-fcm","title":"Fix dashboard: k8s-pvc","description":"Assess and fix/replace k8s-pvc dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:30:34.337406-08:00","updated_at":"2025-12-14T15:57:25.773895-08:00","closed_at":"2025-12-14T15:57:25.773895-08:00","labels":["dashboard","kubernetes"],"dependencies":[{"issue_id":"TALOS-fcm","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:30:34.338366-08:00","created_by":"daemon"}]}
{"id":"TALOS-fds","title":"Audit: All namespaces have dashboard coverage","description":"Verify every namespace (25 total) is covered by at least one dashboard section","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:36.014898-08:00","updated_at":"2025-12-12T18:57:04.534848-08:00","closed_at":"2025-12-12T18:57:04.534848-08:00"}
{"id":"TALOS-ffh","title":"Deploy MinIO Operator","description":"Deploy MinIO Operator to databases namespace via HelmRelease. This enables S3 tenant provisioning via `Tenant` CRD.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T11:30:40.434201-08:00","updated_at":"2025-12-16T11:42:51.548212-08:00","closed_at":"2025-12-16T11:42:51.548212-08:00","labels":["database","minio","operator","s3"],"dependencies":[{"issue_id":"TALOS-ffh","depends_on_id":"TALOS-eoq","type":"blocks","created_at":"2025-12-16T11:30:58.440836-08:00","created_by":"daemon"}]}
{"id":"TALOS-fooo","title":"Add Windows PC as Tdarr worker node","description":"Set up a Windows PC to join the Kubernetes Tdarr control plane as an external worker node. The PC should remain usable for normal Windows tasks while contributing transcoding capacity.\n\n## Requirements\n- Windows PC stays fully functional (not dedicated/headless)\n- Joins existing Tdarr server running in K8s (media namespace)\n- Should leverage Windows GPU (Intel QSV? NVIDIA?)\n- Tdarr node runs as Windows service or tray app\n- Can be paused/throttled when PC in active use\n\n## Implementation Steps\n1. Get Tdarr server external access (NodePort or Ingress)\n2. Install Tdarr Node on Windows (standalone installer)\n3. Configure node to connect to K8s Tdarr server IP/hostname\n4. Set up GPU passthrough (if applicable)\n5. Configure resource limits (CPU%, GPU priority)\n6. Test transcode jobs routing to Windows node\n\n## Network Considerations\n- Tdarr server needs to be reachable from Windows PC\n- Media files need to be accessible (NFS mount or SMB share?)\n- Current Tdarr server: ClusterIP only, needs exposure\n\n## Questions\n- Which Windows PC? What GPU does it have?\n- SMB mount for media access, or separate NFS client?\n- Run as service (always on) or tray app (on-demand)?","notes":"## Progress\n- Created NodePort service (port 30266) for external node access\n- Created Windows config files in `configs/tdarr-windows/`\n- Service applied to cluster\n\n## Remaining\n- User needs to copy config to Windows PC\n- Map network drives on Windows\n- Start Tdarr Node and verify connection","status":"in_progress","priority":2,"issue_type":"feature","created_at":"2025-12-30T14:31:28.01958-08:00","updated_at":"2025-12-30T14:35:27.941414-08:00","labels":["hybrid","tdarr","windows"]}
{"id":"TALOS-fpp","title":"Set up talos02-gpu worker node with Intel Arc","description":"Configure new ASUS NUC 15 Pro (RNUC15U5) as GPU-enabled worker node for media transcoding workloads (Plex, Tdarr). Intel Core Ultra 5 225H with integrated Intel Arc GPU (Xe2 architecture).","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-12T16:53:20.891543-08:00","updated_at":"2025-12-15T13:33:30.861958-08:00","closed_at":"2025-12-15T13:33:30.861958-08:00"}
{"id":"TALOS-fq8","title":"Fix/replace Grafana dashboards for V2 OTEL stack","description":"Dashboards have label mismatches - metrics use `instance` but dashboards expect `kubernetes_io_hostname`. Need to evaluate each dashboard category and fix or replace with OTEL-compatible alternatives.","design":"## Dashboard Assessment Plan\n\n### Problem\nMetrics in Mimir (via Alloy) have different labels than traditional kube-prometheus-stack:\n- Our metrics: `instance` label for nodes\n- Expected: `kubernetes_io_hostname` label\n\n### Available Metrics (1683 total)\n- `kube_*` - kube-state-metrics ✅ Working\n- `container_*` - cAdvisor (labels need mapping)\n- `node_*` - NOT PRESENT (node-exporter not scraped)\n- `alloy_*` - Alloy self-metrics\n- `cilium_*`, `hubble_*` - Cilium metrics\n- `linkerd_*` - Linkerd metrics\n- App-specific exporters\n\n### Dashboard Categories (43 total)\n\n**Kubernetes (6)**\n- k8s-cluster-monitoring\n- k8s-comprehensive  \n- k8s-monitoring-overview\n- k8s-pods-view\n- k8s-pvc\n- k8s-volumes\n\n**Networking (9)**\n- cilium-agent, cilium-hubble, cilium-hubble-flows, cilium-operator, cilium-policy-verdicts\n- linkerd-daemonset, linkerd-deployment, linkerd-route, linkerd-service, linkerd-top-line\n- traefik-services, traefik-v2-alt\n\n**Logging (5)**\n- fluent-bit\n- loki-kubernetes-logs, loki-logs-quick, loki-promtail, loki-stack-monitoring\n\n**Infrastructure (4)**\n- node-exporter-full\n- pod-cleanup\n- postgresql-database\n- resource-efficiency\n\n**Observability (4)**\n- graylog-metrics\n- mongodb-cluster-summary, mongodb-instance-summary\n- opensearch-exporter\n\n**GitOps (2)**\n- argocd-notifications\n- goldilocks-vpa\n\n**Hybrid Cluster (4)**\n- aws-ec2-instances\n- hybrid-cluster-overview\n- liqo-overview\n- llm-scaler\n\n**Kasa (6)**\n- kasa-alerts-monitoring, kasa-battery-sizing, kasa-comparative-analytics\n- kasa-forecasting-analytics, kasa-real-time-monitoring, kasa-tou-cost-optimization\n\n### Strategy\n1. Test each dashboard for data availability\n2. Fix label issues via Alloy relabeling OR dashboard modifications\n3. Replace dashboards that need complete rewrites with OTEL-native alternatives\n4. Add missing scrape configs (node-exporter) to Alloy","notes":"## Progress Update - Traefik Dashboards Fixed\n\n**Issue:** Traefik dashboards showing \"No data\" \n\n**Root Causes:**\n1. Traefik wasn't exporting service/entrypoint/router metric labels\n2. No Alloy scraper configured for Traefik after kube-prometheus-stack migration\n\n**Fixes (commits 7b15ea7, 5987995):**\n1. Traefik HelmRelease: Added `addEntryPointsLabels`, `addRoutersLabels`, `addServicesLabels=true`\n2. Alloy HelmRelease: Added dedicated Traefik scraper for `traefik-metrics:9100`\n\n**Note:** DaemonSet with hostPorts requires manual pod deletion for updates (rolling update deadlocks on port conflict).\n\n**Status:** Traefik dashboards now showing metrics (status codes, response times, entrypoints)","status":"in_progress","priority":2,"issue_type":"task","created_at":"2025-12-14T11:29:45.000516-08:00","updated_at":"2025-12-17T18:50:39.538668-08:00"}
{"id":"TALOS-fwrz","title":"Deploy Home Assistant to cluster","description":"Deploy Home Assistant to the Kubernetes cluster for home automation.\n\n**Goals:**\n- Central hub for smart home devices\n- Integration with existing infrastructure (Authentik SSO, monitoring)\n- Bluetooth device support via ESPHome proxies\n- Z-Wave/Zigbee support (if hardware available)\n\n**Components:**\n- Home Assistant Core container\n- PostgreSQL database (via CloudNativePG operator)\n- Persistent storage for config\n- Traefik IngressRoute with TLS\n- Authentik integration for external access\n\n**Hardware integrations to consider:**\n- Bluetooth (via ESPHome Bluetooth Proxies)\n- Z-Wave (USB stick)\n- Zigbee (USB stick or Zigbee2MQTT)\n- Matter/Thread","design":"## Deployment Architecture\n\n### Phase 1: Core Deployment\n1. Create `home-assistant` namespace\n2. Deploy Home Assistant via Helm or raw manifests\n3. Configure PostgreSQL via CloudNativePG Cluster CR\n4. Set up persistent storage (fatboy-nfs-appdata)\n5. Create IngressRoute: `homeassistant.talos00` (internal) + `ha.knowledgedump.space` (external)\n\n### Phase 2: Integrations\n1. Configure Authentik OIDC provider for HA\n2. Set up Prometheus metrics exporter\n3. Add Grafana dashboard for HA metrics\n\n### Phase 3: Device Support\n1. ESPHome Bluetooth Proxies for BLE devices\n2. USB device passthrough for Z-Wave/Zigbee (if available)\n3. Matter/Thread support\n\n### Storage\n- Config: PVC on fatboy-nfs-appdata\n- Database: PostgreSQL cluster with backup\n\n### Ingress\n- Internal: `homeassistant.talos00`\n- External: `ha.knowledgedump.space` (with Authentik SSO)","acceptance_criteria":"- [ ] Home Assistant pod running and healthy\n- [ ] PostgreSQL database configured\n- [ ] Internal ingress working (homeassistant.talos00)\n- [ ] External ingress with Authentik SSO\n- [ ] At least one integration working (weather, etc)\n- [ ] Metrics exported to Mimir","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-19T21:54:16.559878-08:00","updated_at":"2025-12-19T21:54:16.559878-08:00","labels":["home-automation","infrastructure"],"dependencies":[{"issue_id":"TALOS-fwrz","depends_on_id":"TALOS-oik1","type":"related","created_at":"2025-12-19T21:59:37.775101-08:00","created_by":"daemon"}]}
{"id":"TALOS-g3b","title":"Add GitHub social login to Authentik","description":"Configure GitHub as an OAuth source in Authentik for social login. Users can authenticate with their GitHub account instead of local credentials.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-16T21:09:22.148992-08:00","updated_at":"2025-12-16T21:09:22.148992-08:00","labels":["authentik","sso"]}
{"id":"TALOS-g4g","title":"Configure Jellyfin API key for homepage widget","description":"Jellyfin API keys must be created manually via the Jellyfin web UI:\n1. Go to http://jellyfin.talos00 → Dashboard → API Keys\n2. Create a new API key\n3. Add JELLYFIN_API_KEY to arr-api-keys secret manually:\n   kubectl patch secret arr-api-keys -n media --type=merge -p '{\"stringData\":{\"JELLYFIN_API_KEY\":\"your-key-here\"}}'\n4. Restart homepage deployment","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-15T17:48:25.026337-08:00","updated_at":"2025-12-15T17:48:25.026337-08:00","labels":["api-keys","homepage","jellyfin"]}
{"id":"TALOS-gia","title":"Fix dashboard: k8s-pods-view","description":"Assess and fix/replace k8s-pods-view dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:30:34.256181-08:00","updated_at":"2025-12-14T12:52:14.517097-08:00","closed_at":"2025-12-14T12:52:14.517097-08:00","labels":["dashboard","kubernetes"],"dependencies":[{"issue_id":"TALOS-gia","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:30:34.257217-08:00","created_by":"daemon"}]}
{"id":"TALOS-gu8","title":"Fix dashboard: kasa-alerts-monitoring","description":"Assess and fix/replace kasa-alerts-monitoring dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:32:08.868575-08:00","updated_at":"2025-12-14T15:57:25.872961-08:00","closed_at":"2025-12-14T15:57:25.872961-08:00","labels":["dashboard","kasa"],"dependencies":[{"issue_id":"TALOS-gu8","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:32:08.869552-08:00","created_by":"daemon"}]}
{"id":"TALOS-gxu","title":"Create intel-gpu dashboard","description":"infrastructure/base/intel-gpu/dashboard.sh - NFD status, GPU labels, device plugin, allocatable resources","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.722026-08:00","updated_at":"2025-12-12T18:55:36.518316-08:00","closed_at":"2025-12-12T18:55:36.518316-08:00"}
{"id":"TALOS-hcv1","title":"Define KEDA + Descheduler coordination policy","description":"Document and implement policy for how KEDA and Descheduler interact to prevent conflicts.\n\nKey decisions needed:\n- Which namespaces/workloads are KEDA-managed (scale-to-zero eligible)\n- Which namespaces/workloads are descheduler-managed (rebalancing eligible)\n- Mutual exclusion rules (a workload should not be both)\n- Label convention for identifying KEDA vs static workloads\n- PDB requirements for rebalanced workloads","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-19T18:22:16.296922-08:00","updated_at":"2025-12-19T18:22:16.296922-08:00","dependencies":[{"issue_id":"TALOS-hcv1","depends_on_id":"TALOS-b1gd","type":"blocks","created_at":"2025-12-19T18:22:21.896051-08:00","created_by":"daemon"}]}
{"id":"TALOS-hlk","title":"Fix dashboard: linkerd-route","description":"Assess and fix/replace linkerd-route dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:04.186954-08:00","updated_at":"2025-12-14T11:31:04.186954-08:00","labels":["dashboard","networking"],"dependencies":[{"issue_id":"TALOS-hlk","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:04.187905-08:00","created_by":"daemon"}]}
{"id":"TALOS-ho2k","title":"Provision talos06 - GMKtec Nucbox EVO T1 (Intel Core Ultra 9 285H + Arc 140T)","description":"Set up new Talos worker node with highest-performance Intel Arc GPU in cluster.\n\nHardware:\n- CPU: Intel Core Ultra 9 285H (24 cores, 5.4GHz)\n- RAM: 64GB\n- GPU: Intel Arc 140T (8 Xe cores, AV1 encode/decode)\n- NPU: Intel AI Boost (13 TOPS) - no Talos extension yet\n\nExtensions: intel-ucode, i915, mei\nSchematic ID: 16be3b98edc75480e2b2dfdce9eaf6544b2ffff8bd0494dc845e76c1b53870c9","acceptance_criteria":"- [ ] Download Talos ISO from factory\n- [ ] Boot node with ISO\n- [ ] Get DHCP IP and update worker config\n- [ ] Apply config with talosctl\n- [ ] Verify node joins cluster\n- [ ] Verify GPU device /dev/dri/renderD128 exists\n- [ ] Verify NFD labels applied\n- [ ] Test AV1 transcoding with Tdarr","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-01T08:32:38.537669-08:00","updated_at":"2026-01-01T13:14:12.92947-08:00","closed_at":"2026-01-01T13:14:12.92947-08:00"}
{"id":"TALOS-i11","title":"Add HTTPS/TLS to Traefik with cert-manager","description":"Currently all services are HTTP only. Implement HTTPS:\n\n- Deploy cert-manager\n- Configure Let's Encrypt certificates (or self-signed for internal)\n- Update Traefik IngressRoutes for TLS\n- Add rate limiting middleware\n\nFrom: docs/_archive/TODO.md","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-12T22:20:28.525447-08:00","updated_at":"2025-12-12T22:20:28.525447-08:00","labels":["infrastructure","security","traefik"]}
{"id":"TALOS-inn1","title":"Create ExternalSecret for arr-stack 1Password integration","description":"Create ExternalSecret to pull arr-stack secrets from 1Password:\n\n**1Password Item:** `arr-stack`\n\n**Fields to sync:**\n- sonarr.apiKey\n- radarr.apiKey  \n- prowlarr.apiKey\n- sonarr.encryptionKey\n- radarr.encryptionKey\n- prowlarr.encryptionKey\n\n**Files:**\n- `applications/arr-stack/base/shared/external-secret.yaml`\n\n**Prereq:** User must create 1Password item with values from plan.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T09:35:19.337904-08:00","updated_at":"2025-12-20T10:05:38.001543-08:00","closed_at":"2025-12-20T10:05:38.001543-08:00","dependencies":[{"issue_id":"TALOS-inn1","depends_on_id":"TALOS-n8an","type":"parent-child","created_at":"2025-12-20T09:35:25.914581-08:00","created_by":"daemon"}]}
{"id":"TALOS-is9u","title":"Create migration Job and ConfigMap manifests","description":"Create Kubernetes manifests for migration:\n\n1. **migration/job.yaml** - One-time Job that:\n   - Mounts source data (hostPath or PVC with .drogon-app)\n   - Mounts target config PVC\n   - Runs migration scripts\n\n2. **migration/configmap.yaml** - Scripts for:\n   - Config file copying (config.xml, asp/)\n   - SQLite → PostgreSQL conversion (pgloader or custom)\n   - Validation checks\n\nLocation: `applications/arr-stack/base/migration/`","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T09:24:10.747679-08:00","updated_at":"2025-12-20T10:06:46.483308-08:00","closed_at":"2025-12-20T10:06:46.483308-08:00","dependencies":[{"issue_id":"TALOS-is9u","depends_on_id":"TALOS-n8an","type":"parent-child","created_at":"2025-12-20T09:24:23.070376-08:00","created_by":"daemon"},{"issue_id":"TALOS-is9u","depends_on_id":"TALOS-wm2f","type":"blocks","created_at":"2025-12-20T09:24:39.937508-08:00","created_by":"daemon"},{"issue_id":"TALOS-is9u","depends_on_id":"TALOS-inn1","type":"blocks","created_at":"2025-12-20T09:35:25.963597-08:00","created_by":"daemon"}]}
{"id":"TALOS-j5a","title":"Fix ArgoCD widget","description":"Verify ArgoCD widget is working. Service URL looks correct (argocd-server.argocd.svc.cluster.local) but may need API key configuration.","status":"open","priority":2,"issue_type":"bug","created_at":"2025-12-15T15:48:20.643095-08:00","updated_at":"2025-12-15T15:48:20.643095-08:00","labels":["argocd","homepage"]}
{"id":"TALOS-jtp","title":"Suspend kube-prometheus-stack HelmRelease","description":"Suspend the HelmRelease before removal to validate metrics flow.\n\n**Steps:**\n1. Suspend HelmRelease (keeps resources but stops reconciliation)\n2. Verify Alloy still scraping kube-state-metrics\n3. Verify Mimir receiving metrics\n4. Monitor for 10-15 minutes\n\n**Rollback:** Unsuspend if issues detected.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-13T22:26:51.539689-08:00","updated_at":"2025-12-13T22:34:55.494869-08:00","closed_at":"2025-12-13T22:34:55.494869-08:00","labels":["migration","monitoring"],"dependencies":[{"issue_id":"TALOS-jtp","depends_on_id":"TALOS-zqh","type":"parent-child","created_at":"2025-12-13T22:27:04.77225-08:00","created_by":"daemon"},{"issue_id":"TALOS-jtp","depends_on_id":"TALOS-ofm","type":"blocks","created_at":"2025-12-13T22:27:04.896508-08:00","created_by":"daemon"}]}
{"id":"TALOS-jzy","title":"Deploy Grafana Loki for log storage","description":"Deploy Loki as lightweight log aggregation system.\n\n**Mode:** Single binary or simple-scalable (start simple)\n\n**Storage:** \n- NFS for chunks (or local-path for eval)\n- 30-day retention to match current\n\n**Integration:**\n- OTEL Collector exports logs via Loki exporter\n- Add Loki datasource to existing Grafana","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T09:20:16.045711-08:00","updated_at":"2025-12-13T22:23:07.468614-08:00","closed_at":"2025-12-13T22:23:07.468614-08:00","labels":["logs","loki","otel"],"dependencies":[{"issue_id":"TALOS-jzy","depends_on_id":"TALOS-nh8","type":"parent-child","created_at":"2025-12-13T09:20:28.256726-08:00","created_by":"daemon"},{"issue_id":"TALOS-jzy","depends_on_id":"TALOS-3of","type":"blocks","created_at":"2025-12-13T09:20:28.471317-08:00","created_by":"daemon"}]}
{"id":"TALOS-k2g","title":"Fix dashboard: kasa-battery-sizing","description":"Assess and fix/replace kasa-battery-sizing dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:32:08.938054-08:00","updated_at":"2025-12-14T15:58:15.769977-08:00","closed_at":"2025-12-14T15:58:15.769977-08:00","labels":["dashboard","kasa"],"dependencies":[{"issue_id":"TALOS-k2g","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:32:08.938925-08:00","created_by":"daemon"}]}
{"id":"TALOS-k5w","title":"Add label relabeling in Alloy for dashboard compatibility","description":"Dashboards expect kubernetes_io_hostname label but our metrics have instance. Add relabeling to create compatible labels.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-14T11:32:25.728211-08:00","updated_at":"2025-12-14T12:17:49.774774-08:00","closed_at":"2025-12-14T12:17:49.774774-08:00","labels":["alloy","infrastructure","scraping"]}
{"id":"TALOS-kar","title":"Configure retention limits for Mimir, Loki, Tempo + MinIO quotas","description":"Configure time-based retention and MinIO bucket quotas for observability stack:\n\n**Retention:**\n- Mimir: 1 year (8760h) - full resolution metrics\n- Loki: 1 month (720h) - logs\n- Tempo: 7 days (168h) - traces\n\n**MinIO Bucket Quotas:**\n- mimir: 500GB\n- loki: 300GB\n- tempo: 200GB\n\nFiles modified:\n- infrastructure/base/monitoring/v2-otel/mimir/helmrelease.yaml\n- infrastructure/base/monitoring/v2-otel/loki/helmrelease.yaml\n- infrastructure/base/monitoring/v2-otel/tempo/helmrelease.yaml","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T08:50:51.324672-08:00","updated_at":"2025-12-16T08:53:16.147266-08:00","closed_at":"2025-12-16T08:53:16.147266-08:00"}
{"id":"TALOS-l13q","title":"Create llm-scaler-v2 as KEDA-based POC","description":"Create a parallel llm-scaler-v2 project using KEDA instead of custom scaler. MVP to prove KEDA can replace custom scaling logic for Ollama/LLM workloads. Keep existing llm-scaler running until validated.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-19T18:17:50.645446-08:00","updated_at":"2025-12-19T18:17:50.645446-08:00","dependencies":[{"issue_id":"TALOS-l13q","depends_on_id":"TALOS-b1gd","type":"blocks","created_at":"2025-12-19T18:17:58.437702-08:00","created_by":"daemon"},{"issue_id":"TALOS-l13q","depends_on_id":"TALOS-txzj","type":"blocks","created_at":"2025-12-19T18:17:58.481364-08:00","created_by":"daemon"},{"issue_id":"TALOS-l13q","depends_on_id":"TALOS-qofw","type":"related","created_at":"2025-12-19T18:35:40.01602-08:00","created_by":"daemon"}]}
{"id":"TALOS-lqo","title":"Build beads-manager UI tool","description":"Visual task management UI for beads with:\n- Force-directed dependency graph (catalyst-ui ForceGraph)\n- Live markdown editor (Milkdown)\n- Full CRUD operations\n- Bridge server for real-time updates\n- Tilt dev flow","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-16T08:21:28.590253-08:00","updated_at":"2025-12-16T08:50:59.327058-08:00","closed_at":"2025-12-16T08:50:59.327058-08:00"}
{"id":"TALOS-m0x","title":"Create Mac provisioning for ntfy-desktop (Homebrew/Ansible)","description":"Create automated installation for ntfy-desktop on developer Macs:\n\n**Options to evaluate:**\n1. **Homebrew Cask** - Check if ntfy-desktop has one, or create a custom tap\n2. **Ansible role** - Add to @machines bootstrap playbook\n3. **Shell script** - Simple curl/install script in dotfiles\n\n**Should include:**\n- Download correct binary (darwin-arm64 vs darwin-amd64)\n- Install to /Applications or ~/Applications\n- Pre-configure with cluster topic subscription\n- Set up launch at login (optional)\n- Configure notification preferences\n\n**Integration points:**\n- `catalyst/@machines` - Ansible bootstrap\n- `catalyst/@dotfiles-2024` - Brewfile or install script\n- Document in workspace onboarding docs","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-12T22:16:36.775827-08:00","updated_at":"2025-12-12T22:16:36.775827-08:00","labels":["devex","macos","notifications","provisioning"],"dependencies":[{"issue_id":"TALOS-m0x","depends_on_id":"TALOS-8uv","type":"parent-child","created_at":"2025-12-12T22:16:43.086097-08:00","created_by":"daemon"},{"issue_id":"TALOS-m0x","depends_on_id":"TALOS-v3a","type":"blocks","created_at":"2025-12-12T22:16:43.110579-08:00","created_by":"daemon"}]}
{"id":"TALOS-m3i","title":"Fix Grafana dashboards for V2 OTEL stack","description":"Update all Grafana dashboards to use Mimir datasource instead of Prometheus.\n\n**Changes needed:**\n1. Remove dead prometheus-v2 datasource (points to deleted kube-prometheus-stack)\n2. Update all GrafanaDashboard CRs: `datasourceName: \"Prometheus\"` → `datasourceName: \"Mimir\"`\n\n**Affected files:**\n- infrastructure/base/monitoring/v2-otel/grafana-datasources/datasources.yaml\n- All files in infrastructure/base/monitoring/grafana-dashboards/","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-13T22:40:51.581279-08:00","updated_at":"2025-12-13T22:44:43.178448-08:00","closed_at":"2025-12-13T22:44:43.178448-08:00","labels":["dashboards","grafana","monitoring"]}
{"id":"TALOS-m6xf","title":"Integrate qBittorrent with VPN sidecar into arr-stack","description":"Add qBittorrent torrent client to the arr-stack with all traffic routed through gluetun VPN sidecar (ProtonVPN WireGuard). This follows the same pattern used for SecureXNG vpn-gateway deployment.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-20T13:31:11.029367-08:00","updated_at":"2025-12-20T13:55:29.503728-08:00","closed_at":"2025-12-20T13:55:29.503728-08:00","labels":["arr-stack","infrastructure","vpn"]}
{"id":"TALOS-m7e","title":"Phase 4: Migrate PostgreSQL password to ExternalSecret","description":"Replace hardcoded PostgreSQL password with 1Password-sourced ExternalSecret.\n\n## Tasks\n- [ ] Generate strong random password\n- [ ] Store in 1Password vault\n- [ ] Create ExternalSecret for postgresql-secret\n- [ ] Update connection strings to use secretKeyRef\n- [ ] Remove hardcoded password from secret.yaml\n- [ ] Update sonarr/db-config.yaml to source from env\n- [ ] Rotate password in running PostgreSQL\n- [ ] Test all *arr apps still connect\n\n## Current Issue\npostgresql/secret.yaml contains: `postgres-password: 'changeme123'`","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T17:53:44.553191-08:00","updated_at":"2025-12-17T17:53:44.553191-08:00","labels":["secrets","security"],"dependencies":[{"issue_id":"TALOS-m7e","depends_on_id":"TALOS-wlu","type":"parent-child","created_at":"2025-12-17T17:54:01.757095-08:00","created_by":"daemon"}]}
{"id":"TALOS-mhk","title":"Deploy Grafana Tempo for distributed tracing","description":"Deploy Tempo for trace storage - NEW capability!\n\n**Mode:** Single binary (monolithic for homelab scale)\n\n**Storage:** Local or NFS\n\n**Integration:**\n- OTEL Collector exports traces via OTLP\n- Add Tempo datasource to existing Grafana\n- Enable trace-to-logs correlation","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T09:20:16.101749-08:00","updated_at":"2025-12-13T22:23:07.522863-08:00","closed_at":"2025-12-13T22:23:07.522863-08:00","labels":["otel","tempo","traces"],"dependencies":[{"issue_id":"TALOS-mhk","depends_on_id":"TALOS-nh8","type":"parent-child","created_at":"2025-12-13T09:20:28.309172-08:00","created_by":"daemon"},{"issue_id":"TALOS-mhk","depends_on_id":"TALOS-3of","type":"blocks","created_at":"2025-12-13T09:20:28.52501-08:00","created_by":"daemon"}]}
{"id":"TALOS-mun","title":"Fix dashboard: fluent-bit","description":"Assess and fix/replace fluent-bit dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:23.687325-08:00","updated_at":"2025-12-14T13:04:56.36135-08:00","closed_at":"2025-12-14T13:04:56.36135-08:00","labels":["dashboard","logging"],"dependencies":[{"issue_id":"TALOS-mun","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:23.68826-08:00","created_by":"daemon"}]}
{"id":"TALOS-n3q","title":"Create scratch namespace with example database instances","description":"Create scratch namespace with example CRs demonstrating all three operators:\n- PostgreSQL Cluster CR\n- MongoDBCommunity CR  \n- MinIO Tenant CR\n\nThese serve as reference implementations for other apps.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T11:30:40.540989-08:00","updated_at":"2025-12-16T11:44:09.427948-08:00","closed_at":"2025-12-16T11:44:09.427948-08:00","labels":["database","example","scratch"],"dependencies":[{"issue_id":"TALOS-n3q","depends_on_id":"TALOS-nw6","type":"blocks","created_at":"2025-12-16T11:30:58.505206-08:00","created_by":"daemon"},{"issue_id":"TALOS-n3q","depends_on_id":"TALOS-98j","type":"blocks","created_at":"2025-12-16T11:30:58.554708-08:00","created_by":"daemon"},{"issue_id":"TALOS-n3q","depends_on_id":"TALOS-ffh","type":"blocks","created_at":"2025-12-16T11:30:58.623839-08:00","created_by":"daemon"}]}
{"id":"TALOS-n7k","title":"Phase 3: Set automountServiceAccountToken: false","description":"Disable automatic service account token mounting for pods that don't need K8s API access.\n\n## Tasks\n- [ ] Add `automountServiceAccountToken: false` to all arr-stack deployments\n- [ ] Exception: homepage (needs cluster read access)\n- [ ] Test services still function\n- [ ] Document which services need API access\n\n## Deployments to Update\n- sonarr, radarr, prowlarr, readarr\n- plex, jellyfin, tdarr\n- tautulli, posterr, posterizarr, kometa\n- postgresql","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-17T17:53:43.890324-08:00","updated_at":"2025-12-17T17:53:43.890324-08:00","labels":["rbac","security"],"dependencies":[{"issue_id":"TALOS-n7k","depends_on_id":"TALOS-wlu","type":"parent-child","created_at":"2025-12-17T17:54:01.279632-08:00","created_by":"daemon"}]}
{"id":"TALOS-n8an","title":"Arr-Stack Migration: Drogon SQLite → K8s PostgreSQL","description":"Migrate Sonarr/Radarr/Prowlarr data from local .drogon-app/services/arrs/ to Kubernetes cluster.\n\n**Source:** `.drogon-app/services/arrs/{sonarr,radarr,prowlarr}/`\n- Radarr: 269MB SQLite DB, ~2000 movies\n- Sonarr: 476MB SQLite DB, TV library\n- Prowlarr: Indexer configurations\n\n**Target:** K8s `media` namespace\n- PostgreSQL 16 backend (sonarr_main, radarr_main, prowlarr_main databases)\n- Config PVCs on NFS\n\n**Migration Approach:** App Backup/Restore (native)\n**Data Scope:** Media metadata only (no history, no MediaCover)\n\n**Scope:**\n- Use app's native backup/restore for SQLite → PostgreSQL migration\n- Config files (config.xml, asp/ encryption keys)\n- Tiltfile button for triggering migration\n- Non-destructive to source data","acceptance_criteria":"- [ ] Migration preserves media metadata\n- [ ] API keys and authentication work post-migration\n- [ ] Tiltfile button triggers migration with confirmation\n- [ ] Source .drogon-app data unchanged\n- [ ] Apps connect to PostgreSQL and show library","status":"open","priority":1,"issue_type":"epic","created_at":"2025-12-20T09:23:49.561631-08:00","updated_at":"2025-12-20T09:26:25.287078-08:00"}
{"id":"TALOS-nag","title":"Fix Overseerr widget API key","description":"Verify Overseerr widget is working. May need API key (HOMEPAGE_VAR_OVERSEERR_KEY) configured.","status":"open","priority":2,"issue_type":"bug","created_at":"2025-12-15T15:48:21.408029-08:00","updated_at":"2025-12-15T15:48:21.408029-08:00","labels":["homepage","media"]}
{"id":"TALOS-nh8","title":"OpenTelemetry Stack Evaluation - LGTM Migration","description":"Deploy OpenTelemetry-based observability stack alongside existing monitoring for evaluation.\n\n**Goal:** Evaluate OTEL Collector + Loki + Tempo as replacement for Fluent Bit + OpenSearch + Graylog\n\n**Architecture:**\n\n\n**What's NEW:**\n- OTEL Collector (unified collection)\n- Loki (lightweight log storage, replaces OpenSearch+Graylog)\n- Tempo (distributed tracing - new capability!)\n\n**What's KEPT:**\n- Prometheus (metrics backend)\n- Grafana (add Loki+Tempo datasources)\n- Alertmanager\n\n**What's REPLACED (after evaluation):**\n- Fluent Bit → OTEL Collector\n- OpenSearch → Loki\n- Graylog → Grafana (LogQL)\n\n**Namespace:**  (extend existing) or  (isolated test)\n\n**Success Criteria:**\n- Claude Code metrics flowing via OTLP\n- Logs queryable in Grafana via Loki\n- Traces visible in Grafana via Tempo\n- Compare resource usage vs current stack","design":"**V2 Stack Architecture (Operator-Based):**\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                     V2-OTEL STACK                                │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                  │\n│  OPERATORS (CRD Providers):                                     │\n│  ├── OTEL Operator      → OpenTelemetryCollector, Instrumentation│\n│  ├── Tempo Operator     → TempoStack, TempoMonolithic           │\n│  ├── Grafana Operator   → GrafanaDashboard, GrafanaDatasource   │\n│  └── Prometheus Operator → ServiceMonitor, PrometheusRule       │\n│                                                                  │\n│  DATA PLANE:                                                    │\n│  ├── Alloy             → Unified collector (metrics/logs/traces)│\n│  ├── Mimir             → Metrics backend (replaces Prometheus)  │\n│  ├── Loki              → Logs backend                           │\n│  └── Tempo             → Traces backend (via Tempo Operator)    │\n│                                                                  │\n│  VISUALIZATION:                                                 │\n│  └── Grafana           → Via Grafana Operator CRs               │\n│                                                                  │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n**Directory Structure:**\n```\ninfrastructure/base/monitoring/\n├── shared/\n│   ├── namespace.yaml\n│   └── grafana-operator/\n├── v1/\n│   └── kube-prometheus-stack/\n└── v2-otel/\n    ├── operators/\n    │   ├── otel-operator/\n    │   └── tempo-operator/\n    ├── mimir/\n    ├── loki/\n    ├── tempo/\n    ├── alloy/\n    ├── grafana-datasources/\n    └── grafana-dashboards/\n```","notes":"V2 OTEL Stack - FULLY DEPLOYED\n\n**Base Infrastructure (commits 097c203 - 186980a):**\n\ncert-manager:\n- Version: v1.16.2\n- ClusterIssuers: selfsigned-issuer, homelab-ca-issuer\n- Status: Running\n\nMinIO:\n- Version: v5.2.0\n- Storage: fatboy-nfs-appdata (NFS)\n- Buckets: mimir (with alertmanager/, ruler/ prefixes)\n- Endpoints: minio.talos00 (console), s3.talos00 (API)\n- Status: Running\n\n**Operators:**\n- OTEL Operator: Running\n- Tempo Operator: Running (in monitoring namespace)\n- Grafana Operator: Running\n\n**Data Plane:**\n- Mimir: All 14 components running\n  - Storage: MinIO S3 backend\n  - Separate prefixes for blocks, alertmanager, ruler\n- Loki: Running (3 pods)\n- Tempo: Running (Helm chart)\n- Alloy: Running\n  - Metrics -\u003e Mimir (OTLP HTTP)\n  - Logs -\u003e Loki\n  - Traces -\u003e Tempo\n\n**Grafana Datasources:**\n- Mimir (primary, default)\n- Prometheus (ServiceMonitor metrics)\n- Loki\n- Tempo (with correlations to Mimir/Loki)\n\n**V1 Stack:**\n- monitoring.yaml: suspended\n- observability.yaml: suspended\n- kube-prometheus-stack: resumed for ServiceMonitor scrapes\n\nStack is fully operational and ready for testing.","status":"in_progress","priority":2,"issue_type":"epic","created_at":"2025-12-13T09:19:56.282179-08:00","updated_at":"2025-12-16T10:34:01.03739-08:00","labels":["loki","monitoring","observability","otel","tempo"]}
{"id":"TALOS-nsj","title":"Create linkerd dashboard","description":"infrastructure/base/linkerd/dashboard.sh - control plane, viz, meshed namespaces, linkerd check","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.658563-08:00","updated_at":"2025-12-12T18:55:36.473441-08:00","closed_at":"2025-12-12T18:55:36.473441-08:00"}
{"id":"TALOS-nw3","title":"Fix dashboard: traefik-v2-alt","description":"Assess and fix/replace traefik-v2-alt dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:04.495862-08:00","updated_at":"2025-12-14T11:31:04.495862-08:00","labels":["dashboard","networking"],"dependencies":[{"issue_id":"TALOS-nw3","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:04.496818-08:00","created_by":"daemon"}]}
{"id":"TALOS-nw6","title":"Deploy CloudNativePG Operator","description":"Deploy CloudNativePG operator to databases namespace via HelmRelease. This enables PostgreSQL cluster provisioning via `Cluster` CRD.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T11:30:40.30385-08:00","updated_at":"2025-12-16T11:42:51.391578-08:00","closed_at":"2025-12-16T11:42:51.391578-08:00","labels":["database","operator","postgresql"],"dependencies":[{"issue_id":"TALOS-nw6","depends_on_id":"TALOS-eoq","type":"blocks","created_at":"2025-12-16T11:30:58.285711-08:00","created_by":"daemon"}]}
{"id":"TALOS-o05","title":"Implement comprehensive backup solution (Velero/Restic)","description":"Replace custom SQLite backup cronjobs with a proper backup solution like Velero for PVC backups. Should handle point-in-time recovery for all workloads.","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-17T21:33:53.273422-08:00","updated_at":"2025-12-17T21:33:53.273422-08:00","labels":["backup","infrastructure","velero"]}
{"id":"TALOS-o34","title":"Audit log output and optimize log levels across services","description":"Audit logging across all services to optimize storage usage:\n\n1. Query Loki for log volume by service/namespace\n2. Identify noisy services (high log rate, low value logs)\n3. Recommend log level adjustments:\n   - debug → info for production\n   - info → warn for low-value logs\n4. Reduce storage usage and noise in Grafana\n\nExample queries:\n```logql\nsum(rate({namespace=~\".+\"}[1h])) by (namespace)\nsum(rate({namespace=~\".+\"}[1h])) by (app)\n```","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-16T08:50:51.37754-08:00","updated_at":"2025-12-16T08:50:51.37754-08:00"}
{"id":"TALOS-obf","title":"Fix dashboard: loki-kubernetes-logs","description":"Assess and fix/replace loki-kubernetes-logs dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:23.764813-08:00","updated_at":"2025-12-14T17:10:34.844754-08:00","closed_at":"2025-12-14T17:10:34.844754-08:00","labels":["dashboard","logging"],"dependencies":[{"issue_id":"TALOS-obf","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:23.766812-08:00","created_by":"daemon"}]}
{"id":"TALOS-ofm","title":"Deploy standalone kube-state-metrics","description":"Deploy kube-state-metrics as standalone HelmRelease in v2-otel stack.\n\n**Helm chart:** kube-state-metrics (from prometheus-community)\n**Namespace:** monitoring\n**Key config:**\n- Enable all collectors\n- Add prometheus.io/scrape annotation for Alloy discovery\n- Resource limits appropriate for homelab","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-13T22:26:51.323392-08:00","updated_at":"2025-12-13T22:33:51.622814-08:00","closed_at":"2025-12-13T22:33:51.622814-08:00","labels":["kube-state-metrics","monitoring"],"dependencies":[{"issue_id":"TALOS-ofm","depends_on_id":"TALOS-zqh","type":"parent-child","created_at":"2025-12-13T22:27:04.582015-08:00","created_by":"daemon"}]}
{"id":"TALOS-oik1","title":"Implement Kubernetes Device Plugins for Hardware Provisioning","description":"Deploy proper Kubernetes device plugins for GPU and USB hardware management instead of raw hostPath mounts.\n\n## Current State\n- GPUs exposed via hostPath `/dev/dri` mounts (manual, no scheduling)\n- No USB device management\n- No hardware auto-discovery\n\n## Components to Deploy\n\n### 1. Intel Device Plugin\nProper scheduling for Intel iGPUs (talos02 Arc, etc.)\n```yaml\nresources:\n  limits:\n    gpu.intel.com/i915: 1\n```\n\n### 2. Node Feature Discovery (NFD)\nAuto-labels nodes with hardware capabilities:\n- CPU features (AVX512, etc.)\n- PCI devices (GPUs)\n- USB devices\n- Memory/storage info\n\n### 3. smarter-device-manager\nUSB device passthrough for:\n- Zigbee coordinators\n- Z-Wave sticks\n- Serial devices\n\n## Benefits\n- Kubernetes-native resource scheduling\n- Multiple pods can share GPU fairly\n- Hot-plug USB support\n- Hardware inventory via labels\n- Pod anti-affinity based on hardware\n\n## Implementation\n1. Deploy NFD operator\n2. Deploy Intel device plugin DaemonSet\n3. Deploy smarter-device-manager\n4. Update existing GPU workloads to use device resources\n5. Remove hostPath hacks from tdarr, plex, etc.\n\n## References\n- https://github.com/intel/intel-device-plugins-for-kubernetes\n- https://github.com/kubernetes-sigs/node-feature-discovery\n- https://github.com/smarter-project/smarter-device-manager","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-19T21:59:26.485999-08:00","updated_at":"2025-12-19T21:59:26.485999-08:00","labels":["gpu","hardware","infrastructure"]}
{"id":"TALOS-oje","title":"Fix dashboard: cilium-hubble-flows","description":"Assess and fix/replace cilium-hubble-flows dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:30:49.093071-08:00","updated_at":"2025-12-14T17:59:15.284995-08:00","closed_at":"2025-12-14T17:59:15.284995-08:00","labels":["dashboard","networking"],"dependencies":[{"issue_id":"TALOS-oje","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:30:49.094028-08:00","created_by":"daemon"}]}
{"id":"TALOS-otep","title":"Create IngressRoute for qBittorrent WebUI","description":"Create Traefik IngressRoute for qbittorrent.talos00 to access the WebUI. Consider mTLS option like SecureXNG for added security.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T13:31:38.021922-08:00","updated_at":"2025-12-20T13:34:08.5085-08:00","closed_at":"2025-12-20T13:34:08.5085-08:00","dependencies":[{"issue_id":"TALOS-otep","depends_on_id":"TALOS-m6xf","type":"blocks","created_at":"2025-12-20T13:31:38.023905-08:00","created_by":"daemon"}]}
{"id":"TALOS-ouf8","title":"Add Cloudflare DNS record for linkwarden.knowledgedump.space","description":"Create DNS record in Cloudflare for linkwarden.knowledgedump.space pointing to cluster public IP (or CNAME to existing record like auth.knowledgedump.space).\n\nRequired for external HTTPS access to Linkwarden bookmark manager.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-24T12:10:21.247232-08:00","updated_at":"2025-12-24T12:10:21.247232-08:00"}
{"id":"TALOS-p4v","title":"Fix dashboard: kasa-comparative-analytics","description":"Assess and fix/replace kasa-comparative-analytics dashboard for V2 OTEL stack compatibility","notes":"**Audit completed 2024-12-14:**\n\n**Working panels:**\n- Most Efficient (shows devices with $/W values ~0.35)\n\n**\"No data\" panels:**\n- Highest Power Consumer\n- Most Expensive Device  \n- Device Count\n- Power by Device bar chart\n- Power Share % pie chart\n- Power Comparison Timeline\n- Power Ranking Table\n- Cost by Device bar chart\n- Cost Share % pie chart\n- Cost Comparison Timeline\n\n**Root cause:** Dashboard queries likely using aggregations or time ranges that don't match available data. The \"Most Efficient\" panel works because it uses a simpler query on current_consumption metrics. Other panels may need recording rules or query adjustments.\n\n**Status:** Partial functionality - needs query investigation to fix remaining panels.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:32:09.019881-08:00","updated_at":"2025-12-14T15:59:36.634185-08:00","labels":["dashboard","kasa"],"dependencies":[{"issue_id":"TALOS-p4v","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:32:09.020776-08:00","created_by":"daemon"}]}
{"id":"TALOS-p7dz","title":"Create Nexus proxy repositories for ghcr.io, quay.io, registry.k8s.io","description":"Create additional Docker proxy repositories in Nexus for upstream registries beyond Docker Hub. Each needs its own HTTP port.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-20T16:18:15.744312-08:00","updated_at":"2025-12-20T16:18:15.744312-08:00","dependencies":[{"issue_id":"TALOS-p7dz","depends_on_id":"TALOS-wjb9","type":"blocks","created_at":"2025-12-20T16:18:15.745943-08:00","created_by":"daemon"}]}
{"id":"TALOS-pcz","title":"Add filter by Labels as chip multiselect","description":"**Feature Request**: Add ability to filter issues by labels using a chip-style multiselect UI.\n\n**Implementation**:\n- Add \"Filter by Labels\" section in sidebar\n- Show available labels as chips (derived from all issues)\n- Chips are toggleable (click to select/deselect)\n- Multiple labels can be selected (AND or OR logic?)\n- Selected chips visually distinct (filled vs outlined)\n- Show count of issues per label\n\n**UI pattern**: Similar to Gmail labels or GitHub issue labels filtering\n\n**Related to**: Search functionality (TALOS-dc9) - could be combined","status":"closed","priority":2,"issue_type":"feature","assignee":"subagent-3","created_at":"2025-12-16T10:12:15.105797-08:00","updated_at":"2025-12-16T10:59:01.16166-08:00","closed_at":"2025-12-16T10:59:01.16166-08:00","labels":["beads-manager","feature","ui"]}
{"id":"TALOS-pkz","title":"Fix ArgoCD catalyst-ui sync failure","description":"ArgoCD application for catalyst-ui is defined but not syncing.\n\n**Symptoms:**\n- Application shows as \"not syncing\"\n- Likely repo access or image pull issue\n\n**Investigation needed:**\n- Check repository credentials\n- Test manual sync via CLI\n- Check ArgoCD application-controller logs\n\nFrom: infrastructure/base/argocd/STATUS.md","status":"open","priority":3,"issue_type":"bug","created_at":"2025-12-12T22:36:53.584946-08:00","updated_at":"2025-12-12T22:36:53.584946-08:00","labels":["argocd","bug","catalyst-ui"]}
{"id":"TALOS-pnj","title":"Catalyst DNS Sync - Kubernetes DNS Controller","description":"Go-based Kubernetes controller that syncs Traefik IngressRoutes to Technitium DNS server.\n\n**Current Status:** Phase 1 MVP in progress (~70% complete)\n\n**Phase 1 MVP (Current):**\n- Core CRUD DNS sync (watch Ingress/IngressRoute, create/update/delete A records)\n- Prometheus metrics endpoint\n- Health probes (/healthz, /readyz)\n- Dev mode (updates /etc/hosts instead of DNS)\n- Kubernetes deployment manifests\n\n**Phase 2 (Future):**\n- Web UI Dashboard at dns.talos00/ui\n- DNS test endpoint\n- Certificate expiration tracking\n\n**Location:** catalyst-dns-sync/ (in this repo)\n\nDetailed spec: docs/05-projects/catalyst-dns-sync/proposal.md","status":"open","priority":3,"issue_type":"epic","created_at":"2025-12-12T22:36:53.539634-08:00","updated_at":"2025-12-12T22:36:53.539634-08:00","labels":["controller","dns","go","infrastructure"]}
{"id":"TALOS-pv9z","title":"Add API keys to 1Password for homepage widgets","description":"Homepage widgets need API tokens stored in 1Password for ExternalSecrets to pull:\n\n**Required 1Password items:**\n\n1. `authentik-api-token` (password field)\n   - Create in Authentik Admin \u003e Directory \u003e Tokens \u0026 App Passwords\n   - Grant read access for monitoring\n\n2. `arr-stack` - add these fields:\n   - `readarr.apiKey` - from Readarr Settings \u003e General\n   - `jellyfin.apiKey` - from Jellyfin Dashboard \u003e API Keys\n   - `overseerr.apiKey` - from Overseerr Settings \u003e General\n\nOnce added, homepage widgets will start working automatically on next ExternalSecret refresh (1h) or force sync.","status":"closed","priority":3,"issue_type":"chore","created_at":"2026-01-01T14:58:36.333745-08:00","updated_at":"2026-01-01T19:19:18.681176-08:00","closed_at":"2026-01-01T19:19:18.681176-08:00","labels":["1password","homepage","secrets"]}
{"id":"TALOS-q06","title":"Fix dashboard: k8s-comprehensive","description":"Assess and fix/replace k8s-comprehensive dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:30:34.106759-08:00","updated_at":"2025-12-14T12:54:57.09798-08:00","closed_at":"2025-12-14T12:54:57.09798-08:00","labels":["dashboard","kubernetes"],"dependencies":[{"issue_id":"TALOS-q06","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:30:34.107715-08:00","created_by":"daemon"}]}
{"id":"TALOS-q4s","title":"Configure Flux Alert CRDs for critical events only","description":"Create Alert manifests that filter for critical/error events only:\n\n- eventSeverity: error (not info)\n- Specific event sources (not wildcards initially)\n- Test with intentional failures to verify signal vs noise\n\nBased on findings from investigation task.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-12T22:11:20.109228-08:00","updated_at":"2025-12-12T22:11:20.109228-08:00","labels":["flux","notifications"],"dependencies":[{"issue_id":"TALOS-q4s","depends_on_id":"TALOS-8uv","type":"parent-child","created_at":"2025-12-12T22:11:30.807663-08:00","created_by":"daemon"},{"issue_id":"TALOS-q4s","depends_on_id":"TALOS-26f","type":"blocks","created_at":"2025-12-12T22:11:30.959557-08:00","created_by":"daemon"},{"issue_id":"TALOS-q4s","depends_on_id":"TALOS-dv3","type":"blocks","created_at":"2025-12-12T22:11:31.0012-08:00","created_by":"daemon"}]}
{"id":"TALOS-qofw","title":"Integrate Karpenter for hybrid-llm node provisioning","description":"Use Karpenter to provision AWS GPU nodes on-demand for LLM workloads.\n\nArchitecture:\n1. KEDA detects LLM request (ScaledObject triggers scale 0→1)\n2. Pod goes Pending (no GPU node available)\n3. Karpenter detects unschedulable pod with GPU requirement\n4. Karpenter provisions g4dn.xlarge spot instance\n5. Liqo federates the new node into homelab cluster\n6. Pod schedules on GPU node\n7. After idle timeout, Karpenter terminates instance\n\nThis replaces the custom llm-scaler with standard Kubernetes primitives.\n\nConsiderations:\n- Karpenter needs AWS credentials\n- Need NodePool CRD for GPU instance types\n- Liqo peering must be automatic on new nodes\n- Consider spot instance interruption handling","design":"Components:\n1. Karpenter controller (runs on homelab or lighthouse)\n2. NodePool CRD defining g4dn.xlarge spot instances\n3. EC2NodeClass for AMI, security groups, IAM role\n4. Liqo auto-peering via cloud-init/userdata\n\nFlow: KEDA ScaledObject → Pending Pod → Karpenter → EC2 → Liqo → Pod Scheduled","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-19T18:35:33.090124-08:00","updated_at":"2025-12-19T18:35:33.090124-08:00","dependencies":[{"issue_id":"TALOS-qofw","depends_on_id":"TALOS-b1gd","type":"blocks","created_at":"2025-12-19T18:35:39.895839-08:00","created_by":"daemon"},{"issue_id":"TALOS-qofw","depends_on_id":"TALOS-txzj","type":"blocks","created_at":"2025-12-19T18:35:39.966582-08:00","created_by":"daemon"}]}
{"id":"TALOS-r0vw","title":"Add Bluetooth USB dongle passthrough to Home Assistant","description":"Enable Bluetooth dongle support in Home Assistant deployment for BLE device integrations.\n\n**Current state:** Home Assistant running without Bluetooth (USB portions commented out in deployment.yaml)\n\n**Required changes:**\n1. Identify which node has the BT dongle plugged in\n2. Add nodeSelector: `bluetooth.homeassistant.io/enabled: \"true\"`\n3. Label the node: `kubectl label node \u003cnode\u003e bluetooth.homeassistant.io/enabled=true`\n4. Uncomment USB/dbus volume mounts in deployment.yaml\n5. Add privileged securityContext for USB access\n6. Test Bluetooth integration in HA UI\n\n**File:** `applications/home-stack/base/homeassistant/deployment.yaml`","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-23T16:13:22.062104-08:00","updated_at":"2025-12-23T16:13:22.062104-08:00","labels":["bluetooth","hardware","home-automation"]}
{"id":"TALOS-r3u","title":"Migrate standalone MinIO to operator-managed Tenant","description":"Replace the existing standalone MinIO Helm deployment with an operator-managed Tenant CR.\n\n**Current state:**\n- Standalone MinIO in `minio` namespace via HelmRelease\n- Buckets: mimir, tempo, loki\n- Storage: 100Gi on fatboy-nfs-appdata\n- Consumers: Mimir, Tempo, Loki (OTEL stack)\n\n**Migration steps:**\n1. Deploy MinIO Operator to databases namespace\n2. Create Tenant CR in minio namespace (same namespace, new management)\n3. Wait for tenant to be ready\n4. Update Mimir/Tempo/Loki to use new tenant endpoints\n5. Suspend/delete old HelmRelease\n6. Verify OTEL stack works with new tenant","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T12:23:05.504283-08:00","updated_at":"2025-12-16T12:24:36.469346-08:00","closed_at":"2025-12-16T12:24:36.469346-08:00","labels":["migration","minio","operator"]}
{"id":"TALOS-sy4","title":"Remove kube-prometheus-stack and cleanup CRDs","description":"Final removal of kube-prometheus-stack.\n\n**Steps:**\n1. Delete HelmRelease\n2. Delete ServiceMonitors (orphaned CRs)\n3. Delete PrometheusRules (orphaned CRs)\n4. Optionally remove prometheus-operator CRDs\n5. Clean up any leftover PVCs\n6. Update Flux kustomization\n\n**Cleanup:**\n- Remove monitoring.yaml Flux Kustomization or update path\n- Archive old configs in docs/","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-13T22:26:51.613162-08:00","updated_at":"2025-12-13T22:39:00.430438-08:00","closed_at":"2025-12-13T22:39:00.430438-08:00","labels":["cleanup","monitoring"],"dependencies":[{"issue_id":"TALOS-sy4","depends_on_id":"TALOS-zqh","type":"parent-child","created_at":"2025-12-13T22:27:04.831848-08:00","created_by":"daemon"},{"issue_id":"TALOS-sy4","depends_on_id":"TALOS-jtp","type":"blocks","created_at":"2025-12-13T22:27:04.964314-08:00","created_by":"daemon"}]}
{"id":"TALOS-szzd","title":"Upgrade Kometa configs with custom overlay settings","description":"Import and integrate custom Kometa configuration from shared Google Drive.\n\n## Source\nhttps://drive.google.com/drive/folders/1oVccowVQvbg0MZ51Z4fCayKs4iXZKzrb\n\n## Tasks\n- Review the custom configs in the drive folder\n- Compare with current Kometa config in `applications/arr-stack/base/kometa/`\n- Merge/update overlay settings, collections, metadata\n- Test on a small library subset before full run\n- Document any new overlays or collections added\n\n## Custom Collections to Add/Improve\n\n### Current Collections (from configmap)\n**Movies:**\n- basic, imdb, trakt (charts)\n- streaming (Netflix, Disney+, etc.)\n- oscars, golden_globes, bafta, cannes, sundance (awards)\n- genre, decade, franchise, universe (organization)\n\n**TV Shows:**\n- basic, imdb, trakt, streaming\n- emmy, golden_globes\n- genre, decade, franchise, network\n\n### Desired Custom Collections\n- [ ] Director spotlights (Nolan, Tarantino, Villeneuve, etc.)\n- [ ] Actor collections (specific favorites)\n- [ ] Mood-based (Feel Good, Mind Benders, Comfort Watches)\n- [ ] Holiday collections (Christmas, Halloween, etc.)\n- [ ] Curated \"Best Of\" lists\n- [ ] Hidden Gems / Underrated\n- [ ] Studio collections (A24, Pixar, Studio Ghibli)\n- [ ] Decade \"Best Of\" (Best of 80s, 90s, etc.)\n- [ ] Franchise deep-dives with watch order\n\n### Collection Styling\n- Synthwave-themed collection posters\n- Consistent naming convention\n- Smart ordering (by rating, year, or custom)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-31T20:30:48.560283-08:00","updated_at":"2026-01-01T15:06:02.276886-08:00","closed_at":"2026-01-01T15:06:02.276886-08:00","labels":["config","kometa","media"],"dependencies":[{"issue_id":"TALOS-szzd","depends_on_id":"TALOS-z6nh","type":"blocks","created_at":"2025-12-31T20:30:54.052488-08:00","created_by":"daemon"},{"issue_id":"TALOS-szzd","depends_on_id":"TALOS-4ubj","type":"parent-child","created_at":"2025-12-31T20:39:43.73015-08:00","created_by":"daemon"}]}
{"id":"TALOS-t3p","title":"Add filter by Epic functionality","description":"**Feature Request**: Add ability to filter issues by their parent Epic.\n\n**Implementation**:\n- Add \"Filter by Epic\" section in sidebar (similar to Filter by Status/Type)\n- Show list of Epics with issue counts\n- Clicking an Epic filters to show only that Epic and its children\n- Support multi-select for viewing multiple Epics\n\n**Use case**: Focus on a specific project/initiative by viewing only related issues","status":"closed","priority":2,"issue_type":"feature","assignee":"subagent-2","created_at":"2025-12-16T10:09:39.626936-08:00","updated_at":"2025-12-16T10:59:01.102394-08:00","closed_at":"2025-12-16T10:59:01.102394-08:00","labels":["beads-manager","feature","ui"]}
{"id":"TALOS-t62","title":"Fix dashboard: argocd-notifications","description":"Assess and fix/replace argocd-notifications dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:54.051629-08:00","updated_at":"2025-12-14T11:31:54.051629-08:00","labels":["dashboard","gitops"],"dependencies":[{"issue_id":"TALOS-t62","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:54.052489-08:00","created_by":"daemon"}]}
{"id":"TALOS-t7u","title":"Configure Claude Code OTEL export to cluster","description":"Set up local Mac environment to export Claude Code telemetry to cluster.\n\n**Environment variables:**\n```bash\nexport CLAUDE_CODE_ENABLE_TELEMETRY=1\nexport OTEL_METRICS_EXPORTER=otlp\nexport OTEL_LOGS_EXPORTER=otlp\nexport OTEL_EXPORTER_OTLP_ENDPOINT=http://otel.talos00:4318\n```\n\n**Add to:** ~/.zshrc or shell profile\n\n**Verify:** Metrics appear in Grafana","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-13T09:20:16.222178-08:00","updated_at":"2025-12-13T09:20:16.222178-08:00","labels":["claude-code","macos","otel"],"dependencies":[{"issue_id":"TALOS-t7u","depends_on_id":"TALOS-nh8","type":"parent-child","created_at":"2025-12-13T09:20:28.422174-08:00","created_by":"daemon"},{"issue_id":"TALOS-t7u","depends_on_id":"TALOS-3of","type":"blocks","created_at":"2025-12-13T09:20:28.577573-08:00","created_by":"daemon"}]}
{"id":"TALOS-twd7","title":"Redesign arr-stack dashboard with node-centric hierarchy","description":"Refactor dashboard to group services by node with inline details:\n- Top level = Nodes with disk usage bars\n- Services grouped by which node they run on\n- Credentials, VPN status, volumes shown inline under each service\n- Shared NFS/NAS storage shown separately (not repeated per service)","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-20T20:56:46.821008-08:00","updated_at":"2025-12-20T21:41:16.423554-08:00","closed_at":"2025-12-20T21:41:16.423554-08:00"}
{"id":"TALOS-txzj","title":"Fix hybrid cluster (Liqo) connectivity","description":"Hybrid cluster (Liqo) likely broken after recent changes. Need to verify connectivity and fix any issues before llm-scaler-v2 work.","status":"open","priority":2,"issue_type":"bug","created_at":"2025-12-19T18:17:52.058516-08:00","updated_at":"2025-12-19T18:17:52.058516-08:00"}
{"id":"TALOS-u1i5","title":"Port all dashboards to node-centric layout","description":"After TALOS-twd7 (arr-stack node-centric redesign), apply the same layout pattern to other dashboards:\n- Infrastructure dashboard (if exists)\n- Monitoring dashboard (if exists)\n- Any other stack-specific dashboards\n\nThe pattern: Group services by node, show disk usage per node, inline credentials/VPN, separate shared NFS storage section.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-20T20:59:12.685711-08:00","updated_at":"2025-12-20T20:59:12.685711-08:00","dependencies":[{"issue_id":"TALOS-u1i5","depends_on_id":"TALOS-twd7","type":"blocks","created_at":"2025-12-20T20:59:12.688552-08:00","created_by":"daemon"}]}
{"id":"TALOS-u2a","title":"Fix dashboard: postgresql-database","description":"Assess and fix/replace postgresql-database dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:38.817228-08:00","updated_at":"2025-12-14T11:31:38.817228-08:00","labels":["dashboard","infrastructure"],"dependencies":[{"issue_id":"TALOS-u2a","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:38.81818-08:00","created_by":"daemon"}]}
{"id":"TALOS-u5j","title":"List view crashes with TypeError in IssueList component","description":"**Critical Bug**: Clicking the \"List\" view toggle button crashes the entire app.\n\n**Error**: `TypeError: Cannot read properties of undefined (reading 'length')` in `IssueList` component.\n\n**Steps to reproduce**:\n1. Open beads-manager UI at http://localhost:5173\n2. Click the \"List\" button in the sidebar\n3. App crashes, page goes blank\n\n**Expected**: List view should render issues in a list format\n**Actual**: App crashes with TypeError, requires page refresh","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-16T10:09:39.1634-08:00","updated_at":"2025-12-16T10:23:34.238169-08:00","closed_at":"2025-12-16T10:23:34.238169-08:00","labels":["beads-manager","critical","ui"]}
{"id":"TALOS-uec","title":"Fix dashboard: traefik-services","description":"Assess and fix/replace traefik-services dashboard for V2 OTEL stack compatibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:04.413497-08:00","updated_at":"2025-12-14T11:31:04.413497-08:00","labels":["dashboard","networking"],"dependencies":[{"issue_id":"TALOS-uec","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:04.414529-08:00","created_by":"daemon"}]}
{"id":"TALOS-ul8e","title":"Configure Docker pull-through cache for all Talos nodes","description":"Configure Talos nodes to use Nexus as a pull-through cache for container images (docker.io, ghcr.io) to reduce external network usage and speed up deployments.\n\nCurrent state:\n- Nexus deployed with docker-proxy service on port 5001\n- IngressRoute at docker-proxy.talos00\n- Each node currently pulls directly from upstream registries\n\nTasks:\n1. Verify Nexus docker-proxy repository is configured correctly\n2. Add ghcr.io proxy repository to Nexus (if not exists)\n3. Update Talos machine configs with registry mirrors\n4. Apply config changes to all nodes\n5. Test pull-through cache is working","notes":"Investigation complete:\n\n**Finding**: Nexus docker-proxy NOT configured\n- Only `talos00-registry` (docker hosted) exists on port 5000\n- Port 5001 (docker-proxy) has no repository - returns 502\n- Need to create proxy repositories in Nexus UI:\n  1. docker-hub-proxy (docker.io) on port 5001\n  2. ghcr-proxy (ghcr.io) - needs new port 5002\n\n**BLOCKED**: Requires manual Nexus UI configuration first\nSee: infrastructure/base/registry/README.md lines 229-238 for instructions\n\n**Manual Steps Required**:\n1. http://nexus.talos00 → Settings → Repositories → Create\n2. docker (proxy) → name: docker-proxy, port: 5001, remote: https://registry-1.docker.io\n3. docker (proxy) → name: ghcr-proxy, port: 5002, remote: https://ghcr.io\n4. Update deployment.yaml to expose port 5002\n5. Create IngressRoute for ghcr-proxy.talos00\n6. Update Talos machine configs with registry mirrors\n7. Apply config (requires node reboot)","status":"blocked","priority":2,"issue_type":"task","created_at":"2025-12-19T16:37:39.626527-08:00","updated_at":"2025-12-19T16:39:07.371522-08:00"}
{"id":"TALOS-v3a","title":"Set up ntfy-desktop client on Mac","description":"Install and configure ntfy-desktop on macOS:\n\n1. Download from https://github.com/Aetherinox/ntfy-desktop/releases (darwin-arm64)\n2. Subscribe to the cluster topic\n3. Configure notification preferences (sound, badge, etc.)\n4. Test end-to-end with a manual ntfy publish","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-12T22:11:20.154666-08:00","updated_at":"2025-12-12T22:11:20.154666-08:00","labels":["macos","notifications"],"dependencies":[{"issue_id":"TALOS-v3a","depends_on_id":"TALOS-8uv","type":"parent-child","created_at":"2025-12-12T22:11:30.842375-08:00","created_by":"daemon"},{"issue_id":"TALOS-v3a","depends_on_id":"TALOS-dv3","type":"blocks","created_at":"2025-12-12T22:11:31.042716-08:00","created_by":"daemon"}]}
{"id":"TALOS-vbl3","title":"Hardware Inventory Dashboard","description":"Create a Grafana dashboard showing all hardware assets in the cluster and their current bindings.\n\n## Dashboard Panels\n\n### Node Hardware Overview\n- Per-node hardware summary (CPU, RAM, GPUs, USB devices)\n- Node labels from NFD showing capabilities\n- Resource capacity vs allocated\n\n### GPU Status\n| Node | GPU Type | Device | Bound To | Utilization |\n|------|----------|--------|----------|-------------|\n| talos02 | Intel Arc | /dev/dri/renderD128 | tdarr | 45% |\n| talos03 | AMD Vega | /dev/dri/renderD128 | tdarr-node-gpu | 30% |\n\n### USB Devices\n| Node | Device | VID:PID | Description | Bound To |\n|------|--------|---------|-------------|----------|\n| talos04 | /dev/ttyUSB0 | 1a86:7523 | Zigbee Coordinator | home-assistant |\n\n### Resource Allocation\n- Device plugin allocatable vs allocated\n- Pods requesting hardware resources\n- Unbound/available devices\n\n## Data Sources\n- Prometheus metrics from device plugins\n- Node Feature Discovery labels\n- Kubernetes resource API\n- Custom exporter for USB enumeration\n\n## Prerequisites\n- Node Feature Discovery deployed\n- Device plugins exposing metrics\n- Prometheus scraping device metrics","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-19T21:59:26.544059-08:00","updated_at":"2025-12-19T21:59:26.544059-08:00","labels":["grafana","hardware","monitoring"],"dependencies":[{"issue_id":"TALOS-vbl3","depends_on_id":"TALOS-oik1","type":"blocks","created_at":"2025-12-19T21:59:37.808355-08:00","created_by":"daemon"}]}
{"id":"TALOS-vshi","title":"Create synthwave glow overlay PNG for Posterizarr","description":"Design and create a custom synthwave glow overlay PNG for Posterizarr.\n\n## Requirements\n- Transparent center (poster shows through)\n- Neon glow border effect (inner glow radiating inward)\n- Colors matching Kometa synthwave palette:\n  - Primary glow: #00FFFF (Cyan)\n  - Optional gradient to #FF00FF (Magenta)\n- Poster dimensions: 1000x1500px (standard)\n- Background overlay: 1920x1080px\n\n## Files to Create\n- `overlay-synthwave.png` - Main poster glow\n- `backgroundoverlay-synthwave.png` - Background art glow\n- `seasonoverlay-synthwave.png` - Season poster variant\n\n## Implementation Options\n\n### Option 1: ImageMagick Generation\n```bash\nconvert -size 1000x1500 xc:transparent \\\n  -stroke \"#00FFFF\" -strokewidth 20 \\\n  -fill none -draw \"roundrectangle 10,10 990,1490 20,20\" \\\n  -blur 0x10 \\\n  overlay-synthwave.png\n```\n\n### Option 2: Photoshop/GIMP\n- Create neon glow effect with layer styles\n- Export as PNG with transparency\n\n### Option 3: Community Overlays\n- Check Posterizarr Discord for shared synthwave packs\n- Customize existing glow overlays\n\n## Posterizarr Config Update\n```json\n\"PrerequisitePart\": {\n  \"overlayfile\": \"overlay-synthwave.png\",\n  \"backgroundoverlayfile\": \"backgroundoverlay-synthwave.png\",\n  \"seasonoverlayfile\": \"overlay-synthwave.png\"\n}\n```","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-31T20:39:43.816748-08:00","updated_at":"2026-01-01T09:55:41.271542-08:00","closed_at":"2026-01-01T09:55:41.271542-08:00","labels":["design","posterizarr","synthwave"],"dependencies":[{"issue_id":"TALOS-vshi","depends_on_id":"TALOS-4ubj","type":"parent-child","created_at":"2025-12-31T20:39:48.632807-08:00","created_by":"daemon"}]}
{"id":"TALOS-w1k","title":"Complete Tilt development workflow integration","description":"Finish Tilt extensions integration (currently ~60%):\n\n**Remaining:**\n- [ ] Test `tilt up` end-to-end\n- [ ] Consider namespace extension for dev isolation\n- [ ] Optimize build/deploy cycles\n- [ ] Team training documentation\n\n**Completed:**\n- helm_resource, k8s_attach, uibutton, dotenv\n\nFrom: docs/06-project-management/enhancement-roadmap.md (Stream 2)","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-12T22:20:28.74797-08:00","updated_at":"2025-12-12T22:20:28.74797-08:00","labels":["devex","tilt"]}
{"id":"TALOS-w6g","title":"MongoDB Community Operator - MongoDB Database Management","description":"Deploy MongoDB Community Operator for declarative MongoDB replica set management.\n\n**Goal:** Enable provisioning MongoDB databases via CRDs instead of manual deployments.\n\n**Operator:** MongoDB Community Operator (Official)\n- Repo: https://github.com/mongodb/mongodb-kubernetes-operator\n- Helm: https://mongodb.github.io/helm-charts\n\n**CRDs Provided:**\n- `MongoDBCommunity` - MongoDB replica set\n\n**Features:**\n- Declarative replica set provisioning\n- Automated member management\n- TLS encryption\n- SCRAM authentication\n- Prometheus metrics via mongodb-exporter\n\n**Integration:**\n- Storage: local-path or fatboy-nfs-appdata\n- Monitoring: ServiceMonitor for Prometheus\n- Potential users: Graylog (currently using manual MongoDB)","status":"open","priority":2,"issue_type":"epic","created_at":"2025-12-13T12:29:44.304084-08:00","updated_at":"2025-12-13T12:29:44.304084-08:00","labels":["database","infrastructure","mongodb","operator"]}
{"id":"TALOS-w99f","title":"Clean up old tdarr resources (default/media namespace)","description":"Remove orphaned tdarr PVCs, PVs, and deployments from default and media namespaces after migration to dedicated tdarr namespace.","status":"closed","priority":2,"issue_type":"chore","created_at":"2026-01-01T14:17:12.471108-08:00","updated_at":"2026-01-01T14:50:31.932522-08:00","closed_at":"2026-01-01T14:50:31.932522-08:00"}
{"id":"TALOS-wbb","title":"Reorganize Talos config directory structure","description":"Move machine configs to configs/nodes/, create patches/ directory, add README.md documentation","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T16:53:29.837756-08:00","updated_at":"2025-12-12T17:02:27.81464-08:00","closed_at":"2025-12-12T17:02:27.81464-08:00","dependencies":[{"issue_id":"TALOS-wbb","depends_on_id":"TALOS-fpp","type":"parent-child","created_at":"2025-12-12T16:59:14.468368-08:00","created_by":"daemon"}]}
{"id":"TALOS-wbc","title":"Enhance pod-cleanup job for comprehensive homelab maintenance","description":"Improve the pod-cleanup CronJob to be more comprehensive and safe for a homelab Kubernetes cluster.\n\nCurrent cleanup job handles:\n- Failed pods (status.phase == Failed)\n- Succeeded pods (status.phase == Succeeded)\n\nEnhancements to implement:\n1. **Stuck/Evicted pods** - Clean up pods in Evicted state\n2. **ImagePullBackOff pods** - Clean up pods stuck in ImagePullBackOff for extended periods (configurable threshold)\n3. **CrashLoopBackOff pods** - Optionally clean pods stuck in CrashLoopBackOff (with safeguards)\n4. **Completed Jobs** - Clean up Jobs that have finished and exceeded TTL (supplement ttlSecondsAfterFinished)\n5. **Orphaned ReplicaSets** - Remove ReplicaSets with 0 replicas from old deployments\n6. **Unused ConfigMaps/Secrets** - Optionally identify and clean orphaned configmaps/secrets\n\nSafety measures:\n- Dry-run mode by default, actual deletion requires explicit flag\n- Namespace exclusions (kube-system, monitoring, etc. by default)\n- Label-based exclusions (e.g., cleanup.ignore=true)\n- Age thresholds for each cleanup type (don't delete recently failed pods)\n- Enhanced metrics for each cleanup category\n\nDashboard enhancements:\n- Breakdown by cleanup type (failed, succeeded, evicted, imagebackoff, etc.)\n- Namespace distribution of cleaned resources\n- Trend visualization over time","status":"closed","priority":2,"issue_type":"feature","assignee":"me","created_at":"2025-12-14T21:44:33.663222-08:00","updated_at":"2025-12-14T22:15:58.078743-08:00","closed_at":"2025-12-14T22:15:58.078743-08:00","labels":["cronjob","homelab","maintenance","monitoring"]}
{"id":"TALOS-wjb9","title":"Container Registry Pull-Through Cache","description":"Configure Talos containerd to use local Nexus proxy registries for all image pulls. This provides caching, rate limit avoidance, and faster subsequent pulls - all transparent to workloads (no manifest changes needed).","design":"## Architecture\n\nTalos containerd → Nexus proxy repos → Upstream registries\n\n```\nPod pulls ghcr.io/foo:v1\n  → containerd checks mirrors config\n  → routes to nexus-ghcr-proxy.registry.svc:5002\n  → Nexus checks cache, returns if hit\n  → Nexus fetches from ghcr.io if miss, caches, returns\n```\n\n## Required Nexus Proxy Repositories\n\n| Registry | Nexus Repo | Port | Upstream |\n|----------|-----------|------|----------|\n| docker.io | docker-proxy | 5001 | registry-1.docker.io (EXISTS) |\n| ghcr.io | ghcr-proxy | 5002 | ghcr.io |\n| lscr.io | lscr-proxy | 5003 | lscr.io (redirects to ghcr) |\n| quay.io | quay-proxy | 5004 | quay.io |\n| registry.k8s.io | k8s-proxy | 5005 | registry.k8s.io |\n\n## Talos Machine Config\n\n```yaml\nmachine:\n  registries:\n    mirrors:\n      docker.io:\n        endpoints:\n          - http://nexus-docker-proxy.registry.svc.cluster.local:5001\n      ghcr.io:\n        endpoints:\n          - http://nexus-ghcr-proxy.registry.svc.cluster.local:5002\n      # ... etc\n    config:\n      nexus-docker-proxy.registry.svc.cluster.local:5001:\n        tls:\n          insecureSkipVerify: true\n```\n\n## Considerations\n\n- Nexus needs sufficient storage for cached layers\n- First pull still goes to upstream (cold cache)\n- Need k8s Services for each proxy port\n- Machine config change requires node reboot","acceptance_criteria":"- [ ] All Nexus proxy repositories created and verified\n- [ ] K8s Services expose each proxy port\n- [ ] Talos machine config updated with registry mirrors\n- [ ] Nodes rebooted and config applied\n- [ ] Test pull from each registry routes through proxy\n- [ ] Verify cache hit on second pull (check Nexus logs)","status":"open","priority":2,"issue_type":"epic","created_at":"2025-12-20T16:18:00.539409-08:00","updated_at":"2025-12-20T16:18:00.539409-08:00"}
{"id":"TALOS-wlu","title":"Security Hardening: Implement Layered Defense-in-Depth","description":"Comprehensive security hardening based on architectural audit. Implements tiered security model with network segmentation, edge hardening, container security, and data protection.\n\n## Current State\n- Cilium CNI installed but no NetworkPolicies\n- Traefik with TLS capability but not applied to arr-stack\n- Authentik deployed but middleware not applied\n- ExternalSecrets working but PostgreSQL password hardcoded\n- Containers running as root with :latest tags\n\n## Target Architecture\n- Tier 1 (Public): TLS + Authentik SSO + Rate Limiting\n- Tier 2 (Management): TLS + Auth + NetworkPolicy isolation\n- Tier 3 (Data): No ingress, internal auth, pod-restricted access\n- Tier 4 (Infrastructure): TLS + Authentik + IP allowlist\n\n## Key Findings from Audit\n- CRITICAL: All arr-stack services exposed HTTP without auth\n- CRITICAL: Traefik insecure API enabled\n- HIGH: No NetworkPolicies in media namespace\n- HIGH: PostgreSQL accessible from any pod\n- HIGH: Containers lack securityContext\n- MEDIUM: :latest image tags throughout","status":"open","priority":1,"issue_type":"epic","created_at":"2025-12-17T17:52:54.809721-08:00","updated_at":"2025-12-17T17:52:54.809721-08:00","labels":["epic","infrastructure","security"]}
{"id":"TALOS-wm2f","title":"Analyze SQLite schema and identify migration tables","description":"Analyze the Sonarr/Radarr SQLite databases to understand:\n- Table structure and schema versions\n- Which tables contain media metadata vs logs/history\n- Foreign key relationships\n- Data volumes per table\n\nOutput: List of tables to migrate, estimated data sizes, migration order.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T09:24:10.430203-08:00","updated_at":"2025-12-20T10:05:38.077516-08:00","closed_at":"2025-12-20T10:05:38.077516-08:00","dependencies":[{"issue_id":"TALOS-wm2f","depends_on_id":"TALOS-n8an","type":"parent-child","created_at":"2025-12-20T09:24:23.02537-08:00","created_by":"daemon"}]}
{"id":"TALOS-wqc","title":"Fix dashboard: mongodb-cluster-summary","description":"Assess and fix/replace mongodb-cluster-summary dashboard for V2 OTEL stack compatibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:39.047997-08:00","updated_at":"2025-12-14T13:04:56.638676-08:00","closed_at":"2025-12-14T13:04:56.638676-08:00","labels":["dashboard","observability"],"dependencies":[{"issue_id":"TALOS-wqc","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:39.048862-08:00","created_by":"daemon"}]}
{"id":"TALOS-wzax","title":"Create cluster startup/health verification checklist","description":"Create a comprehensive checklist script/doc for verifying cluster health after startup or maintenance.\n\nShould verify:\n- All nodes Ready\n- Flux kustomizations healthy\n- HelmReleases reconciled\n- Grafana dashboards loaded and datasources connected\n- Tdarr GPU nodes have device access (/dev/dri)\n- NFS mounts accessible (Synology, TrueNAS)\n- Key services responding (Traefik, ArgoCD, Prometheus/Mimir)\n- Backup jobs not failing\n- No pods in CrashLoopBackOff/Error state\n\nCould be a script like `scripts/cluster-health-check.sh` or added to existing `cluster-status.sh`","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T14:18:16.783536-08:00","updated_at":"2026-01-01T14:18:16.783536-08:00","labels":["documentation","monitoring","operations"]}
{"id":"TALOS-x6a","title":"Fix dashboard: node-exporter-full","description":"Assess and fix/replace node-exporter-full dashboard for V2 OTEL stack compatibility. NOTE: node-exporter metrics may not be scraped yet.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T11:31:24.077502-08:00","updated_at":"2025-12-14T12:19:00.280856-08:00","closed_at":"2025-12-14T12:19:00.280856-08:00","labels":["dashboard","infrastructure"],"dependencies":[{"issue_id":"TALOS-x6a","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:31:24.078354-08:00","created_by":"daemon"}]}
{"id":"TALOS-xnh","title":"Dashboard System Audit \u0026 Refactor","description":"Restore broken dashboard library, create domain dashboards (nebula, liqo, linkerd, intel-gpu), build root orchestrator with inline/interactive modes, audit all deployed components for dashboard coverage","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-12T18:40:14.361907-08:00","updated_at":"2025-12-12T18:57:04.576917-08:00","closed_at":"2025-12-12T18:57:04.576917-08:00"}
{"id":"TALOS-xp1","title":"Fix dashboard: kasa-tou-cost-optimization","description":"Assess and fix/replace kasa-tou-cost-optimization dashboard for V2 OTEL stack compatibility","notes":"**Audit 2025-12-14**: Partial functionality\n- ✅ Working: Current rate class, rate display, cost by rate class pie chart, cost distribution table\n- ❌ \"No data\": Potential savings (super off-peak/off-peak), power by rate class, recommendations\n- Root cause: Dashboard queries for off-peak/super-off-peak data that doesn't exist during on-peak periods\n- Likely intentional - these panels populate when off-peak period data is available","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T11:32:15.153149-08:00","updated_at":"2025-12-14T15:57:44.227818-08:00","labels":["dashboard","kasa"],"dependencies":[{"issue_id":"TALOS-xp1","depends_on_id":"TALOS-fq8","type":"blocks","created_at":"2025-12-14T11:32:15.154041-08:00","created_by":"daemon"}]}
{"id":"TALOS-xv2y","title":"Investigate Tdarr failFlow error on talos03-amd-vaapi worker","description":"The talos03-amd-vaapi GPU worker is showing a failFlow error in logs:\n\n```\n[ERROR] Tdarr_Node - Error: Forcing flow to fail!\n    at Object.plugin (/app/Tdarr_Node/assets/app/plugins/FlowPlugins/CommunityFlowPlugins/tools/failFlow/1.0.0/index.js:26:11)\n```\n\nThis could indicate:\n1. A flow is intentionally configured with a failFlow plugin (for testing)\n2. A misconfigured flow that's hitting an error condition\n3. A flow that needs proper VAAPI stage configuration\n\nInvestigation needed:\n- Check Tdarr UI for flow configurations\n- Identify which flow is triggering the error\n- Verify VAAPI stages are properly configured for talos03-amd-vaapi worker","status":"open","priority":3,"issue_type":"bug","created_at":"2025-12-19T18:01:45.94473-08:00","updated_at":"2025-12-19T18:01:45.94473-08:00","labels":["gpu","talos03","tdarr"]}
{"id":"TALOS-y0k8","title":"Deploy Home Assistant with Bluetooth USB passthrough in new home-stack","description":"Create a new `applications/home-stack/` directory structure (alongside arr-stack) and deploy Home Assistant with Bluetooth USB capability.\n\n## Requirements\n- New stack structure: `applications/home-stack/base/homeassistant/`\n- Home Assistant deployment with persistent storage\n- Bluetooth USB passthrough from host to container (requires Talos USB device plugin or privileged container)\n- Traefik IngressRoute for `homeassistant.talos00`\n\n## Technical Considerations\n- Talos Linux requires special handling for USB device passthrough\n- May need to use `hostPath` volume or device plugin for Bluetooth adapter\n- Consider using the `linuxserver/homeassistant` or official `homeassistant/home-assistant` image\n- Bluetooth integration may require `NET_ADMIN` and `NET_RAW` capabilities\n- Node affinity to pin to node with physical Bluetooth USB adapter","design":"## Structure Created\n\n```\napplications/home-stack/\n└── base/\n    ├── kustomization.yaml\n    └── homeassistant/\n        ├── deployment.yaml   # Privileged, hostNetwork, USB mounts\n        ├── service.yaml\n        ├── pvc.yaml          # 10Gi local-path\n        ├── ingressroute.yaml # homeassistant.talos00\n        └── kustomization.yaml\n\ninfrastructure/base/namespaces/home-automation.yaml\nclusters/catalyst-cluster/home-stack.yaml  # Flux Kustomization\n```\n\n## Bluetooth USB Setup\n\nDeployment requires node label before it will schedule:\n```bash\nkubectl label node \u003cnode-name\u003e bluetooth.homeassistant.io/enabled=true\n```\n\nMounts for Bluetooth:\n- `/dev/bus/usb` - USB device bus\n- `/run/dbus` - D-Bus for BlueZ\n- `/dev/bluetooth` - Bluetooth device nodes\n- `/run/udev` - Device events\n\n## Access\n- URL: http://homeassistant.talos00\n- Namespace: home-automation","notes":"Created manifests, pending commit and deploy.\nNode with BT dongle needs label: bluetooth.homeassistant.io/enabled=true","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-19T21:52:11.667516-08:00","updated_at":"2025-12-23T16:13:22.110479-08:00","closed_at":"2025-12-23T16:13:22.110479-08:00","labels":["bluetooth","home-automation","new-stack"]}
{"id":"TALOS-y1b","title":"Implement ProtonVPN egress gateway for SecureXNG","description":"Route traffic through ProtonVPN using Talos-native WireGuard + Cilium Egress Gateway, with SOCKS5 proxy exposed via Traefik.\n\n**Architecture:**\n```\nHome Device → vpn-proxy.talos00:1080 (Traefik TCP) → SOCKS5 Pod → Cilium Egress → WireGuard → ProtonVPN → Internet\n```\n\n**Components:**\n- WireGuard at OS layer (Talos MachineConfig) on talos01\n- Cilium Egress Gateway steers labeled pods to wg0 interface\n- SOCKS5 proxy (microsocks) exposed via Traefik IngressRouteTCP\n- No privileged DaemonSet needed\n\n**Current State:**\n- Cilium: Deployed with SPIRE mTLS\n- Egress Gateway: NOT enabled - CRD does not exist\n- ProtonVPN configs: 4 servers ready (BE, DE, IN, NL)\n- Cluster CIDRs: Pod=10.244.0.0/16, Service=10.96.0.0/12, Local=192.168.1.0/24","design":"**Phase 1: Enable Cilium Egress Gateway**\n1. Update `configs/cilium-values.yaml` - add `egressGateway.enabled: true`\n2. Regenerate manifest: `helm template cilium cilium/cilium --version 1.16.6 -n kube-system -f configs/cilium-values.yaml --kube-version 1.34.0`\n3. Apply and restart Cilium\n\n**Phase 2: Talos WireGuard Config**\n1. Parse ProtonVPN .toml configs to extract keys/endpoints\n2. Create `configs/patches/wireguard-egress.yaml` for talos01:\n   - wg0 interface with ProtonVPN peer\n   - Node label `egress-gateway: vpn`\n3. Apply config (no reboot needed for network changes)\n\n**Phase 3: SOCKS5 Proxy + Egress Policy**\nCreate `infrastructure/base/vpn-proxy/`:\n- `namespace.yaml` - vpn-proxy namespace\n- `deployment.yaml` - microsocks SOCKS5 proxy\n- `service.yaml` - ClusterIP on port 1080\n- `ingressroutetcp.yaml` - Traefik TCP route for vpn-proxy.talos00:1080\n- `egress-policy.yaml` - CiliumEgressGatewayPolicy targeting proxy pod\n\n**Phase 4: Verification**\n1. Test WireGuard tunnel: `talosctl -n 192.168.1.177 read /proc/net/wireguard`\n2. Test proxy returns VPN IP: `curl --socks5 vpn-proxy.talos00:1080 ifconfig.me`\n3. Test from home device browser\n\n**Phase 5 (Future): IP Rotation**\n- CronJob to rotate ProtonVPN server configs\n- Store multiple configs as Secrets","acceptance_criteria":"- [ ] Cilium egress gateway CRD exists: `kubectl get crd | grep ciliumegressgateway`\n- [ ] wg0 interface on talos01: `talosctl -n 192.168.1.177 get links | grep wg0`\n- [ ] WireGuard tunnel up: `talosctl -n 192.168.1.177 read /proc/net/wireguard`\n- [ ] Egress policy applied: `kubectl get ciliumegressgatewaypolicy`\n- [ ] SearXNG returns ProtonVPN IP: `kubectl exec -n catalyst-llm deploy/searxng -- curl -s ifconfig.me`","notes":"## Session 2025-12-17: WireGuard routing issue\n\n### What was attempted:\n- Enabled Cilium Egress Gateway in cilium-values.yaml\n- Configured WireGuard on talos01 with ProtonVPN credentials\n- Applied WireGuard config with 0.0.0.0/0 default route through wg0\n- Deployed SOCKS5 proxy (go-socks5-proxy) with CiliumEgressGatewayPolicy\n- Created fail-closed CiliumNetworkPolicy\n\n### Problem encountered:\n- WireGuard default route (0.0.0.0/0 via wg0 metric 100) breaks node connectivity\n- VPN endpoint route (185.159.156.171/32) was added but didn't help\n- talos01 became unreachable - all traffic trying to go through wg0 which has no working tunnel\n- Chicken-egg problem: can't establish tunnel because outbound traffic goes through wg0\n\n### Resolution plan:\n- Need dedicated physical NIC for VPN egress on talos01\n- Primary NIC (enp3s0) stays for cluster traffic\n- Secondary NIC gets WireGuard tunnel\n- This isolates VPN routing from cluster networking\n\n### Recovery needed:\n- talos01 needs physical reboot\n- Apply recovery patch: configs/patches/wireguard-remove-talos01.yaml\n- Or boot into maintenance mode with clean config\n\n### Files created:\n- configs/patches/wireguard-egress-talos01.yaml (WireGuard config - needs dedicated NIC)\n- configs/patches/wireguard-remove-talos01.yaml (recovery patch)\n- infrastructure/base/vpn-proxy/* (deleted, will recreate)","status":"blocked","priority":2,"issue_type":"feature","created_at":"2025-12-17T08:22:32.918197-08:00","updated_at":"2025-12-17T22:18:33.862167-08:00","labels":["cilium","infrastructure","searxng","vpn"],"dependencies":[{"issue_id":"TALOS-y1b","depends_on_id":"TALOS-bxh","type":"blocks","created_at":"2025-12-17T12:25:18.034565-08:00","created_by":"daemon"}]}
{"id":"TALOS-y1c","title":"Boot talos02-gpu with custom Talos image","description":"Generate custom Talos ISO from factory.talos.dev with i915 extension, boot NUC, apply machine config","notes":"**Progress:**\n- Generated schematic ID: `4b3cd373a192c8469e859b7a0cfbed3ecc3577c4a2d346a37b0aeff9cd17cdb0`\n- Downloaded custom ISO to `configs/talos02-gpu-v1.11.1.iso`\n- Updated `configs/nodes/worker-talos02-gpu.yaml`:\n  - Added IP 192.168.1.144 to certSANs\n  - Updated install.image to use factory schematic\n- Updated `clusters/catalyst-cluster/cluster-settings.yaml`:\n  - Added TALOS02_GPU_NODE_IP: '192.168.1.144'\n- Node booted from USB, now in maintenance mode at 192.168.1.144\n\n**Next:** Apply machine config to install to disk","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-12T17:02:36.902399-08:00","updated_at":"2025-12-15T13:22:23.933691-08:00","closed_at":"2025-12-15T13:22:23.933691-08:00","dependencies":[{"issue_id":"TALOS-y1c","depends_on_id":"TALOS-fpp","type":"parent-child","created_at":"2025-12-12T17:02:44.406656-08:00","created_by":"daemon"}]}
{"id":"TALOS-yzt","title":"Fix Prometheus widget - use Mimir endpoint","description":"Update Prometheus widget URL from kube-prometheus-stack-prometheus to mimir-nginx.monitoring.svc.cluster.local:80. Note: Mimir has a Prometheus-compatible API.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-15T15:48:19.908608-08:00","updated_at":"2025-12-15T16:02:07.464299-08:00","closed_at":"2025-12-15T16:02:07.464299-08:00","labels":["homepage","monitoring"]}
{"id":"TALOS-z5s","title":"Implement authentication gateway with Authentik","description":"Deploy Authentik as identity provider for external-facing services on knowledgedump.space.\n\n**Completed:**\n- ✅ Authentik deployed via Helm (server, worker, postgresql, redis)\n- ✅ ExternalSecret syncing from 1Password\n- ✅ Let's Encrypt wildcard TLS via TLSStore\n- ✅ IngressRoutes for auth.knowledgedump.space\n- ✅ ForwardAuth middleware created\n- ✅ whoami test service deployed with auth middleware\n\n**Pending (manual UI setup required):**\n- Create Application in Authentik admin\n- Create Proxy Provider (ForwardAuth mode)\n- Create/configure embedded Outpost\n- Test complete auth flow\n\n**Note:** Cloudflare DNS is proxying traffic. Consider setting to DNS-only for direct cluster access with Let's Encrypt cert.","notes":"Switched from Authelia to Authentik due to K8s service discovery env var conflicts in Authelia.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-16T18:47:16.236199-08:00","updated_at":"2025-12-16T21:19:44.028294-08:00","closed_at":"2025-12-16T21:19:44.028294-08:00","labels":["authelia","security","traefik"]}
{"id":"TALOS-z6nh","title":"Deconflict Kometa and Posterizarr overlay pipelines","description":"Kometa and Posterizarr may both be applying overlays to Plex posters, causing conflicts or double-processing.\n\n## Current State\n- Kometa: Applies overlays (resolution, audio, HDR, etc.) via Plex API\n- Posterizarr: Also applies overlays/generates posters\n\n## Options to Resolve\n\n### Option 1: Kometa only (disable Posterizarr overlays)\n- Use Kometa for all overlay work\n- Posterizarr only for poster generation/sourcing\n\n### Option 2: Posterizarr only (disable Kometa overlays)\n- Posterizarr handles everything\n- Kometa only for collections/metadata\n\n### Option 3: Full Posterizarr integration\n- Migrate all overlay config to Posterizarr\n- More modern UI, better preview\n\n### Option 4: Kometa WebUI\n- Add Kometa-UI or similar for easier config management\n- Keep Kometa as the overlay engine\n\n## Investigation Needed\n- Which overlays are currently enabled in each?\n- Are they stepping on each other?\n- Which tool produces better results?\n- Performance impact of running both","notes":"## Investigation Results\n\n### Kometa Overlays (ACTIVE)\n**Movies:**\n- `ratings` - IMDb score badge (top right, cyan #00FFFF)\n- `mediastinger` - Post-credits indicator (top left, magenta #FF00FF)\n- `resolution` - 4K/1080p badge (bottom left)\n- `video_format` - HDR/DV badge (bottom left, gold #FFD700)\n- `ribbon` - Award ribbons (bottom right)\n- `audio_codec` - Atmos/DTS:X badge (bottom right, pink #FF1493)\n\n**TV Shows:**\n- `status` - Airing/Returning/Ended/Canceled (top left, color-coded)\n- `ratings` - IMDb score badge (top right)\n- `resolution` - Quality badge (bottom left)\n- `ribbon` - Award ribbons (bottom right)\n- `network` - Network logo (bottom right)\n\n**Theme:** Synthwave (#1a1a2e dark base with neon accents)\n\n---\n\n### Posterizarr Overlays (ALSO ACTIVE)\n**PosterOverlayPart:**\n- `AddOverlay: true`\n- `AddBorder: true` - 30px border in #3DFFA5 (mint green)\n- `AddText: true` - Title at bottom in #A700BD (purple)\n\n**BackgroundOverlayPart:**\n- `AddOverlay: true`\n- `AddBorder: true` - 30px black border\n- `AddText: true` - White text\n\n**SeasonPosterOverlayPart:**\n- `AddOverlay: true` - Similar to poster overlay\n\n---\n\n### Conflict Analysis\n\n| Aspect | Kometa | Posterizarr | Conflict? |\n|--------|--------|-------------|-----------|\n| Poster modifications | Info badges | Border + title text | ⚠️ YES |\n| Background art | No | Border + text | No |\n| Season posters | No | Border + text | No |\n| Title cards | No | Yes (TitleCardOverlayPart) | No |\n| Collections | Yes (posters) | Yes (CollectionPosterOverlayPart) | ⚠️ YES |\n\n### The Problem\nBoth tools are modifying **movie/show posters** in Plex:\n1. Posterizarr adds decorative borders + title styling\n2. Kometa then adds info overlays on top\n\nResult: Potentially cluttered posters with both border/text AND badges.\n\n### Recommendations\n\n**Option A: Posterizarr for visuals, Kometa for metadata only**\n- Disable Kometa `overlay_files` sections\n- Keep Kometa `collection_files` for collections/charts\n- Posterizarr handles all poster styling\n\n**Option B: Kometa for everything, Posterizarr as poster source only**\n- Disable Posterizarr overlays (`AddOverlay: false`, `AddBorder: false`, `AddText: false`)\n- Keep Posterizarr for textless poster sourcing\n- Kometa applies synthwave overlays\n\n**Option C: Separate domains**\n- Posterizarr: Title cards, season posters, backgrounds only\n- Kometa: Main posters with info badges","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-31T20:30:48.656696-08:00","updated_at":"2025-12-31T22:29:48.296633-08:00","closed_at":"2025-12-31T22:29:48.296633-08:00","labels":["dedup","kometa","media","posterizarr"],"dependencies":[{"issue_id":"TALOS-z6nh","depends_on_id":"TALOS-4ubj","type":"parent-child","created_at":"2025-12-31T20:39:43.642489-08:00","created_by":"daemon"}]}
{"id":"TALOS-za1","title":"Phase 3: Add securityContext to all arr-stack deployments","description":"Harden containers with proper securityContext settings.\n\n## Tasks\n- [ ] Add pod securityContext with fsGroup\n- [ ] Add container securityContext:\n  - runAsNonRoot: true (where possible)\n  - readOnlyRootFilesystem: true (where possible)\n  - allowPrivilegeEscalation: false\n  - capabilities.drop: [ALL]\n- [ ] Test each service still works\n- [ ] Document exceptions (linuxserver images may need root)\n\n## Note\nLinuxServer.io images use PUID/PGID env vars but may still need root initially. Test carefully.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-17T17:53:42.698693-08:00","updated_at":"2025-12-17T17:53:42.698693-08:00","labels":["containers","security"],"dependencies":[{"issue_id":"TALOS-za1","depends_on_id":"TALOS-wlu","type":"parent-child","created_at":"2025-12-17T17:54:00.3364-08:00","created_by":"daemon"},{"issue_id":"TALOS-za1","depends_on_id":"TALOS-22p","type":"blocks","created_at":"2025-12-17T17:54:12.054282-08:00","created_by":"daemon"}]}
{"id":"TALOS-zd3","title":"Redact secrets from git history","description":"Secrets were detected in git history from Talos config files that were committed before being gitignored. Need to safely redact these using git-filter-repo or BFG on a clean branch.\n\nFiles with secrets:\n- configs/worker-talos01.yaml (lines 351, 391, 396)\n- Other configs/nodes/*.yaml files\n\nDetected secrets:\n- aescbcEncryptionSecret\n- secretboxEncryptionSecret\n- Shared secrets\n\nThis needs to be done carefully on a clean branch to avoid corrupting history.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T21:25:23.558702-08:00","updated_at":"2025-12-14T21:25:23.558702-08:00","labels":["git","security"]}
{"id":"TALOS-zh5u","title":"Create qBittorrent deployment with gluetun VPN sidecar","description":"Create the main deployment manifest with qBittorrent container and gluetun sidecar. Use network_mode to route all qbit traffic through gluetun. Reference: infrastructure/base/vpn-gateway/securexng.yaml pattern.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T13:31:36.998703-08:00","updated_at":"2025-12-20T13:34:08.305467-08:00","closed_at":"2025-12-20T13:34:08.305467-08:00","dependencies":[{"issue_id":"TALOS-zh5u","depends_on_id":"TALOS-m6xf","type":"blocks","created_at":"2025-12-20T13:31:37.004114-08:00","created_by":"daemon"}]}
{"id":"TALOS-zn4z","title":"Add ProtonVPN Switzerland/Iceland WireGuard keys","description":"Current protonvpn-credentials secret only has: se-be-1 (Belgium), se-de-1 (Germany), se-in-1 (India), se-nl-1 (Netherlands).\n\nNeed to add Swiss and Icelandic servers for better privacy:\n1. Login to ProtonVPN account\n2. Generate WireGuard configs for Switzerland (CH) and Iceland (IS) servers\n3. Add keys to 1Password vault (synced via ESO)\n4. Update secure-chrome and securexng to use Swiss exit nodes","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T15:49:54.809646-08:00","updated_at":"2026-01-01T15:49:54.809646-08:00","labels":["protonvpn","security","vpn"]}
{"id":"TALOS-zqh","title":"Remove kube-prometheus-stack and migrate to V2 OTEL","description":"Migrate from kube-prometheus-stack to pure V2 OTEL stack (Alloy + Mimir + Loki + Tempo).\n\n**Components to remove:**\n- Prometheus (replaced by Mimir)\n- Alertmanager (Mimir has built-in)\n- prometheus-operator (Alloy uses native discovery)\n- Bundled Grafana (using grafana-operator)\n\n**Components to migrate:**\n- kube-state-metrics → standalone deployment\n- 32 PrometheusRules → Mimir ruler format\n- 56 ServiceMonitors → Document/archive (Alloy uses native discovery)\n\n**Success criteria:**\n- Standalone kube-state-metrics running\n- Alerting rules migrated to Mimir\n- kube-prometheus-stack HelmRelease removed\n- Metrics still flowing to Mimir","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-13T22:26:29.825365-08:00","updated_at":"2025-12-13T22:39:32.683209-08:00","closed_at":"2025-12-13T22:39:32.683209-08:00","labels":["migration","monitoring","otel"]}
{"id":"TALOS-zut","title":"Create nebula dashboard","description":"infrastructure/base/nebula/dashboard.sh - mesh nodes, lighthouse, AWS worker connectivity, overlay IPs","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-12T18:40:35.557884-08:00","updated_at":"2025-12-12T18:55:36.384594-08:00","closed_at":"2025-12-12T18:55:36.384594-08:00"}
