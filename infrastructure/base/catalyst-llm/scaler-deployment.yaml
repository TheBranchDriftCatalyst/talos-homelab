---
# LLM Scaler - Scale-to-zero proxy for AWS Ollama worker
# Automatically starts/stops EC2 instance based on request activity
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-scaler
  namespace: catalyst-llm
  labels:
    app.kubernetes.io/name: llm-scaler
    app.kubernetes.io/component: proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: llm-scaler
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llm-scaler
        app.kubernetes.io/component: proxy
    spec:
      serviceAccountName: llm-scaler
      containers:
        - name: scaler
          image: ghcr.io/thebranchdriftcatalyst/llm-scaler:latest
          ports:
            - name: http
              containerPort: 8080
            - name: metrics
              containerPort: 9090
          env:
            - name: LISTEN_ADDR
              value: ":8080"
            - name: METRICS_ADDR
              value: ":9090"
            - name: OLLAMA_URL
              value: "http://10.42.2.1:11434"
            - name: IDLE_TIMEOUT
              value: "40m"
            - name: WARMUP_TIMEOUT
              value: "5m"
            - name: WORKER_SCRIPT
              value: "/app/llm-worker.sh"
            - name: AWS_REGION
              value: "us-west-2"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: secret-access-key
          volumeMounts:
            - name: worker-script
              mountPath: /app/llm-worker.sh
              subPath: llm-worker.sh
            - name: worker-state
              mountPath: /app/.output
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 5
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 2
            periodSeconds: 10
          resources:
            requests:
              cpu: 50m
              memory: 64Mi
            limits:
              cpu: 200m
              memory: 128Mi
      volumes:
        - name: worker-script
          configMap:
            name: llm-worker-script
            defaultMode: 0755
        - name: worker-state
          configMap:
            name: llm-worker-state
---
apiVersion: v1
kind: Service
metadata:
  name: llm-scaler
  namespace: catalyst-llm
  labels:
    app.kubernetes.io/name: llm-scaler
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8080
      targetPort: http
    - name: metrics
      port: 9090
      targetPort: metrics
  selector:
    app.kubernetes.io/name: llm-scaler
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: llm-scaler
  namespace: catalyst-llm
---
# ServiceMonitor for Prometheus scraping
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: llm-scaler
  namespace: catalyst-llm
  labels:
    app.kubernetes.io/name: llm-scaler
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: llm-scaler
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
