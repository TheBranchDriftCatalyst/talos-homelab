---
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: prometheus-community
  namespace: monitoring
spec:
  interval: 24h
  url: https://prometheus-community.github.io/helm-charts
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 30m
  timeout: 20m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: '>=65.0.0 <66.0.0' # Lock to v65.x for stability
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: monitoring
      interval: 12h

  # Ensure CRDs are managed
  install:
    crds: Create
    createNamespace: false
    remediation:
      retries: 3

  upgrade:
    crds: CreateReplace
    cleanupOnFail: true
    remediation:
      retries: 3

  values:
    # Global settings
    fullnameOverride: 'kube-prometheus-stack'

    # Prometheus Operator
    prometheusOperator:
      enabled: true
      admissionWebhooks:
        enabled: true
        patch:
          enabled: true
      resources:
        requests:
          cpu: 25m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 256Mi

    # Prometheus
    prometheus:
      enabled: true

      prometheusSpec:
        # Target worker node for monitoring workloads
        nodeSelector:
          node-role.kubernetes.io/worker: ''
        # Retention and storage
        retention: 30d
        retentionSize: '45GB'

        # Storage configuration (using local-path)
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: local-path
              accessModes: ['ReadWriteOnce']
              resources:
                requests:
                  storage: 50Gi

        # Resource limits - increased for worker node capacity
        resources:
          requests:
            cpu: 500m
            memory: 2Gi
          limits:
            cpu: 4000m
            memory: 8Gi

        # Service monitors
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false

        # Enable all service monitors
        serviceMonitorSelector: {}
        podMonitorSelector: {}

        # Scrape intervals
        scrapeInterval: 30s
        evaluationInterval: 30s

        # Enable features
        enableAdminAPI: true
        enableRemoteWriteReceiver: false

        # Additional scrape configs for services without ServiceMonitors
        # NOTE: Graylog, OpenSearch, and MongoDB now use dedicated exporters with ServiceMonitors
        # See infrastructure/base/observability/*/exporter.yaml
        additionalScrapeConfigs: []

        # Security context - run as root for NFS compatibility
        securityContext:
          runAsNonRoot: false
          runAsUser: 0
          fsGroup: 0
          seccompProfile:
            type: RuntimeDefault

    # Alertmanager
    alertmanager:
      enabled: true

      alertmanagerSpec:
        # Target worker node for monitoring workloads
        nodeSelector:
          node-role.kubernetes.io/worker: ''
        # Storage configuration
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: local-path
              accessModes: ['ReadWriteOnce']
              resources:
                requests:
                  storage: 10Gi

        # Resource limits - increased for worker node capacity
        resources:
          requests:
            cpu: 50m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi

        # Security context
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
          runAsGroup: 65534
          fsGroup: 65534
          seccompProfile:
            type: RuntimeDefault

    # Grafana
    grafana:
      enabled: true

      # Target worker node for monitoring workloads
      nodeSelector:
        node-role.kubernetes.io/worker: ''

      # Admin credentials managed via ExternalSecret (grafana-admin-credentials)
      admin:
        existingSecret: kube-prometheus-stack-grafana
        userKey: admin-user
        passwordKey: admin-password

      # Persistence
      persistence:
        enabled: true
        storageClassName: local-path
        accessModes: ['ReadWriteOnce']
        size: 10Gi

      # Resource limits - increased for worker node capacity
      resources:
        requests:
          cpu: 100m
          memory: 512Mi
        limits:
          cpu: 1000m
          memory: 1Gi

      # Datasources - configured statically via Helm values (simpler for single-node homelab)
      # Sidecar datasource discovery is disabled to avoid duplicate default datasource errors
      sidecar:
        dashboards:
          enabled: true
          label: grafana_dashboard
          labelValue: '1'
          searchNamespace: ALL
        datasources:
          enabled: false  # Disabled - using static datasources below instead

      # Static datasource configuration
      datasources:
        datasources.yaml:
          apiVersion: 1
          datasources:
            - name: Prometheus
              type: prometheus
              url: http://kube-prometheus-stack-prometheus.monitoring.svc:9090
              access: proxy
              isDefault: true
              editable: true
            - name: Alertmanager
              type: alertmanager
              url: http://kube-prometheus-stack-alertmanager.monitoring.svc:9093
              access: proxy
              editable: true
              jsonData:
                implementation: prometheus

      # Default dashboards
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: 'default'
              orgId: 1
              folder: ''
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/default

      # Dashboards are managed via GrafanaDashboard CRDs
      # See infrastructure/base/monitoring/grafana-dashboards/
      dashboards: {}

      # Grafana.ini customizations
      grafana.ini:
        server:
          root_url: 'http://grafana.talos00:80'
        analytics:
          check_for_updates: false
        security:
          allow_embedding: true

    # Node Exporter
    nodeExporter:
      enabled: true
      resources:
        requests:
          cpu: 25m
          memory: 32Mi
        limits:
          cpu: 200m
          memory: 128Mi

    # Kube State Metrics
    kube-state-metrics:
      enabled: true
      resources:
        requests:
          cpu: 25m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 256Mi
      # Note: VPA metrics are collected via VPA recommender directly, not kube-state-metrics
      # kube-state-metrics v2.13.0 requires custom resource state config for VPA which is complex

    # Default rules and alerts
    defaultRules:
      create: true
      rules:
        alertmanager: true
        etcd: true
        configReloaders: true
        general: true
        k8s: true
        kubeApiserverAvailability: true
        kubeApiserverBurnrate: true
        kubeApiserverHistogram: true
        kubeApiserverSlos: true
        # Disabled for Talos Linux - these run as static pods without exposed metrics
        kubeControllerManager: false
        kubelet: true
        # Disabled for Talos Linux - kube-proxy runs as static pod without exposed metrics
        kubeProxy: false
        kubePrometheusGeneral: true
        kubePrometheusNodeRecording: true
        kubernetesApps: true
        kubernetesResources: true
        kubernetesStorage: true
        kubernetesSystem: true
        # Disabled for Talos Linux - scheduler runs as static pod without exposed metrics
        kubeSchedulerAlerting: false
        kubeSchedulerRecording: false
        kubeStateMetrics: true
        network: true
        node: true
        nodeExporterAlerting: true
        nodeExporterRecording: true
        prometheus: true
        prometheusOperator: true

    # Component-specific service monitors
    coreDns:
      enabled: true

    kubeDns:
      enabled: false

    kubeEtcd:
      enabled: true

    # Disabled on Talos Linux - these components don't expose metrics on standard ports
    kubeControllerManager:
      enabled: false

    kubeScheduler:
      enabled: false

    kubeProxy:
      enabled: false

    kubeApiServer:
      enabled: true
