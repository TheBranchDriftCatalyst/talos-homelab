---
# ConfigMap for pod-cleanup configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: pod-cleanup-config
  namespace: kube-system
  labels:
    app.kubernetes.io/name: pod-cleanup
    app.kubernetes.io/component: maintenance
data:
  DRY_RUN: "false"
  FAILED_POD_AGE_THRESHOLD: "3600"
  EVICTED_POD_AGE_THRESHOLD: "1800"
  IMAGEPULL_AGE_THRESHOLD: "7200"
  CRASHLOOP_AGE_THRESHOLD: "14400"
  COMPLETED_JOB_AGE_THRESHOLD: "86400"
  ORPHAN_RS_AGE_THRESHOLD: "86400"
  CLEANUP_SUCCEEDED_PODS: "true"
  CLEANUP_FAILED_PODS: "true"
  CLEANUP_EVICTED_PODS: "true"
  CLEANUP_IMAGEPULL_PODS: "true"
  CLEANUP_CRASHLOOP_PODS: "false"
  CLEANUP_COMPLETED_JOBS: "true"
  CLEANUP_ORPHAN_REPLICASETS: "true"
  EXCLUDED_NAMESPACES: "kube-system,kube-public,kube-node-lease"
  EXCLUSION_LABEL: "cleanup.kubernetes.io/ignore"
  CRASHLOOP_RESTART_THRESHOLD: "10"
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: pod-cleanup
  namespace: kube-system
  labels:
    app.kubernetes.io/name: pod-cleanup
    app.kubernetes.io/component: maintenance
spec:
  schedule: '0 */2 * * *'
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 3600
      template:
        metadata:
          labels:
            app.kubernetes.io/name: pod-cleanup
        spec:
          serviceAccountName: pod-cleanup
          restartPolicy: OnFailure
          containers:
            - name: cleanup
              image: bitnami/kubectl:latest
              envFrom:
                - configMapRef:
                    name: pod-cleanup-config
              env:
                - name: PUSHGATEWAY_URL
                  value: 'http://prometheus-pushgateway.monitoring.svc.cluster.local:9091'
              command:
                - /bin/sh
                - -c
                - |
                  set -e
                  START_TIME=$(date +%s)

                  # Single temp file for candidates
                  CANDIDATES=$(mktemp)
                  trap "rm -f $CANDIDATES" EXIT

                  log_info() { echo "[INFO] $1"; }
                  log_warn() { echo "[WARN] $1"; }

                  is_excluded_ns() {
                    echo "$EXCLUDED_NAMESPACES" | tr ',' '\n' | grep -qx "$1"
                  }

                  # Delete resources from candidates file, returns count
                  cleanup_resources() {
                    kind=$1
                    count=0
                    while IFS=' ' read -r ns name; do
                      [ -z "$ns" ] || [ -z "$name" ] && continue
                      is_excluded_ns "$ns" && continue
                      if [ "$DRY_RUN" = "true" ]; then
                        echo "[DRY-RUN] Would delete $kind $ns/$name"
                      else
                        kubectl delete "$kind" -n "$ns" "$name" --ignore-not-found --wait=false 2>/dev/null && \
                          echo "[DELETE] $kind $ns/$name"
                      fi
                      count=$((count + 1))
                    done < "$CANDIDATES"
                    echo $count
                  }

                  echo "=== Kubernetes Cleanup Job - $(date) ==="
                  echo "Mode: $([ "$DRY_RUN" = "true" ] && echo "DRY-RUN" || echo "LIVE")"
                  echo ""

                  # 1. Succeeded Pods
                  SUCCEEDED=0
                  if [ "$CLEANUP_SUCCEEDED_PODS" = "true" ]; then
                    log_info "Cleaning Succeeded Pods..."
                    kubectl get pods -A --field-selector=status.phase==Succeeded -o jsonpath='{range .items[*]}{.metadata.namespace} {.metadata.name}{"\n"}{end}' > "$CANDIDATES" 2>/dev/null || true
                    SUCCEEDED=$(cleanup_resources pod)
                    log_info "Succeeded pods: $SUCCEEDED"
                  fi

                  # 2. Failed Pods
                  FAILED=0
                  if [ "$CLEANUP_FAILED_PODS" = "true" ]; then
                    log_info "Cleaning Failed Pods (>${FAILED_POD_AGE_THRESHOLD}s)..."
                    kubectl get pods -A --field-selector=status.phase==Failed -o json 2>/dev/null | \
                      jq -r --argjson t "$FAILED_POD_AGE_THRESHOLD" '.items[] | select((now - (.status.startTime | fromdateiso8601 // 0)) > $t) | "\(.metadata.namespace) \(.metadata.name)"' > "$CANDIDATES" 2>/dev/null || true
                    FAILED=$(cleanup_resources pod)
                    log_info "Failed pods: $FAILED"
                  fi

                  # 3. Evicted Pods
                  EVICTED=0
                  if [ "$CLEANUP_EVICTED_PODS" = "true" ]; then
                    log_info "Cleaning Evicted Pods..."
                    kubectl get pods -A -o json 2>/dev/null | \
                      jq -r --argjson t "$EVICTED_POD_AGE_THRESHOLD" '.items[] | select(.status.reason == "Evicted") | select((now - (.status.startTime | fromdateiso8601 // 0)) > $t) | "\(.metadata.namespace) \(.metadata.name)"' > "$CANDIDATES" 2>/dev/null || true
                    EVICTED=$(cleanup_resources pod)
                    log_info "Evicted pods: $EVICTED"
                  fi

                  # 4. ImagePullBackOff Pods
                  IMAGEPULL=0
                  if [ "$CLEANUP_IMAGEPULL_PODS" = "true" ]; then
                    log_info "Cleaning ImagePullBackOff Pods..."
                    kubectl get pods -A -o json 2>/dev/null | \
                      jq -r --argjson t "$IMAGEPULL_AGE_THRESHOLD" '.items[] | select(.status.containerStatuses[]?.state.waiting.reason == "ImagePullBackOff" or .status.containerStatuses[]?.state.waiting.reason == "ErrImagePull") | select((now - (.metadata.creationTimestamp | fromdateiso8601 // 0)) > $t) | "\(.metadata.namespace) \(.metadata.name)"' 2>/dev/null | sort -u > "$CANDIDATES" || true
                    IMAGEPULL=$(cleanup_resources pod)
                    log_info "ImagePullBackOff pods: $IMAGEPULL"
                  fi

                  # 5. CrashLoopBackOff Pods (disabled by default)
                  CRASHLOOP=0
                  if [ "$CLEANUP_CRASHLOOP_PODS" = "true" ]; then
                    log_info "Cleaning CrashLoopBackOff Pods..."
                    kubectl get pods -A -o json 2>/dev/null | \
                      jq -r --argjson t "$CRASHLOOP_AGE_THRESHOLD" --argjson r "$CRASHLOOP_RESTART_THRESHOLD" '.items[] | select(.status.containerStatuses[]?.state.waiting.reason == "CrashLoopBackOff") | select(.status.containerStatuses[].restartCount > $r) | select((now - (.metadata.creationTimestamp | fromdateiso8601 // 0)) > $t) | "\(.metadata.namespace) \(.metadata.name)"' 2>/dev/null | sort -u > "$CANDIDATES" || true
                    CRASHLOOP=$(cleanup_resources pod)
                    log_info "CrashLoopBackOff pods: $CRASHLOOP"
                  fi

                  # 6. Completed Jobs (not owned by CronJobs)
                  JOBS=0
                  if [ "$CLEANUP_COMPLETED_JOBS" = "true" ]; then
                    log_info "Cleaning Completed Jobs..."
                    kubectl get jobs -A -o json 2>/dev/null | \
                      jq -r --argjson t "$COMPLETED_JOB_AGE_THRESHOLD" '.items[] | select(.status.succeeded >= 1 or .status.failed >= 1) | select(.status.completionTime != null) | select((now - (.status.completionTime | fromdateiso8601 // 0)) > $t) | select(.metadata.ownerReferences == null or (.metadata.ownerReferences | map(.kind) | contains(["CronJob"]) | not)) | "\(.metadata.namespace) \(.metadata.name)"' > "$CANDIDATES" 2>/dev/null || true
                    JOBS=$(cleanup_resources job)
                    log_info "Completed jobs: $JOBS"
                  fi

                  # 7. Orphaned ReplicaSets
                  REPLICASETS=0
                  if [ "$CLEANUP_ORPHAN_REPLICASETS" = "true" ]; then
                    log_info "Cleaning Orphaned ReplicaSets..."
                    kubectl get replicasets -A -o json 2>/dev/null | \
                      jq -r --argjson t "$ORPHAN_RS_AGE_THRESHOLD" '.items[] | select(.spec.replicas == 0 and .status.replicas == 0) | select((now - (.metadata.creationTimestamp | fromdateiso8601 // 0)) > $t) | "\(.metadata.namespace) \(.metadata.name)"' > "$CANDIDATES" 2>/dev/null || true
                    REPLICASETS=$(cleanup_resources replicaset)
                    log_info "Orphaned ReplicaSets: $REPLICASETS"
                  fi

                  # Summary
                  END_TIME=$(date +%s)
                  DURATION=$((END_TIME - START_TIME))
                  TOTAL=$((SUCCEEDED + FAILED + EVICTED + IMAGEPULL + CRASHLOOP + JOBS + REPLICASETS))

                  echo ""
                  echo "=== Summary (${DURATION}s) ==="
                  echo "Succeeded:$SUCCEEDED Failed:$FAILED Evicted:$EVICTED ImagePull:$IMAGEPULL CrashLoop:$CRASHLOOP Jobs:$JOBS RS:$REPLICASETS"
                  echo "TOTAL: $TOTAL"

                  # Push metrics
                  cat <<EOF | curl -s --data-binary @- "${PUSHGATEWAY_URL}/metrics/job/pod_cleanup/instance/talos00" && echo "Metrics pushed" || echo "Metrics push failed"
                  pod_cleanup_last_run_timestamp_seconds $END_TIME
                  pod_cleanup_duration_seconds $DURATION
                  pod_cleanup_dry_run $([ "$DRY_RUN" = "true" ] && echo 1 || echo 0)
                  pod_cleanup_resources_total $TOTAL
                  pod_cleanup_succeeded_pods $SUCCEEDED
                  pod_cleanup_failed_pods $FAILED
                  pod_cleanup_evicted_pods $EVICTED
                  pod_cleanup_imagepull_pods $IMAGEPULL
                  pod_cleanup_crashloop_pods $CRASHLOOP
                  pod_cleanup_completed_jobs $JOBS
                  pod_cleanup_orphan_replicasets $REPLICASETS
                  pod_cleanup_job_success 1
                  EOF
              resources:
                requests:
                  cpu: 50m
                  memory: 64Mi
                limits:
                  cpu: 200m
                  memory: 128Mi
