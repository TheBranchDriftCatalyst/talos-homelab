---
# LLM Scaler - Scale-to-zero proxy for AWS Ollama worker
# Automatically starts/stops EC2 instance based on request activity
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-proxy
  namespace: catalyst-llm
  labels:
    app.kubernetes.io/name: llm-proxy
    app.kubernetes.io/component: proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: llm-proxy
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llm-proxy
        app.kubernetes.io/component: proxy
    spec:
      serviceAccountName: llm-proxy
      containers:
        - name: scaler
          image: ghcr.io/thebranchdriftcatalyst/llm-proxy:latest
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 8080
            - name: metrics
              containerPort: 9090
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: LISTEN_ADDR
              value: ':8080'
            - name: METRICS_ADDR
              value: ':9090'
            # Primary: Local ollama on talos06 (fallback/always-on)
            - name: OLLAMA_URL
              value: 'http://ollama-local.catalyst-llm.svc.cluster.local:11434'
            # Reduced idle timeout to 15 mins to save costs
            - name: IDLE_TIMEOUT
              value: '15m'
            # Remote AWS instance (for bigger models when needed)
            - name: REMOTE_OLLAMA_URL
              value: 'http://10.42.2.1:11434'
            - name: WARMUP_TIMEOUT
              value: '5m'
            - name: WORKER_SCRIPT
              value: '/app/llm-worker.sh'
            - name: AWS_REGION
              value: 'us-west-2'
            - name: STATE_FILE
              value: '/app/.output/worker-state.json'
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: secret-access-key
            # RabbitMQ Configuration (for broker mode)
            - name: RABBITMQ_HOST
              valueFrom:
                configMapKeyRef:
                  name: catalyst-llm-config
                  key: RABBITMQ_HOST
            - name: RABBITMQ_PORT
              valueFrom:
                configMapKeyRef:
                  name: catalyst-llm-config
                  key: RABBITMQ_PORT
            - name: RABBITMQ_VHOST
              valueFrom:
                configMapKeyRef:
                  name: catalyst-llm-config
                  key: RABBITMQ_VHOST
            - name: RABBITMQ_USER
              valueFrom:
                configMapKeyRef:
                  name: catalyst-llm-config
                  key: RABBITMQ_USER
            - name: RABBITMQ_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: rabbitmq-credentials
                  key: password
                  optional: true
            - name: BROKER_MODE
              valueFrom:
                configMapKeyRef:
                  name: catalyst-llm-config
                  key: BROKER_MODE
            # Fleet API for dynamic worker discovery
            - name: FLEET_API_URL
              value: 'http://control-plane.ec2-agents.svc.cluster.local:8090'
          volumeMounts:
            - name: worker-state
              mountPath: /app/.output
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 5
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 2
            periodSeconds: 10
          resources:
            requests:
              cpu: 50m
              memory: 64Mi
      volumes:
        - name: worker-state
          persistentVolumeClaim:
            claimName: llm-proxy-state
---
apiVersion: v1
kind: Service
metadata:
  name: llm-proxy
  namespace: catalyst-llm
  labels:
    app.kubernetes.io/name: llm-proxy
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8080
      targetPort: http
    - name: metrics
      port: 9090
      targetPort: metrics
  selector:
    app.kubernetes.io/name: llm-proxy
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: llm-proxy
  namespace: catalyst-llm
---
# RBAC for IngressRoute discovery (dynamic tabs)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: llm-proxy
  namespace: catalyst-llm
rules:
  - apiGroups: ["traefik.io"]
    resources: ["ingressroutes"]
    verbs: ["list", "get", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: llm-proxy
  namespace: catalyst-llm
subjects:
  - kind: ServiceAccount
    name: llm-proxy
    namespace: catalyst-llm
roleRef:
  kind: Role
  name: llm-proxy
  apiGroup: rbac.authorization.k8s.io
---
# ServiceMonitor for Prometheus scraping
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: llm-proxy
  namespace: catalyst-llm
  labels:
    app.kubernetes.io/name: llm-proxy
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: llm-proxy
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
