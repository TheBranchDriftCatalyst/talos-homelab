---
# Ollama Model Configuration
# Edit this ConfigMap to manage which models are pulled to local Ollama
# After editing, restart the ollama deployment to trigger the init container
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-models
  namespace: catalyst-llm
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/part-of: catalyst-llm
data:
  # List of models to pull on startup (one per line)
  # Format: model_name:tag
  # Use # for comments
  models.txt: |
    # === Base Models ===
    llama3.2:latest
    mistral:latest
    qwen2.5-coder:7b

    # === Uncensored Models (Eric Hartford's Dolphin Series) ===
    # The gold standard for uncensored - high quality, compliant
    dolphin-llama3:8b
    dolphin-mistral
    dolphin-mixtral:8x7b

    # Classic uncensored models
    llama2-uncensored
    wizard-vicuna-uncensored:13b
    wizardlm-uncensored:13b

    # Nous Hermes - creativity focused, lower hallucination
    nous-hermes2
    hermes3:8b

    # === Abliterated Models (Refusal Mechanisms Removed) ===
    # These have safety alignment surgically removed via abliteration
    # See: huggingface.co/blog/mlabonne/abliteration

    # Mistral abliterated (fast, capable)
    huihui_ai/mistral-small-abliterated
    evolveon/mistral-7b-instruct-v0.3-abliterated

    # Llama 3.1 abliterated (mlabonne's technique)
    mlabonne/meta-llama-3.1-8b-instruct-abliterated

    # Qwen abliterated (strong coding + general)
    mlabonne/qwen2.5-14b-instruct-abliterated

    # === Large Models (for EC2 GPU worker) ===
    # Only pull these on high-VRAM systems
    # dolphin-llama3:70b
    # qwen2.5:72b-instruct-abliterated
