---
# Configuration for Catalyst LLM
apiVersion: v1
kind: ConfigMap
metadata:
  name: catalyst-llm-config
  namespace: catalyst-llm
  labels:
    app.kubernetes.io/name: catalyst-llm
    app.kubernetes.io/component: config
data:
  # Ollama endpoint (via Nebula mesh to AWS worker)
  OLLAMA_HOST: 'http://ollama-remote:11434'

  # AWS Worker Configuration
  AWS_WORKER_NEBULA_IP: '10.42.2.1'
  AWS_WORKER_REGION: 'us-west-2'

  # Nebula Mesh Configuration
  NEBULA_LIGHTHOUSE_IP: '10.42.0.1'
  NEBULA_HOMELAB_IP: '10.42.1.1'

  # Default model to use
  DEFAULT_MODEL: 'llama3.2'

  # API Configuration
  API_PORT: '8080'
  LOG_LEVEL: 'info'

  # RabbitMQ Configuration
  RABBITMQ_HOST: 'rabbitmq.catalyst-llm.svc.cluster.local'
  RABBITMQ_PORT: '5672'
  RABBITMQ_VHOST: 'llm'
  RABBITMQ_USER: 'llmproxy'
  # Exchanges
  RABBITMQ_INFERENCE_EXCHANGE: 'llm.inference'
  RABBITMQ_PRIORITY_EXCHANGE: 'llm.priority'
  RABBITMQ_WORKERS_EXCHANGE: 'llm.workers'
  # Enable broker mode (set to 'true' to use RabbitMQ instead of direct HTTP)
  BROKER_MODE: 'false'
