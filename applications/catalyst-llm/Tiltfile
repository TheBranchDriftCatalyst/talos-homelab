# Catalyst LLM Tiltfile
# Hybrid LLM infrastructure with local Ollama + EC2 scale-to-zero
#
# Can run standalone: cd applications/catalyst-llm && tilt up
# Or included from root Tiltfile
#
# Features:
#   - llm-scaler: Go reverse proxy with WebSocket control panel
#   - ollama: Local inference on talos06 (Intel Arc GPU)
#   - open-webui: ChatGPT-like web UI
#   - EC2 worker: Scale-to-zero remote inference

# Load extensions
load('ext://uibutton', 'cmd_button', 'location')
load('ext://k8s_attach', 'k8s_attach')

# Configuration
namespace = 'catalyst-llm'
LABEL_LLM = '7-catalyst-llm'
LABEL_SCALER = LABEL_LLM
LABEL_INFERENCE = LABEL_LLM
LABEL_UI = LABEL_LLM
LABEL_FLUX = LABEL_LLM
REGISTRY = 'ghcr.io/thebranchdriftcatalyst'

# Cluster context (same as root Tiltfile)
allow_k8s_contexts('admin@catalyst-cluster')

# ============================================
# Flux Management - Suspend during dev
# ============================================

# Suspend Flux on Tilt startup to prevent conflicts
local_resource(
    'flux-suspend',
    cmd='flux suspend kustomization catalyst-llm 2>/dev/null || echo "Flux not available or already suspended"',
    labels=[LABEL_FLUX],
    auto_init=True,
)

# Button to resume Flux (for when you're done with Tilt)
cmd_button(
    name='btn-flux-resume',
    resource='flux-suspend',
    argv=['sh', '-c', '''
        flux resume kustomization catalyst-llm && \
        flux reconcile kustomization catalyst-llm --with-source && \
        echo "Flux resumed and reconciled"
    '''],
    text='Resume Flux',
    icon_name='play_arrow'
)

cmd_button(
    name='btn-flux-status',
    resource='flux-suspend',
    argv=['sh', '-c', 'flux get kustomization catalyst-llm'],
    text='Flux Status',
    icon_name='info'
)

# ============================================
# Service Links (IngressRoutes)
# ============================================

# All services accessible via Traefik ingress
local_resource(
    'service-links',
    cmd='echo "All catalyst-llm services available via ingress"',
    labels=['0-links'],
    auto_init=False,
    links=[
        link('http://chat.talos00', 'Open WebUI (Chat)'),
        link('http://llm-scaler.talos00/_/ui', 'LLM Scaler Dashboard'),
        link('http://llm.talos00', 'LLM API'),
        link('http://ollama.talos00', 'Ollama API'),
        link('http://sillytavern.talos00', 'SillyTavern'),
        link('http://searxng.talos00', 'SearXNG Search'),
    ]
)

# ============================================
# LLM Scaler - Go Reverse Proxy
# ============================================

# Build llm-scaler with live update
docker_build(
    REGISTRY + '/llm-scaler',
    context='./llm-scaler',
    dockerfile='./llm-scaler/Dockerfile',
    only=[
        'go.mod',
        'go.sum',
        'main.go',
        'scaler.go',
        'ui.go',
        'websocket.go',
        'scripts/',
    ],
    live_update=[
        # Sync steps must come before run steps
        sync('./llm-scaler/main.go', '/build/main.go'),
        sync('./llm-scaler/scaler.go', '/build/scaler.go'),
        sync('./llm-scaler/ui.go', '/build/ui.go'),
        sync('./llm-scaler/websocket.go', '/build/websocket.go'),
        sync('./llm-scaler/scripts/llm-worker.sh', '/app/llm-worker.sh'),
        # Rebuild binary in container
        run('cd /build && CGO_ENABLED=0 go build -o /app/llm-scaler . && kill -HUP 1'),
    ]
)

# Apply llm-scaler deployment only (not full kustomization to avoid conflicts with Flux)
k8s_yaml(['scaler-deployment.yaml', 'service.yaml'])
watch_file('scaler-deployment.yaml')
watch_file('service.yaml')

k8s_resource(
    'llm-scaler',
    port_forwards=[
        '8080:8080',   # Proxy + UI
        '9090:9090',   # Metrics
    ],
    labels=[LABEL_SCALER],
    links=[
        link('http://localhost:8080/_/ui', 'Control Panel'),
        link('http://localhost:9090/metrics', 'Metrics'),
        link('http://llm-scaler.talos00/_/ui', 'Control Panel (Ingress)'),
    ]
)

# ============================================
# Control Buttons
# ============================================

cmd_button(
    name='btn-ec2-status',
    resource='llm-scaler',
    argv=['sh', '-c', '''
        kubectl exec -n catalyst-llm deploy/llm-scaler -- /app/llm-worker.sh status
    '''],
    text='EC2 Status',
    icon_name='info'
)

cmd_button(
    name='btn-ec2-start',
    resource='llm-scaler',
    argv=['sh', '-c', '''
        echo "Starting EC2 worker..." && \
        kubectl exec -n catalyst-llm deploy/llm-scaler -- /app/llm-worker.sh start && \
        echo "EC2 worker started"
    '''],
    text='Start EC2',
    icon_name='play_arrow'
)

cmd_button(
    name='btn-ec2-stop',
    resource='llm-scaler',
    argv=['sh', '-c', '''
        echo "Stopping EC2 worker..." && \
        kubectl exec -n catalyst-llm deploy/llm-scaler -- /app/llm-worker.sh stop && \
        echo "EC2 worker stopped"
    '''],
    text='Stop EC2',
    icon_name='stop'
)

cmd_button(
    name='btn-ec2-warm',
    resource='llm-scaler',
    argv=['sh', '-c', '''
        echo "Warming EC2 worker (start + wait for Ollama ready)..." && \
        kubectl exec -n catalyst-llm deploy/llm-scaler -- /app/llm-worker.sh warm && \
        echo "EC2 worker ready"
    '''],
    text='Warm EC2',
    icon_name='whatshot'
)

cmd_button(
    name='btn-local-models',
    resource='llm-scaler',
    argv=['sh', '-c', '''
        echo "=== Local Ollama Models ===" && \
        kubectl exec -n catalyst-llm deploy/ollama -- ollama list
    '''],
    text='List Models',
    icon_name='list'
)

# ============================================
# Inference Backends (Flux-managed, observe only)
# ============================================

k8s_attach('ollama', 'deployment/ollama', namespace=namespace,
           port_forwards=['11434:11434'], labels=[LABEL_INFERENCE],
           links=[
               link('http://ollama.talos00', 'Ollama (Ingress)'),
               link('http://localhost:11434', 'Ollama (Local)'),
           ])

# ============================================
# Web UIs (Flux-managed, observe only)
# ============================================

k8s_attach('open-webui', 'deployment/open-webui', namespace=namespace,
           port_forwards=['3000:8080'], labels=[LABEL_UI],
           links=[
               link('http://chat.talos00', 'Open WebUI (Ingress)'),
               link('http://localhost:3000', 'Open WebUI (Local)'),
           ])

k8s_attach('sillytavern', 'deployment/sillytavern', namespace=namespace,
           port_forwards=['8000:8000'], labels=[LABEL_UI],
           links=[
               link('http://sillytavern.talos00', 'SillyTavern (Ingress)'),
               link('http://localhost:8000', 'SillyTavern (Local)'),
           ])

# Ollama utility buttons
cmd_button(
    name='btn-ollama-list',
    resource='ollama',
    argv=['sh', '-c', 'kubectl exec -n catalyst-llm deploy/ollama -- ollama list'],
    text='List Models',
    icon_name='list'
)

cmd_button(
    name='btn-ollama-ps',
    resource='ollama',
    argv=['sh', '-c', 'kubectl exec -n catalyst-llm deploy/ollama -- ollama ps'],
    text='Running Models',
    icon_name='memory'
)

# Model pull buttons - common models
cmd_button(
    name='btn-pull-llama3.2',
    resource='ollama',
    argv=['sh', '-c', 'kubectl exec -n catalyst-llm deploy/ollama -- ollama pull llama3.2:latest'],
    text='Pull llama3.2',
    icon_name='download'
)

cmd_button(
    name='btn-pull-llama3.2-3b',
    resource='ollama',
    argv=['sh', '-c', 'kubectl exec -n catalyst-llm deploy/ollama -- ollama pull llama3.2:3b'],
    text='Pull llama3.2:3b',
    icon_name='download'
)

cmd_button(
    name='btn-pull-qwen2.5-coder',
    resource='ollama',
    argv=['sh', '-c', 'kubectl exec -n catalyst-llm deploy/ollama -- ollama pull qwen2.5-coder:7b'],
    text='Pull qwen2.5-coder',
    icon_name='download'
)

cmd_button(
    name='btn-pull-deepseek-coder',
    resource='ollama',
    argv=['sh', '-c', 'kubectl exec -n catalyst-llm deploy/ollama -- ollama pull deepseek-coder-v2:latest'],
    text='Pull deepseek-coder',
    icon_name='download'
)

cmd_button(
    name='btn-pull-mistral',
    resource='ollama',
    argv=['sh', '-c', 'kubectl exec -n catalyst-llm deploy/ollama -- ollama pull mistral:latest'],
    text='Pull mistral',
    icon_name='download'
)

cmd_button(
    name='btn-pull-codellama',
    resource='ollama',
    argv=['sh', '-c', 'kubectl exec -n catalyst-llm deploy/ollama -- ollama pull codellama:latest'],
    text='Pull codellama',
    icon_name='download'
)

cmd_button(
    name='btn-pull-phi3',
    resource='ollama',
    argv=['sh', '-c', 'kubectl exec -n catalyst-llm deploy/ollama -- ollama pull phi3:latest'],
    text='Pull phi3',
    icon_name='download'
)

cmd_button(
    name='btn-ollama-rm',
    resource='ollama',
    argv=['sh', '-c', '''
        echo "=== Current Models ===" && \
        kubectl exec -n catalyst-llm deploy/ollama -- ollama list && \
        echo "" && \
        echo "To remove a model, run:" && \
        echo "kubectl exec -n catalyst-llm deploy/ollama -- ollama rm <model>"
    '''],
    text='Remove Model Help',
    icon_name='delete'
)

# ============================================
# Dev utilities
# ============================================

local_resource(
    'dashboard',
    cmd='./dashboard.sh --plain 2>/dev/null || echo "Dashboard script not found"',
    deps=['dashboard.sh'],
    labels=[LABEL_SCALER],
    auto_init=False,
)

cmd_button(
    name='btn-rebuild-push',
    resource='llm-scaler',
    argv=['sh', '-c', '''
        cd applications/catalyst-llm/llm-scaler && \
        docker buildx build --platform linux/amd64,linux/arm64 \
            -t ghcr.io/thebranchdriftcatalyst/llm-scaler:latest --push . && \
        kubectl rollout restart deploy/llm-scaler -n catalyst-llm && \
        kubectl rollout status deploy/llm-scaler -n catalyst-llm --timeout=60s && \
        echo "Rebuild and deploy complete"
    '''],
    text='Rebuild & Push',
    icon_name='build'
)
