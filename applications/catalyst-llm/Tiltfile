# Catalyst LLM Tiltfile
# Hybrid LLM infrastructure with local Ollama + EC2 scale-to-zero
#
# Can run standalone: cd applications/catalyst-llm && tilt up
# Or included from root Tiltfile
#
# Features:
#   - llm-proxy: Go reverse proxy with WebSocket control panel
#   - ollama: Local inference on talos06 (Intel Arc GPU)
#   - open-webui: ChatGPT-like web UI
#   - EC2 worker: Scale-to-zero remote inference

# Load extensions
load('ext://uibutton', 'cmd_button', 'location', 'text_input')
load('ext://k8s_attach', 'k8s_attach')

# Configuration
namespace = 'catalyst-llm'
LABEL_LLM = '7-catalyst-llm'
LABEL_SCALER = LABEL_LLM
LABEL_INFERENCE = LABEL_LLM
LABEL_UI = LABEL_LLM
LABEL_FLUX = LABEL_LLM
LABEL_LOCAL = '8-local-dev'
REGISTRY = 'ghcr.io/thebranchdriftcatalyst'

# Get Mac's local IP for cluster access (used for Mac dev endpoint)
MAC_IP = str(local('ipconfig getifaddr en0 2>/dev/null || ipconfig getifaddr en1 2>/dev/null || echo "localhost"', quiet=True)).strip()

# Cluster context (same as root Tiltfile)
allow_k8s_contexts('admin@catalyst-cluster')

# ============================================
# Flux Management - Suspend during dev
# ============================================

# Suspend Flux on Tilt startup to prevent conflicts
local_resource(
    'flux-suspend',
    cmd='flux suspend kustomization catalyst-llm 2>/dev/null || echo "Flux not available or already suspended"',
    labels=[LABEL_FLUX],
    auto_init=True,
)

# Button to resume Flux (for when you're done with Tilt)
cmd_button(
    name='btn-flux-resume',
    resource='flux-suspend',
    argv=['sh', '-c', '''
        flux resume kustomization catalyst-llm && \
        flux reconcile kustomization catalyst-llm --with-source && \
        echo "Flux resumed and reconciled"
    '''],
    text='Resume Flux',
    icon_name='play_arrow'
)

cmd_button(
    name='btn-flux-status',
    resource='flux-suspend',
    argv=['sh', '-c', 'flux get kustomization catalyst-llm'],
    text='Flux Status',
    icon_name='info'
)

# ============================================
# Service Links (IngressRoutes)
# ============================================

# All services accessible via Traefik ingress
local_resource(
    'service-links',
    cmd='echo "All catalyst-llm services available via ingress"',
    labels=['0-links'],
    auto_init=False,
    links=[
        link('http://chat.talos00', 'Open WebUI (Chat)'),
        link('http://llm-proxy.talos00/_/ui', 'LLM Scaler Dashboard'),
        link('http://llm.talos00', 'LLM API'),
        link('http://ollama.talos00', 'Ollama API'),
        link('http://sillytavern.talos00', 'SillyTavern'),
        link('http://searxng.talos00', 'SearXNG Search'),
    ]
)

# ============================================
# LLM Scaler - Go Reverse Proxy
# ============================================

# Build llm-proxy with live update
docker_build(
    REGISTRY + '/llm-proxy',
    context='./llm-proxy',
    dockerfile='./llm-proxy/Dockerfile',
    only=[
        'go.mod',
        'go.sum',
        'main.go',
        'scaler.go',
        'broker.go',
        'worker.go',
        'ui.go',
        'websocket.go',
        'scripts/',
        'ui/',  # Include React UI for production builds
    ],
    live_update=[
        # Sync steps must come before run steps
        sync('./llm-proxy/main.go', '/build/main.go'),
        sync('./llm-proxy/scaler.go', '/build/scaler.go'),
        sync('./llm-proxy/broker.go', '/build/broker.go'),
        sync('./llm-proxy/worker.go', '/build/worker.go'),
        sync('./llm-proxy/ui.go', '/build/ui.go'),
        sync('./llm-proxy/websocket.go', '/build/websocket.go'),
        sync('./llm-proxy/scripts/llm-worker.sh', '/app/llm-worker.sh'),
        # Rebuild binary in container
        run('cd /build && CGO_ENABLED=0 go build -o /app/llm-proxy . && kill -HUP 1'),
    ]
)

# ============================================
# React UI Development (optional - run locally with Vite)
# ============================================

# Local Vite dev server for React UI hot-reload
# This proxies WebSocket to the Go backend running in cluster
local_resource(
    'llm-proxy-ui-dev',
    serve_cmd='cd llm-proxy/ui && yarn install && yarn dev',
    serve_dir='llm-proxy/ui',
    deps=['llm-proxy/ui/src', 'llm-proxy/ui/package.json'],
    readiness_probe=probe(
        http_get=http_get_action(port=5173, path='/'),
        initial_delay_secs=5,
        period_secs=3,
    ),
    labels=[LABEL_LOCAL],
    links=[
        link('http://localhost:5173', 'React UI (Dev Server)'),
    ],
    resource_deps=['llm-proxy'],  # Needs backend for WebSocket
    auto_init=False,  # Manual start - run only when developing UI
)

cmd_button(
    name='btn-ui-install',
    resource='llm-proxy-ui-dev',
    argv=['sh', '-c', 'cd llm-proxy/ui && yarn install && echo "Dependencies installed"'],
    text='Install Deps',
    icon_name='download'
)

cmd_button(
    name='btn-ui-build',
    resource='llm-proxy-ui-dev',
    argv=['sh', '-c', 'cd llm-proxy/ui && yarn build && echo "UI built to ui/dist"'],
    text='Build UI',
    icon_name='build'
)

# Apply llm-proxy deployment only (not full kustomization to avoid conflicts with Flux)
# Note: We patch the deployment to add MAC_OLLAMA_URL for dev mode
k8s_yaml(['proxy-deployment.yaml', 'service.yaml'])
watch_file('proxy-deployment.yaml')
watch_file('service.yaml')

# Inject MAC_OLLAMA_URL environment variable for dev mode
# This enables the "Mac" routing option in the UI
k8s_resource(
    'llm-proxy',
    objects=['llm-proxy:serviceaccount', 'llm-proxy:servicemonitor'],
    port_forwards=[
        '8080:8080',   # Proxy + UI
        '9090:9090',   # Metrics
    ],
    labels=[LABEL_SCALER],
    links=[
        link('http://localhost:8080/_/ui', 'Control Panel'),
        link('http://localhost:9090/metrics', 'Metrics'),
        link('http://llm-proxy.talos00/_/ui', 'Control Panel (Ingress)'),
    ]
)

# Patch deployment to add MAC_OLLAMA_URL env var
local_resource(
    'patch-mac-env',
    cmd='''
        kubectl patch deployment llm-proxy -n catalyst-llm --type=json -p='[
          {"op": "add", "path": "/spec/template/spec/containers/0/env/-", "value": {"name": "MAC_OLLAMA_URL", "value": "http://''' + MAC_IP + ''':11434"}}
        ]' 2>/dev/null || echo "Patch applied or deployment not ready"
    ''',
    resource_deps=['llm-proxy'],
    labels=[LABEL_SCALER],
    auto_init=True,
)


# ============================================
# Control Buttons
# ============================================

cmd_button(
    name='btn-ec2-status',
    resource='llm-proxy',
    argv=['sh', '-c', '''
        kubectl exec -n catalyst-llm deploy/llm-proxy -- /app/llm-worker.sh status
    '''],
    text='EC2 Status',
    icon_name='info'
)

cmd_button(
    name='btn-ec2-start',
    resource='llm-proxy',
    argv=['sh', '-c', '''
        echo "Starting EC2 worker..." && \
        kubectl exec -n catalyst-llm deploy/llm-proxy -- /app/llm-worker.sh start && \
        echo "EC2 worker started"
    '''],
    text='Start EC2',
    icon_name='play_arrow'
)

cmd_button(
    name='btn-ec2-stop',
    resource='llm-proxy',
    argv=['sh', '-c', '''
        echo "Stopping EC2 worker..." && \
        kubectl exec -n catalyst-llm deploy/llm-proxy -- /app/llm-worker.sh stop && \
        echo "EC2 worker stopped"
    '''],
    text='Stop EC2',
    icon_name='stop'
)

cmd_button(
    name='btn-ec2-warm',
    resource='llm-proxy',
    argv=['sh', '-c', '''
        echo "Warming EC2 worker (start + wait for Ollama ready)..." && \
        kubectl exec -n catalyst-llm deploy/llm-proxy -- /app/llm-worker.sh warm && \
        echo "EC2 worker ready"
    '''],
    text='Warm EC2',
    icon_name='whatshot'
)

cmd_button(
    name='btn-local-models',
    resource='llm-proxy',
    argv=['sh', '-c', '''
        echo "=== Local Ollama Models ===" && \
        kubectl exec -n catalyst-llm deploy/ollama -- ollama list
    '''],
    text='List Models',
    icon_name='list'
)

# ============================================
# Inference Backends (Flux-managed, observe only)
# ============================================

k8s_attach('ollama', 'deployment/ollama', namespace=namespace,
           port_forwards=['11434:11434'], labels=[LABEL_INFERENCE],
           links=[
               link('http://ollama.talos00', 'Ollama (Ingress)'),
               link('http://localhost:11434', 'Ollama (Local)'),
           ])

# ============================================
# RabbitMQ Message Broker
# ============================================

k8s_attach('rabbitmq', 'statefulset/rabbitmq', namespace=namespace,
           port_forwards=[
               '5672:5672',    # AMQP
               '15672:15672',  # Management UI
               '15692:15692',  # Prometheus metrics
           ],
           labels=[LABEL_LLM],
           links=[
               link('http://localhost:15672', 'RabbitMQ Management'),
           ])

cmd_button(
    name='btn-rabbitmq-status',
    resource='rabbitmq',
    argv=['sh', '-c', '''
        kubectl exec -n catalyst-llm rabbitmq-0 -- rabbitmqctl status | head -30
    '''],
    text='Status',
    icon_name='info'
)

cmd_button(
    name='btn-rabbitmq-queues',
    resource='rabbitmq',
    argv=['sh', '-c', '''
        echo "=== Queues ===" && \
        kubectl exec -n catalyst-llm rabbitmq-0 -- rabbitmqctl list_queues --vhost llm name messages consumers
    '''],
    text='List Queues',
    icon_name='list'
)

cmd_button(
    name='btn-rabbitmq-bindings',
    resource='rabbitmq',
    argv=['sh', '-c', '''
        echo "=== Bindings ===" && \
        kubectl exec -n catalyst-llm rabbitmq-0 -- rabbitmqctl list_bindings --vhost llm source_name destination_name routing_key
    '''],
    text='List Bindings',
    icon_name='link'
)

# ============================================
# Web UIs (Flux-managed, observe only)
# ============================================

k8s_attach('open-webui', 'deployment/open-webui', namespace=namespace,
           port_forwards=['3030:8080'], labels=[LABEL_UI],
           links=[
               link('http://chat.talos00', 'Open WebUI (Ingress)'),
               link('http://localhost:3030', 'Open WebUI (Local)'),
           ])

k8s_attach('sillytavern', 'deployment/sillytavern', namespace=namespace,
           port_forwards=['8088:8000'], labels=[LABEL_UI],
           links=[
               link('http://sillytavern.talos00', 'SillyTavern (Ingress)'),
               link('http://localhost:8088', 'SillyTavern (Local)'),
           ])

# Ollama utility buttons
cmd_button(
    name='btn-ollama-list',
    resource='ollama',
    argv=['sh', '-c', 'kubectl exec -n catalyst-llm deploy/ollama -- ollama list'],
    text='List Models',
    icon_name='list'
)

cmd_button(
    name='btn-ollama-ps',
    resource='ollama',
    argv=['sh', '-c', 'kubectl exec -n catalyst-llm deploy/ollama -- ollama ps'],
    text='Running Models',
    icon_name='memory'
)

# Model pull with text input - type any model name (e.g., llama3.2, qwen2.5-coder:7b)
cmd_button(
    name='btn-pull-model',
    resource='ollama',
    argv=['sh', '-c', '''
        if [ -z "$MODEL_NAME" ]; then
            echo "Error: Please enter a model name"
            exit 1
        fi
        echo "Pulling model: $MODEL_NAME" && \
        kubectl exec -n catalyst-llm deploy/ollama -- ollama pull "$MODEL_NAME" && \
        echo "Model $MODEL_NAME pulled successfully"
    '''],
    text='Pull Model',
    icon_name='download',
    inputs=[
        text_input('MODEL_NAME', 'Model name (e.g., llama3.2, qwen2.5-coder:7b, mistral)')
    ]
)

# Delete model with text input
cmd_button(
    name='btn-delete-model',
    resource='ollama',
    argv=['sh', '-c', '''
        if [ -z "$MODEL_NAME" ]; then
            echo "Error: Please enter a model name"
            exit 1
        fi
        echo "Deleting model: $MODEL_NAME" && \
        kubectl exec -n catalyst-llm deploy/ollama -- ollama rm "$MODEL_NAME" && \
        echo "Model $MODEL_NAME deleted"
    '''],
    text='Delete Model',
    icon_name='delete',
    inputs=[
        text_input('MODEL_NAME', 'Model to delete')
    ]
)

# ============================================
# Local Mac Ollama (dev only - runs on host machine)
# ============================================

# Default models to pull (same as init container in ollama-talos06.yaml)
DEFAULT_MODELS = ['llama3.2:latest', 'qwen2.5-coder:7b', 'mistral:latest']

# Local Ollama management - ensures ollama is running on Mac
local_resource(
    'ollama-mac',
    serve_cmd='OLLAMA_HOST=0.0.0.0:11434 ollama serve 2>&1 || echo "Ollama already running"',
    readiness_probe=probe(
        http_get=http_get_action(port=11434, path='/'),
        initial_delay_secs=2,
        period_secs=5,
    ),
    labels=[LABEL_LOCAL],
    links=[
        link('http://localhost:11434', 'Mac Ollama API'),
    ],
    allow_parallel=True,
)

# Auto-pull default models (runs once after ollama-mac is ready)
local_resource(
    'ollama-mac-models',
    cmd='''
        echo "=== Pulling default models for Mac Ollama ==="
        for model in ''' + ' '.join(DEFAULT_MODELS) + '''; do
            echo "Checking $model..."
            if ollama list | grep -q "^$model"; then
                echo "  $model already present"
            else
                echo "  Pulling $model..."
                ollama pull "$model" || echo "  Warning: Failed to pull $model"
            fi
        done
        echo "=== Model sync complete ==="
        ollama list
    ''',
    resource_deps=['ollama-mac'],
    labels=[LABEL_LOCAL],
)

# Button to list models on Mac
cmd_button(
    name='btn-mac-ollama-list',
    resource='ollama-mac',
    argv=['ollama', 'list'],
    text='List Models',
    icon_name='list'
)

# Button to pull model on Mac
cmd_button(
    name='btn-mac-pull-model',
    resource='ollama-mac',
    argv=['sh', '-c', '''
        if [ -z "$MODEL_NAME" ]; then
            echo "Error: Please enter a model name"
            exit 1
        fi
        echo "Pulling model on Mac: $MODEL_NAME" && \
        ollama pull "$MODEL_NAME" && \
        echo "Model $MODEL_NAME pulled successfully"
    '''],
    text='Pull Model',
    icon_name='download',
    inputs=[
        text_input('MODEL_NAME', 'Model name (e.g., llama3.2, qwen2.5-coder:7b)')
    ]
)

# Button to show Mac Ollama connection info
cmd_button(
    name='btn-mac-ollama-info',
    resource='ollama-mac',
    argv=['sh', '-c', '''
        echo "Mac Ollama Endpoint Info"
        echo "========================"
        echo ""
        echo "Mac IP: ''' + MAC_IP + '''"
        echo "Ollama URL: http://''' + MAC_IP + ''':11434"
        echo ""
        echo "To use from cluster, set llm-proxy routing to:"
        echo "  curl -X POST http://localhost:8080/_/api/routing -d 'mode=local'"
        echo ""
        echo "Note: Mac must be on same network as Talos cluster"
    '''],
    text='Connection Info',
    icon_name='info'
)

# ============================================
# Dev utilities
# ============================================

local_resource(
    'dashboard',
    cmd='./dashboard.sh --plain 2>/dev/null || echo "Dashboard script not found"',
    deps=['dashboard.sh'],
    labels=[LABEL_SCALER],
    auto_init=False,
)

cmd_button(
    name='btn-rebuild-push',
    resource='llm-proxy',
    argv=['sh', '-c', '''
        cd applications/catalyst-llm/llm-proxy && \
        docker buildx build --platform linux/amd64,linux/arm64 \
            -t ghcr.io/thebranchdriftcatalyst/llm-proxy:latest --push . && \
        kubectl rollout restart deploy/llm-proxy -n catalyst-llm && \
        kubectl rollout status deploy/llm-proxy -n catalyst-llm --timeout=60s && \
        echo "Rebuild and deploy complete"
    '''],
    text='Rebuild & Push',
    icon_name='build'
)
