{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NER Training Data Preparation\n",
        "\n",
        "Combine datasets from all domains and prepare for NER model training.\n",
        "\n",
        "**Goals:**\n",
        "- Unify document formats across domains\n",
        "- Create BIO/BILOU annotations\n",
        "- Export to spaCy and HuggingFace formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.insert(0, str(Path('../corpus-core/src').resolve()))\n",
        "sys.path.insert(0, str(Path('../pipelines/src').resolve()))\n",
        "\n",
        "from corpus_core.loaders import ParquetLoader\n",
        "from corpus_core.models import Document\n",
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load All Domain Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = ParquetLoader(Path('../datasets'))\n",
        "\n",
        "# Collect all documents\n",
        "all_documents = []\n",
        "\n",
        "# Congress documents\n",
        "if loader.exists('congress', 'congress_documents'):\n",
        "    congress_docs = loader.read('congress', 'congress_documents').to_pylist()\n",
        "    all_documents.extend(congress_docs)\n",
        "    print(f\"Congress documents: {len(congress_docs)}\")\n",
        "\n",
        "# EDGAR documents  \n",
        "if loader.exists('edgar', 'edgar_documents'):\n",
        "    edgar_docs = loader.read('edgar', 'edgar_documents').to_pylist()\n",
        "    all_documents.extend(edgar_docs)\n",
        "    print(f\"EDGAR documents: {len(edgar_docs)}\")\n",
        "\n",
        "# Reddit documents\n",
        "if loader.exists('reddit', 'reddit_documents'):\n",
        "    reddit_docs = loader.read('reddit', 'reddit_documents').to_pylist()\n",
        "    all_documents.extend(reddit_docs)\n",
        "    print(f\"Reddit documents: {len(reddit_docs)}\")\n",
        "\n",
        "print(f\"\\nTotal documents: {len(all_documents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrame for analysis\n",
        "if all_documents:\n",
        "    docs_df = pd.DataFrame(all_documents)\n",
        "    print(\"\\nDocument distribution by domain:\")\n",
        "    print(docs_df['domain'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_training_example(doc: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Prepare a document for NER training.\n",
        "    \n",
        "    Returns a dict with:\n",
        "    - id: Document ID\n",
        "    - text: Full text content\n",
        "    - domain: Source domain\n",
        "    - metadata: Additional context\n",
        "    \"\"\"\n",
        "    # Combine title and content\n",
        "    text = doc.get('title', '')\n",
        "    content = doc.get('content', '')\n",
        "    if content:\n",
        "        text = f\"{text}\\n\\n{content}\"\n",
        "    \n",
        "    # Add sections if present\n",
        "    sections = doc.get('sections', {})\n",
        "    if sections:\n",
        "        for section_name, section_text in sections.items():\n",
        "            if section_text:\n",
        "                text += f\"\\n\\n[{section_name}]\\n{section_text}\"\n",
        "    \n",
        "    return {\n",
        "        'id': doc.get('id', ''),\n",
        "        'text': text[:50000],  # Cap at 50k chars\n",
        "        'domain': doc.get('domain', 'unknown'),\n",
        "        'document_type': doc.get('document_type', ''),\n",
        "        'source': doc.get('source', ''),\n",
        "    }\n",
        "\n",
        "# Prepare all examples\n",
        "if all_documents:\n",
        "    training_examples = [prepare_training_example(doc) for doc in all_documents]\n",
        "    print(f\"Prepared {len(training_examples)} training examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample\n",
        "if training_examples:\n",
        "    sample = training_examples[0]\n",
        "    print(f\"ID: {sample['id']}\")\n",
        "    print(f\"Domain: {sample['domain']}\")\n",
        "    print(f\"Type: {sample['document_type']}\")\n",
        "    print(f\"Text length: {len(sample['text'])} chars\")\n",
        "    print(\"-\" * 50)\n",
        "    print(sample['text'][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate NER Annotations (Auto-labeling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import spacy\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    \n",
        "    def auto_annotate(text: str, max_length: int = 5000):\n",
        "        \"\"\"\n",
        "        Auto-annotate text using spaCy.\n",
        "        \n",
        "        Returns list of (start, end, label) tuples.\n",
        "        \"\"\"\n",
        "        doc = nlp(text[:max_length])\n",
        "        return [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
        "    \n",
        "    # Test on sample\n",
        "    if training_examples:\n",
        "        sample_text = training_examples[0]['text']\n",
        "        annotations = auto_annotate(sample_text)\n",
        "        \n",
        "        print(f\"Found {len(annotations)} entities in sample:\")\n",
        "        for start, end, label in annotations[:10]:\n",
        "            print(f\"  {label:10} | {sample_text[start:end][:50]}\")\n",
        "            \n",
        "except ImportError:\n",
        "    print(\"spaCy not installed. Run: pip install spacy && python -m spacy download en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export to spaCy Training Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_spacy_format(text: str, annotations: list) -> tuple:\n",
        "    \"\"\"\n",
        "    Convert to spaCy training format.\n",
        "    \n",
        "    Returns (text, {\"entities\": [(start, end, label), ...]})\n",
        "    \"\"\"\n",
        "    return (text, {\"entities\": annotations})\n",
        "\n",
        "# Create spaCy training data\n",
        "if training_examples and 'nlp' in dir():\n",
        "    spacy_training_data = []\n",
        "    \n",
        "    for example in training_examples[:100]:  # Sample for demo\n",
        "        text = example['text']\n",
        "        annotations = auto_annotate(text, max_length=2000)\n",
        "        spacy_training_data.append(to_spacy_format(text[:2000], annotations))\n",
        "    \n",
        "    print(f\"Created {len(spacy_training_data)} spaCy training examples\")\n",
        "    \n",
        "    # Save to file\n",
        "    output_path = Path('../datasets/training/spacy_train.json')\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(spacy_training_data, f)\n",
        "    print(f\"Saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export to HuggingFace Format (IOB2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_iob2_format(text: str, annotations: list) -> list:\n",
        "    \"\"\"\n",
        "    Convert to IOB2 token format for HuggingFace.\n",
        "    \n",
        "    Returns list of {\"tokens\": [...], \"ner_tags\": [...]} dicts.\n",
        "    \"\"\"\n",
        "    if 'nlp' not in dir():\n",
        "        return []\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    ner_tags = ['O'] * len(tokens)\n",
        "    \n",
        "    # Map annotations to tokens\n",
        "    for start, end, label in annotations:\n",
        "        for i, token in enumerate(doc):\n",
        "            if token.idx >= start and token.idx + len(token) <= end:\n",
        "                if token.idx == start:\n",
        "                    ner_tags[i] = f'B-{label}'\n",
        "                else:\n",
        "                    ner_tags[i] = f'I-{label}'\n",
        "    \n",
        "    return {\"tokens\": tokens, \"ner_tags\": ner_tags}\n",
        "\n",
        "# Test\n",
        "if training_examples and 'nlp' in dir():\n",
        "    sample_text = training_examples[0]['text'][:500]\n",
        "    sample_annotations = auto_annotate(sample_text, max_length=500)\n",
        "    iob_example = to_iob2_format(sample_text, sample_annotations)\n",
        "    \n",
        "    print(\"Sample IOB2 output:\")\n",
        "    for token, tag in list(zip(iob_example['tokens'], iob_example['ner_tags']))[:20]:\n",
        "        if tag != 'O':\n",
        "            print(f\"  {token:20} -> {tag}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create HuggingFace dataset\n",
        "if training_examples and 'nlp' in dir():\n",
        "    hf_training_data = []\n",
        "    \n",
        "    for example in training_examples[:100]:\n",
        "        text = example['text'][:1000]\n",
        "        annotations = auto_annotate(text, max_length=1000)\n",
        "        iob_data = to_iob2_format(text, annotations)\n",
        "        iob_data['id'] = example['id']\n",
        "        iob_data['domain'] = example['domain']\n",
        "        hf_training_data.append(iob_data)\n",
        "    \n",
        "    # Save to JSONL\n",
        "    output_path = Path('../datasets/training/ner_train.jsonl')\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(output_path, 'w') as f:\n",
        "        for item in hf_training_data:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "    print(f\"Saved {len(hf_training_data)} examples to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if training_examples:\n",
        "    # Overall stats\n",
        "    total_chars = sum(len(ex['text']) for ex in training_examples)\n",
        "    total_words = sum(len(ex['text'].split()) for ex in training_examples)\n",
        "    \n",
        "    print(\"Dataset Statistics:\")\n",
        "    print(f\"  Total documents: {len(training_examples):,}\")\n",
        "    print(f\"  Total characters: {total_chars:,}\")\n",
        "    print(f\"  Total words: {total_words:,}\")\n",
        "    print(f\"  Avg chars/doc: {total_chars // len(training_examples):,}\")\n",
        "    print(f\"  Avg words/doc: {total_words // len(training_examples):,}\")\n",
        "    \n",
        "    # By domain\n",
        "    print(\"\\nBy Domain:\")\n",
        "    domain_stats = {}\n",
        "    for ex in training_examples:\n",
        "        domain = ex['domain']\n",
        "        if domain not in domain_stats:\n",
        "            domain_stats[domain] = {'count': 0, 'chars': 0}\n",
        "        domain_stats[domain]['count'] += 1\n",
        "        domain_stats[domain]['chars'] += len(ex['text'])\n",
        "    \n",
        "    for domain, stats in domain_stats.items():\n",
        "        print(f\"  {domain}: {stats['count']} docs, {stats['chars']:,} chars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Manual Annotation**: Use Prodigy or Label Studio for correction\n",
        "2. **Train Custom Model**: Fine-tune transformer on domain data\n",
        "3. **Evaluate**: Compare against baseline spaCy model\n",
        "4. **Iterate**: Add more data sources, refine entity types"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
