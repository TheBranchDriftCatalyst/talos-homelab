{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reddit Data Exploration\n",
        "\n",
        "Explore the Reddit/Pushshift domain data for NER training.\n",
        "\n",
        "**Goals:**\n",
        "- Load and inspect submissions and comments\n",
        "- Analyze discourse patterns by subreddit\n",
        "- Test entity extraction on political/news content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.insert(0, str(Path('../corpus-core/src').resolve()))\n",
        "sys.path.insert(0, str(Path('../pipelines/src').resolve()))\n",
        "\n",
        "from corpus_core.loaders import ParquetLoader\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Reddit Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = ParquetLoader(Path('../datasets'))\n",
        "\n",
        "# List Reddit datasets\n",
        "reddit_datasets = [ds for ds in loader.list_datasets() if ds['domain'] == 'reddit']\n",
        "print(\"Reddit datasets:\")\n",
        "for ds in reddit_datasets:\n",
        "    print(f\"  - {ds['name']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load submissions\n",
        "if loader.exists('reddit', 'reddit_submissions'):\n",
        "    submissions_df = loader.read_pandas('reddit', 'reddit_submissions')\n",
        "    print(f\"Submissions: {len(submissions_df)} records\")\n",
        "    display(submissions_df[['subreddit', 'title', 'score', 'num_comments']].head(10))\n",
        "else:\n",
        "    print(\"Run the Dagster pipeline first: dagster dev\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load comments\n",
        "if loader.exists('reddit', 'reddit_comments'):\n",
        "    comments_df = loader.read_pandas('reddit', 'reddit_comments')\n",
        "    print(f\"Comments: {len(comments_df)} records\")\n",
        "    display(comments_df[['subreddit', 'body', 'score']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Subreddit Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'submissions_df' in dir():\n",
        "    subreddit_counts = submissions_df['subreddit'].value_counts().head(20)\n",
        "    subreddit_counts.plot(kind='barh', title='Submissions by Subreddit')\n",
        "    plt.xlabel('Count')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score distribution\n",
        "if 'submissions_df' in dir():\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    submissions_df['score'].clip(upper=1000).hist(ax=axes[0], bins=50)\n",
        "    axes[0].set_title('Score Distribution (capped at 1000)')\n",
        "    axes[0].set_xlabel('Score')\n",
        "    \n",
        "    submissions_df['num_comments'].clip(upper=500).hist(ax=axes[1], bins=50)\n",
        "    axes[1].set_title('Comments per Post (capped at 500)')\n",
        "    axes[1].set_xlabel('Number of Comments')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Content Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Content length by subreddit category\n",
        "TARGET_SUBREDDITS = {\n",
        "    'political': ['politics', 'news', 'worldnews', 'geopolitics'],\n",
        "    'finance': ['investing', 'stocks', 'wallstreetbets', 'business'],\n",
        "    'science': ['science', 'technology', 'programming', 'machinelearning'],\n",
        "}\n",
        "\n",
        "def categorize_subreddit(sub):\n",
        "    sub_lower = sub.lower()\n",
        "    for category, subs in TARGET_SUBREDDITS.items():\n",
        "        if sub_lower in subs:\n",
        "            return category\n",
        "    return 'other'\n",
        "\n",
        "if 'submissions_df' in dir():\n",
        "    submissions_df['category'] = submissions_df['subreddit'].apply(categorize_subreddit)\n",
        "    submissions_df['content_length'] = (submissions_df['title'] + ' ' + submissions_df['selftext'].fillna('')).str.len()\n",
        "    \n",
        "    submissions_df.groupby('category')['content_length'].mean().plot(kind='bar')\n",
        "    plt.ylabel('Avg Content Length')\n",
        "    plt.title('Content Length by Category')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Entity Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample political post\n",
        "if 'submissions_df' in dir():\n",
        "    political = submissions_df[submissions_df['category'] == 'political']\n",
        "    if len(political) > 0:\n",
        "        sample = political.iloc[0]\n",
        "        print(f\"Subreddit: r/{sample['subreddit']}\")\n",
        "        print(f\"Title: {sample['title']}\")\n",
        "        print(f\"Score: {sample['score']}\")\n",
        "        print(\"-\" * 50)\n",
        "        print(sample['selftext'][:1000] if sample['selftext'] else '[No body text]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entity extraction with spaCy\n",
        "try:\n",
        "    import spacy\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    \n",
        "    if 'sample' in dir():\n",
        "        text = f\"{sample['title']}\\n\\n{sample['selftext'] or ''}\"\n",
        "        doc = nlp(text[:2000])\n",
        "        \n",
        "        print(\"\\nExtracted entities:\")\n",
        "        for ent in doc.ents:\n",
        "            print(f\"  {ent.label_:10} | {ent.text}\")\n",
        "except ImportError:\n",
        "    print(\"spaCy not installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entity Type Analysis by Category\n",
        "\n",
        "Expected entity types by subreddit category:\n",
        "\n",
        "**Political/News:**\n",
        "- PERSON: Politicians, public figures\n",
        "- ORG: Political parties, government agencies\n",
        "- GPE: Countries, states, cities\n",
        "- DATE: Event dates, election dates\n",
        "\n",
        "**Finance:**\n",
        "- ORG: Companies, exchanges\n",
        "- MONEY: Stock prices, investments\n",
        "- PERCENT: Returns, growth rates\n",
        "\n",
        "**Science/Tech:**\n",
        "- ORG: Tech companies, research institutions\n",
        "- PRODUCT: Technologies, software\n",
        "- PERSON: Researchers, founders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare entity distributions across categories\n",
        "try:\n",
        "    import spacy\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    \n",
        "    if 'submissions_df' in dir():\n",
        "        entity_stats = {}\n",
        "        \n",
        "        for category in ['political', 'finance', 'science']:\n",
        "            cat_posts = submissions_df[submissions_df['category'] == category]\n",
        "            if len(cat_posts) == 0:\n",
        "                continue\n",
        "                \n",
        "            entity_counts = {}\n",
        "            for _, row in cat_posts.head(50).iterrows():\n",
        "                text = f\"{row['title']}\\n{row['selftext'] or ''}\"\n",
        "                doc = nlp(text[:1000])\n",
        "                for ent in doc.ents:\n",
        "                    entity_counts[ent.label_] = entity_counts.get(ent.label_, 0) + 1\n",
        "            \n",
        "            entity_stats[category] = entity_counts\n",
        "        \n",
        "        # Display comparison\n",
        "        stats_df = pd.DataFrame(entity_stats).fillna(0).astype(int)\n",
        "        display(stats_df)\n",
        "except ImportError:\n",
        "    print(\"spaCy not installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. Expand data collection (more subreddits, longer time range)\n",
        "2. Filter high-quality posts (score threshold, length requirements)\n",
        "3. Create category-specific NER models\n",
        "4. Handle Reddit-specific entities (usernames, subreddits)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
