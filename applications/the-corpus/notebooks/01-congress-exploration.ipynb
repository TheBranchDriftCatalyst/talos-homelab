{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Congress.gov Data Exploration\n",
        "\n",
        "Explore the congressional domain data for NER training.\n",
        "\n",
        "**Goals:**\n",
        "- Load and inspect bills, members, committees\n",
        "- Visualize entity distributions\n",
        "- Test NER extraction patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies if needed\n",
        "# !pip install -e ../corpus-core\n",
        "# !pip install -e ../pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add corpus-core to path\n",
        "sys.path.insert(0, str(Path('../corpus-core/src').resolve()))\n",
        "sys.path.insert(0, str(Path('../pipelines/src').resolve()))\n",
        "\n",
        "from corpus_core.loaders import ParquetLoader\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Congressional Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize loader\n",
        "loader = ParquetLoader(Path('../datasets'))\n",
        "\n",
        "# Check available datasets\n",
        "print(\"Available datasets:\")\n",
        "for ds in loader.list_datasets():\n",
        "    print(f\"  - {ds['domain']}/{ds['name']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load bills dataset\n",
        "if loader.exists('congress', 'congress_bills'):\n",
        "    bills_df = loader.read_pandas('congress', 'congress_bills')\n",
        "    print(f\"Bills: {len(bills_df)} records\")\n",
        "    display(bills_df.head())\n",
        "else:\n",
        "    print(\"Run the Dagster pipeline first: dagster dev\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load members dataset\n",
        "if loader.exists('congress', 'congress_members'):\n",
        "    members_df = loader.read_pandas('congress', 'congress_members')\n",
        "    print(f\"Members: {len(members_df)} records\")\n",
        "    display(members_df.head())\n",
        "else:\n",
        "    print(\"Run the Dagster pipeline first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Entity Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Party distribution\n",
        "if 'members_df' in dir():\n",
        "    party_counts = members_df['party'].value_counts()\n",
        "    party_counts.plot(kind='bar', title='Members by Party')\n",
        "    plt.xlabel('Party')\n",
        "    plt.ylabel('Count')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bills by policy area\n",
        "if 'bills_df' in dir():\n",
        "    policy_counts = bills_df['policy_area'].value_counts().head(15)\n",
        "    policy_counts.plot(kind='barh', title='Bills by Policy Area (Top 15)')\n",
        "    plt.xlabel('Count')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test NER Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample text for NER testing\n",
        "if 'bills_df' in dir():\n",
        "    sample_bill = bills_df.iloc[0]\n",
        "    sample_text = f\"{sample_bill['title']}\\n\\n{sample_bill.get('latest_action_text', '')}\"\n",
        "    print(\"Sample text for NER:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(sample_text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with spaCy (if available)\n",
        "try:\n",
        "    import spacy\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    \n",
        "    if 'sample_text' in dir():\n",
        "        doc = nlp(sample_text[:1000])\n",
        "        \n",
        "        print(\"\\nExtracted entities:\")\n",
        "        for ent in doc.ents:\n",
        "            print(f\"  {ent.text:30} -> {ent.label_}\")\n",
        "except ImportError:\n",
        "    print(\"spaCy not installed. Run: pip install spacy && python -m spacy download en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entity Type Analysis\n",
        "\n",
        "Expected entity types in congressional data:\n",
        "- **PERSON**: Congress members, presidents, officials\n",
        "- **ORG**: Committees, agencies, departments\n",
        "- **GPE**: States, countries, districts\n",
        "- **DATE**: Bill dates, term dates\n",
        "- **LAW**: Bill numbers, acts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze entity patterns in member names\n",
        "if 'members_df' in dir():\n",
        "    print(\"Sample member names:\")\n",
        "    for name in members_df['name'].head(10):\n",
        "        print(f\"  - {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. Run Dagster pipeline to populate datasets\n",
        "2. Analyze entity extraction accuracy\n",
        "3. Create training data annotations\n",
        "4. Export to spaCy/HuggingFace format"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
